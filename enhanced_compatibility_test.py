#!/usr/bin/env python3
"""
å¢å¼ºçš„LLMæä¾›å•†å…¼å®¹æ€§æµ‹è¯•
éªŒè¯ä¸LangChain init_chat_modelå®Œå…¨ä¸€è‡´çš„æä¾›å•†æ£€æµ‹å’Œå…¼å®¹æ€§
"""

import sys
sys.path.append('.')

from src.open_deep_research.message_manager import (
    UniversalMessageManager, 
    validate_and_fix_messages,
    LLMProvider
)

def test_langchain_compatibility():
    """æµ‹è¯•ä¸LangChain init_chat_modelçš„å®Œå…¨å…¼å®¹æ€§"""
    print("ğŸ”— LangChainå…¼å®¹æ€§æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•LangChainçš„æ¨¡å‹åç§°æ¨æ–­è§„åˆ™
    langchain_test_cases = [
        # OpenAIç³»åˆ—
        ("gpt-3.5-turbo", LLMProvider.OPENAI, "GPT-3.5"),
        ("gpt-4o", LLMProvider.OPENAI, "GPT-4o"),
        ("o1-preview", LLMProvider.OPENAI, "o1 Preview"),
        ("openai:gpt-4", LLMProvider.OPENAI, "OpenAI GPT-4"),
        
        # Anthropicç³»åˆ—
        ("claude-3-sonnet", LLMProvider.ANTHROPIC, "Claude 3 Sonnet"),
        ("claude-3-5-haiku", LLMProvider.ANTHROPIC, "Claude 3.5 Haiku"),
        ("anthropic:claude-3-opus", LLMProvider.ANTHROPIC, "Anthropic Claude 3 Opus"),
        
        # Googleç³»åˆ—
        ("gemini-pro", LLMProvider.GOOGLE_VERTEXAI, "Gemini Pro"),
        ("gemini-1.5-flash", LLMProvider.GOOGLE_VERTEXAI, "Gemini 1.5 Flash"),
        ("google_vertexai:gemini-2.0", LLMProvider.GOOGLE_VERTEXAI, "Google VertexAI Gemini"),
        ("google_genai:gemini-pro", LLMProvider.GOOGLE_GENAI, "Google GenAI Gemini"),
        
        # AWS Bedrock
        ("amazon.titan-text", LLMProvider.BEDROCK, "Amazon Titan"),
        ("bedrock:claude-3", LLMProvider.BEDROCK, "Bedrock Claude"),
        ("bedrock_converse:gpt-4", LLMProvider.BEDROCK_CONVERSE, "Bedrock Converse"),
        
        # Cohere
        ("command-r", LLMProvider.COHERE, "Command R"),
        ("command-r-plus", LLMProvider.COHERE, "Command R Plus"),
        ("cohere:command", LLMProvider.COHERE, "Cohere Command"),
        
        # Fireworks
        ("accounts/fireworks/models/llama", LLMProvider.FIREWORKS, "Fireworks Llama"),
        ("fireworks:llama-2", LLMProvider.FIREWORKS, "Fireworks Llama-2"),
        
        # MistralAI
        ("mistral-large", LLMProvider.MISTRALAI, "Mistral Large"),
        ("mistral-7b", LLMProvider.MISTRALAI, "Mistral 7B"),
        ("mistralai:mixtral", LLMProvider.MISTRALAI, "MistralAI Mixtral"),
        
        # DeepSeek
        ("deepseek-chat", LLMProvider.DEEPSEEK, "DeepSeek Chat"),
        ("deepseek-coder", LLMProvider.DEEPSEEK, "DeepSeek Coder"),
        ("deepseek:v2", LLMProvider.DEEPSEEK, "DeepSeek V2"),
        
        # XAI
        ("grok-beta", LLMProvider.XAI, "Grok Beta"),
        ("grok-1", LLMProvider.XAI, "Grok 1"),
        ("xai:grok", LLMProvider.XAI, "XAI Grok"),
        
        # Perplexity
        ("sonar-small", LLMProvider.PERPLEXITY, "Sonar Small"),
        ("sonar-medium", LLMProvider.PERPLEXITY, "Sonar Medium"),
        ("perplexity:sonar", LLMProvider.PERPLEXITY, "Perplexity Sonar"),
        
        # å…¶ä»–æä¾›å•†
        ("groq:llama-3", LLMProvider.GROQ, "Groq Llama"),
        ("ollama:llama2", LLMProvider.OLLAMA, "Ollama Llama2"),
        ("huggingface:bert", LLMProvider.HUGGINGFACE, "HuggingFace BERT"),
        ("together:llama", LLMProvider.TOGETHER, "Together Llama"),
        ("ibm:granite", LLMProvider.IBM, "IBM Granite"),
        ("nvidia:nemotron", LLMProvider.NVIDIA, "Nvidia Nemotron"),
        ("azure_ai:phi", LLMProvider.AZURE_AI, "Azure AI Phi"),
        ("azure_openai:gpt-4", LLMProvider.AZURE_OPENAI, "Azure OpenAI GPT-4"),
    ]
    
    print(f"\nğŸ“‹ æµ‹è¯• {len(langchain_test_cases)} ä¸ªLangChainå…¼å®¹æ¡ˆä¾‹")
    print("-" * 60)
    
    success_count = 0
    for model_name, expected_provider, description in langchain_test_cases:
        try:
            manager = UniversalMessageManager(model_name)
            detected_provider = manager.provider
            
            if detected_provider == expected_provider:
                status = "âœ…"
                success_count += 1
            else:
                status = "âŒ"
            
            print(f"{status} {description:<25} | {model_name:<30} -> {detected_provider.value:<15} (æœŸæœ›: {expected_provider.value})")
            
        except Exception as e:
            print(f"âŒ {description:<25} | {model_name:<30} -> ERROR: {e}")
    
    compatibility_rate = (success_count / len(langchain_test_cases)) * 100
    print(f"\nğŸ“Š LangChainå…¼å®¹æ€§: {success_count}/{len(langchain_test_cases)} ({compatibility_rate:.1f}%)")
    
    return compatibility_rate

def test_enhanced_message_processing():
    """æµ‹è¯•å¢å¼ºçš„æ¶ˆæ¯å¤„ç†èƒ½åŠ›"""
    print("\nğŸ”§ å¢å¼ºæ¶ˆæ¯å¤„ç†æµ‹è¯•")
    print("=" * 60)
    
    # å¤æ‚çš„æ¶ˆæ¯åºåˆ—æµ‹è¯•
    complex_messages = [
        {"role": "system", "content": "You are a research assistant."},
        {"role": "assistant", "content": "I'll help with research."},
        {"role": "assistant", "content": "Let me search for information."},
        {"role": "assistant", "content": "I found some results."},
        {"role": "user", "content": "What did you find?"},
        {"role": "assistant", "content": "Here's what I found...", 
         "tool_calls": [{"id": "1", "name": "search", "args": {"q": "AI research"}}]},
        {"role": "assistant", "content": "Additional analysis..."},
        {"role": "tool", "content": "Search results: AI research papers...", "tool_call_id": "1"},
        {"role": "user", "content": "Please summarize."}
    ]
    
    # æµ‹è¯•æ‰€æœ‰æ–°å¢çš„æä¾›å•†
    new_providers = [
        "cohere:command-r",
        "fireworks:llama-2", 
        "mistralai:mixtral",
        "xai:grok-beta",
        "perplexity:sonar-medium",
        "bedrock:claude-3",
        "together:llama-3",
        "ibm:granite",
        "nvidia:nemotron",
        "azure_ai:phi-3"
    ]
    
    print(f"\nğŸ“ æµ‹è¯• {len(new_providers)} ä¸ªæ–°å¢æä¾›å•†çš„æ¶ˆæ¯å¤„ç†")
    print("-" * 60)
    
    for provider_model in new_providers:
        try:
            fixed_messages, fixes = validate_and_fix_messages(complex_messages.copy(), provider_model)
            
            provider_name = provider_model.split(':')[0]
            print(f"âœ… {provider_name:<12}: {len(complex_messages)} -> {len(fixed_messages)} æ¡æ¶ˆæ¯, {len(fixes)} ä¸ªä¿®å¤")
            
            if fixes:
                print(f"   ä¿®å¤: {', '.join(fixes[:2])}{'...' if len(fixes) > 2 else ''}")
                
        except Exception as e:
            print(f"âŒ {provider_model:<20}: é”™è¯¯ - {e}")

def test_real_world_scenarios():
    """æµ‹è¯•çœŸå®ä¸–ç•Œåœºæ™¯"""
    print("\nğŸŒ çœŸå®ä¸–ç•Œåœºæ™¯æµ‹è¯•")
    print("=" * 60)
    
    scenarios = [
        {
            "name": "å¤šæ¨¡æ€AIç ”ç©¶å·¥ä½œæµ",
            "providers": ["openai:gpt-4o", "anthropic:claude-3-opus", "google_genai:gemini-pro"],
            "messages": [
                {"role": "user", "content": "åˆ†æè¿™ä¸ªå›¾åƒä¸­çš„AIæ¨¡å‹æ¶æ„"},
                {"role": "assistant", "content": "æˆ‘æ¥åˆ†æè¿™ä¸ªæ¶æ„å›¾...", 
                 "tool_calls": [{"id": "1", "name": "vision_analysis"}]},
                {"role": "tool", "content": "å›¾åƒåˆ†æç»“æœ...", "tool_call_id": "1"},
                {"role": "assistant", "content": "åŸºäºåˆ†æç»“æœ..."},
                {"role": "assistant", "content": "è®©æˆ‘æœç´¢ç›¸å…³è®ºæ–‡...", 
                 "tool_calls": [{"id": "2", "name": "search"}]},
                {"role": "tool", "content": "è®ºæ–‡æœç´¢ç»“æœ...", "tool_call_id": "2"},
                {"role": "assistant", "content": "ç»¼åˆåˆ†æ..."}
            ]
        },
        {
            "name": "ä»£ç ç”Ÿæˆä¸ä¼˜åŒ–å·¥ä½œæµ", 
            "providers": ["deepseek:deepseek-coder", "mistralai:codestral", "cohere:command-r"],
            "messages": [
                {"role": "user", "content": "ä¼˜åŒ–è¿™æ®µPythonä»£ç çš„æ€§èƒ½"},
                {"role": "assistant", "content": "æˆ‘æ¥åˆ†æä»£ç æ€§èƒ½...", 
                 "tool_calls": [{"id": "1", "name": "code_analysis"}]},
                {"role": "assistant", "content": "å‘ç°äº†å‡ ä¸ªä¼˜åŒ–ç‚¹..."},
                {"role": "tool", "content": "æ€§èƒ½åˆ†ææŠ¥å‘Š...", "tool_call_id": "1"},
                {"role": "assistant", "content": "å»ºè®®çš„ä¼˜åŒ–æ–¹æ¡ˆ..."}
            ]
        },
        {
            "name": "ä¼ä¸šçº§AIåŠ©æ‰‹å·¥ä½œæµ",
            "providers": ["azure_openai:gpt-4", "bedrock:claude-3", "ibm:granite"],
            "messages": [
                {"role": "system", "content": "ä¼ä¸šAIåŠ©æ‰‹ç³»ç»Ÿæç¤º"},
                {"role": "user", "content": "ç”Ÿæˆå­£åº¦ä¸šåŠ¡æŠ¥å‘Š"},
                {"role": "assistant", "content": "æˆ‘æ¥æ”¶é›†æ•°æ®...", 
                 "tool_calls": [{"id": "1", "name": "database_query"}]},
                {"role": "assistant", "content": "åˆ†æä¸šåŠ¡æŒ‡æ ‡..."},
                {"role": "tool", "content": "æ•°æ®æŸ¥è¯¢ç»“æœ...", "tool_call_id": "1"},
                {"role": "assistant", "content": "ç”ŸæˆæŠ¥å‘Š..."}
            ]
        }
    ]
    
    for scenario in scenarios:
        print(f"\nğŸ“‹ {scenario['name']}")
        print("-" * 40)
        
        for provider in scenario['providers']:
            try:
                fixed, fixes = validate_and_fix_messages(scenario['messages'], provider)
                provider_short = provider.split(':')[0]
                print(f"  {provider_short:<15}: {len(scenario['messages'])} -> {len(fixed)} æ¡, {len(fixes)} ä¿®å¤")
            except Exception as e:
                print(f"  {provider:<15}: âŒ {e}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¢å¼ºçš„LLMå…¼å®¹æ€§æµ‹è¯•")
    print("ğŸ”— éªŒè¯ä¸LangChain init_chat_modelçš„å®Œå…¨å…¼å®¹æ€§")
    print("=" * 80)
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    langchain_compatibility = test_langchain_compatibility()
    test_enhanced_message_processing()
    test_real_world_scenarios()
    
    print("\n" + "=" * 80)
    print("ğŸ‰ æµ‹è¯•å®Œæˆ!")
    print(f"âœ… LangChainå…¼å®¹æ€§: {langchain_compatibility:.1f}%")
    
    if langchain_compatibility >= 95:
        print("ğŸ† ä¼˜ç§€ï¼ä¸LangChainé«˜åº¦å…¼å®¹")
    elif langchain_compatibility >= 85:
        print("ğŸ‘ è‰¯å¥½ï¼åŸºæœ¬å…¼å®¹LangChain")
    else:
        print("âš ï¸ éœ€è¦æ”¹è¿›å…¼å®¹æ€§")
    
    print("\nğŸ“‹ æ”¯æŒçš„æä¾›å•†æ€»æ•°:", len(LLMProvider) - 1)  # å‡å»UNKNOWN
    print("ğŸ”— å®Œå…¨éµå¾ªLangChain init_chat_modelæ¨æ–­è§„åˆ™")

if __name__ == "__main__":
    main() 