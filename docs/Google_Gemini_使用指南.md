# åœ¨ open_deep_research é¡¹ç›®ä¸­ä½¿ç”¨ Google Gemini æ¨¡å‹

## ğŸ“‹ å‰ç½®æ¡ä»¶

1. ç¡®ä¿å·²å®‰è£…é¡¹ç›®ä¾èµ–ï¼ˆé¡¹ç›®å·²åŒ…å« Google AI æ”¯æŒï¼‰
2. è·å– Google AI API å¯†é’¥

## ğŸ”‘ API å¯†é’¥è®¾ç½®

### è·å– API å¯†é’¥

1. è®¿é—® [Google AI Studio](https://makersuite.google.com/)
2. ç™»å½• Google è´¦å·
3. åˆ›å»ºæ–°çš„ API å¯†é’¥
4. å¤åˆ¶ç”Ÿæˆçš„ API å¯†é’¥

### ç¯å¢ƒå˜é‡é…ç½®

åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `.env` æ–‡ä»¶ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰ï¼Œå¹¶æ·»åŠ ï¼š

```bash
# Google AI API å¯†é’¥
GOOGLE_API_KEY=your_google_api_key_here

# å¯é€‰ï¼šå¦‚æœä½¿ç”¨ Vertex AI
GOOGLE_CLOUD_PROJECT=your_project_id
GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account-key.json
```

### PowerShell ä¸­è®¾ç½®ç¯å¢ƒå˜é‡

```powershell
# è®¾ç½® Google AI API å¯†é’¥
$env:GOOGLE_API_KEY="your_google_api_key_here"

# éªŒè¯è®¾ç½®
echo $env:GOOGLE_API_KEY
```

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. Graph-based å·¥ä½œæµæ¨¡å¼

åœ¨ Jupyter Notebook æˆ– Python è„šæœ¬ä¸­ï¼š

```python
import uuid
from open_deep_research.graph import builder
from langgraph.checkpoint.memory import MemorySaver

# ç¼–è¯‘å›¾
memory = MemorySaver()
graph = builder.compile(checkpointer=memory)

# é…ç½® Gemini æ¨¡å‹
thread = {
    "configurable": {
        "thread_id": str(uuid.uuid4()),
        "search_api": "tavily",  # æœç´¢API
        
        # ä½¿ç”¨ Gemini ä½œä¸ºè§„åˆ’å™¨
        "planner_provider": "google_genai",
        "planner_model": "gemini-2.0-flash",
        
        # ä½¿ç”¨ Gemini ä½œä¸ºå†™ä½œå™¨
        "writer_provider": "google_genai", 
        "writer_model": "gemini-2.0-flash",
        
        # å¯é€‰ï¼šæ‘˜è¦æ¨¡å‹ä¹Ÿä½¿ç”¨ Gemini
        "summarization_model_provider": "google_genai",
        "summarization_model": "gemini-2.0-flash",
        
        "max_search_depth": 2,
    }
}

# æ‰§è¡Œç ”ç©¶ä»»åŠ¡
topic = "AIæ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æœ€æ–°å‘å±•"

async for event in graph.astream({"topic": topic}, thread, stream_mode="updates"):
    if '__interrupt__' in event:
        interrupt_value = event['__interrupt__'][0].value
        print(interrupt_value)
```

### 2. Multi-Agent æ¨¡å¼

```python
import uuid
from open_deep_research.multi_agent import graph

# é…ç½®å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ
config = {
    "thread_id": str(uuid.uuid4()),
    "search_api": "tavily",
    
    # Supervisor ä½¿ç”¨ Gemini
    "supervisor_model": "google_genai:gemini-2.0-flash",
    
    # Researcher ä½¿ç”¨ Gemini  
    "researcher_model": "google_genai:gemini-2.0-flash",
    
    # å¯é€‰é…ç½®
    "ask_for_clarification": True,
}

thread_config = {"configurable": config}

# æ‰§è¡Œç ”ç©¶
msg = [{"role": "user", "content": "è¯·ç ”ç©¶é‡å­è®¡ç®—çš„æœ€æ–°è¿›å±•"}]
response = await graph.ainvoke({"messages": msg}, config=thread_config)
```

## ğŸ¯ å¯ç”¨çš„ Gemini æ¨¡å‹

### Google AI (google_genai provider)

```python
# æœ€æ–°çš„ Gemini 2.0 æ¨¡å‹
"planner_provider": "google_genai"
"planner_model": "gemini-2.0-flash"

# Gemini 1.5 ç³»åˆ—
"planner_model": "gemini-1.5-pro"
"planner_model": "gemini-1.5-flash" 
"planner_model": "gemini-1.5-flash-8b"

# Gemini 1.0 ç³»åˆ—ï¼ˆè¾ƒè€ç‰ˆæœ¬ï¼‰
"planner_model": "gemini-pro"
"planner_model": "gemini-pro-vision"
```

### Vertex AI (google-vertexai provider)

```python
# ä½¿ç”¨ Vertex AI
"planner_provider": "google-vertexai"
"planner_model": "gemini-2.0-flash"

# éœ€è¦é¢å¤–é…ç½®
GOOGLE_CLOUD_PROJECT=your_project_id
GOOGLE_APPLICATION_CREDENTIALS=/path/to/credentials.json
```

## âš™ï¸ é«˜çº§é…ç½®

### æ¨¡å‹å‚æ•°è°ƒä¼˜

```python
thread = {
    "configurable": {
        # ... å…¶ä»–é…ç½® ...
        
        # è‡ªå®šä¹‰æ¨¡å‹å‚æ•°
        "planner_model_kwargs": {
            "temperature": 0.7,
            "max_tokens": 4096,
            "top_p": 0.9,
        },
        
        "writer_model_kwargs": {
            "temperature": 0.8,
            "max_tokens": 8192,
        }
    }
}
```

### æ··åˆæ¨¡å‹ç­–ç•¥

```python
# è§„åˆ’ç”¨ Geminiï¼Œå†™ä½œç”¨å…¶ä»–æ¨¡å‹
thread = {
    "configurable": {
        "planner_provider": "google_genai",
        "planner_model": "gemini-2.0-flash",
        
        "writer_provider": "anthropic", 
        "writer_model": "claude-3-5-sonnet-latest",
        
        "summarization_model_provider": "google_genai",
        "summarization_model": "gemini-1.5-flash",
    }
}
```

## ğŸ” å®ç”¨ç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šåŸºç¡€ç ”ç©¶æŠ¥å‘Š

```python
import os
import asyncio
from open_deep_research.graph import builder
from langgraph.checkpoint.memory import MemorySaver

# è®¾ç½® API å¯†é’¥
os.environ["GOOGLE_API_KEY"] = "your_api_key"
os.environ["TAVILY_API_KEY"] = "your_tavily_key"

async def generate_report():
    memory = MemorySaver()
    graph = builder.compile(checkpointer=memory)
    
    config = {
        "configurable": {
            "thread_id": "gemini-test-1",
            "search_api": "tavily",
            "planner_provider": "google_genai",
            "planner_model": "gemini-2.0-flash",
            "writer_provider": "google_genai",
            "writer_model": "gemini-1.5-pro",
            "max_search_depth": 2,
        }
    }
    
    topic = "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—è¯Šæ–­ä¸­çš„åº”ç”¨ç°çŠ¶ä¸å‘å±•è¶‹åŠ¿"
    
    async for event in graph.astream({"topic": topic}, config):
        print(f"Event: {event}")
    
    # è·å–æœ€ç»ˆæŠ¥å‘Š
    final_state = graph.get_state(config)
    if final_state.values.get('final_report'):
        print("=== æœ€ç»ˆæŠ¥å‘Š ===")
        print(final_state.values['final_report'])

# è¿è¡Œ
asyncio.run(generate_report())
```

### ç¤ºä¾‹ 2ï¼šå¤šæ™ºèƒ½ä½“åä½œ

```python
from open_deep_research.multi_agent import graph

async def multi_agent_research():
    config = {
        "configurable": {
            "thread_id": "gemini-multi-1",
            "search_api": "googlesearch",
            "supervisor_model": "google_genai:gemini-2.0-flash",
            "researcher_model": "google_genai:gemini-1.5-pro",
        }
    }
    
    messages = [{
        "role": "user", 
        "content": "è¯·ç ”ç©¶å¹¶åˆ†æåŒºå—é“¾æŠ€æœ¯åœ¨ä¾›åº”é“¾ç®¡ç†ä¸­çš„åº”ç”¨æ¡ˆä¾‹"
    }]
    
    result = await graph.ainvoke({"messages": messages}, config=config)
    print(result["final_report"])

asyncio.run(multi_agent_research())
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. æ¨¡å‹é€‰æ‹©å»ºè®®

- **è§„åˆ’ä»»åŠ¡**: `gemini-2.0-flash` - é€Ÿåº¦å¿«ï¼Œæ¨ç†èƒ½åŠ›å¼º
- **é•¿æ–‡æœ¬å†™ä½œ**: `gemini-1.5-pro` - æ›´å¥½çš„æ–‡æœ¬ç”Ÿæˆè´¨é‡
- **æ‘˜è¦ä»»åŠ¡**: `gemini-1.5-flash` - é«˜æ•ˆä¸”ç»æµ

### 2. æˆæœ¬ä¼˜åŒ–

```python
# æˆæœ¬æ•æ„Ÿçš„é…ç½®
config = {
    "planner_provider": "google_genai",
    "planner_model": "gemini-1.5-flash",  # æ›´ç»æµçš„é€‰æ‹©
    
    "writer_provider": "google_genai", 
    "writer_model": "gemini-1.5-flash",   # å¹³è¡¡æ€§èƒ½å’Œæˆæœ¬
    
    "max_search_depth": 1,  # å‡å°‘æœç´¢è¿­ä»£
}
```

### 3. é”™è¯¯å¤„ç†

```python
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)

try:
    result = await graph.ainvoke({"messages": messages}, config=config)
except Exception as e:
    logging.error(f"Gemini API é”™è¯¯: {e}")
    # å›é€€åˆ°å…¶ä»–æ¨¡å‹
    config["configurable"]["planner_provider"] = "anthropic"
    config["configurable"]["planner_model"] = "claude-3-haiku-20240307"
```

## ğŸš¨ æ³¨æ„äº‹é¡¹

1. **API é™åˆ¶**: Gemini API æœ‰è¯·æ±‚é€Ÿç‡é™åˆ¶ï¼Œé«˜é¢‘ä½¿ç”¨æ—¶æ³¨æ„æ§åˆ¶å¹¶å‘
2. **Token é™åˆ¶**: ä¸åŒæ¨¡å‹æœ‰ä¸åŒçš„ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶
3. **åœ°åŒºé™åˆ¶**: æŸäº›åœ°åŒºå¯èƒ½æ— æ³•è®¿é—® Gemini API
4. **æˆæœ¬æ§åˆ¶**: ç›‘æ§ API ä½¿ç”¨é‡ï¼Œé¿å…æ„å¤–é«˜é¢è´¹ç”¨

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **æ¨¡å‹æä¾›å•†æ¨æ–­é”™è¯¯**
   
   **é”™è¯¯ä¿¡æ¯**: `Unable to infer model provider for model='google_genai:gemini-1.5-flash'`
   
   **åŸå› **: åœ¨å¤šæ™ºèƒ½ä½“æ¨¡å¼ä¸­ä½¿ç”¨äº†é”™è¯¯çš„ provider åç§°æ ¼å¼
   
   **è§£å†³æ–¹æ¡ˆ**:
   ```python
   # âŒ é”™è¯¯å†™æ³•
   config = {
       "configurable": {
           "supervisor_model": "google_genai:gemini-1.5-flash"
       }
   }
   
   # âœ… æ­£ç¡®å†™æ³•
   config = {
       "configurable": {
           "supervisor_model": "google_genai:gemini-1.5-flash"  # æ³¨æ„ä¸‹åˆ’çº¿
       }
   }
   ```

2. **è®¤è¯é”™è¯¯**
   ```python
   # ç¡®ä¿ API å¯†é’¥æ­£ç¡®è®¾ç½®
   import os
   print(os.environ.get("GOOGLE_API_KEY"))
   ```

3. **æ¨¡å‹ä¸å¯ç”¨**
   ```python
   # æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®
   from langchain.chat_models import init_chat_model
   
   try:
       model = init_chat_model(
           model="gemini-2.0-flash",
           model_provider="google_genai"
       )
       print("æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
   except Exception as e:
       print(f"é”™è¯¯: {e}")
   ```

4. **é…ç½®æ ¼å¼å·®å¼‚**
   
   ä¸åŒæ¨¡å¼ä¸‹çš„é…ç½®æ ¼å¼ç•¥æœ‰ä¸åŒï¼š
   ```python
   # Graph æ¨¡å¼ - ä½¿ç”¨åˆ†ç¦»çš„ provider å’Œ model å­—æ®µ
   graph_config = {
       "configurable": {
           "planner_provider": "google_genai",
           "planner_model": "gemini-1.5-flash",
       }
   }
   
   # Multi-Agent æ¨¡å¼ - ä½¿ç”¨ç»„åˆçš„æ¨¡å‹å­—ç¬¦ä¸²
   multi_agent_config = {
       "configurable": {
           "supervisor_model": "google_genai:gemini-1.5-flash",  # æ³¨æ„ provider åç§°
           "researcher_model": "google_genai:gemini-1.5-flash",
       }
   }
   ```

5. **ç½‘ç»œè¿æ¥é—®é¢˜**
   - ç¡®ä¿ç½‘ç»œå¯ä»¥è®¿é—® Google AI API
   - å¦‚åœ¨ä¸­å›½å¤§é™†ï¼Œå¯èƒ½éœ€è¦ä»£ç†

### è·å–æ›´å¤šå¸®åŠ©

- [Google AI å®˜æ–¹æ–‡æ¡£](https://ai.google.dev/)
- [LangChain Google é›†æˆæ–‡æ¡£](https://python.langchain.com/docs/integrations/chat/google_generative_ai)
- [é¡¹ç›® GitHub Issues](https://github.com/langchain-ai/open_deep_research/issues) 