{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Researcher\n",
    "\n",
    "This notebook demonstrates the multi-agent research approach, which uses a supervisor-researcher collaborative pattern to create comprehensive reports. The system consists of:\n",
    "\n",
    "1. A **Supervisor Agent** that plans the overall report structure and coordinates work\n",
    "2. Multiple **Research Agents** that investigate specific topics in parallel\n",
    "3. A workflow that produces a structured report with introduction, body sections, and conclusion\n",
    "\n",
    "## From repo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g:\\MyProjects\\open_deep_research\\src\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 🔧 错误修复说明\n",
    "\n",
    "### 📋 **已修复的问题**\n",
    "\n",
    "#### 1. **parallel_tool_calls 参数问题**\n",
    "- **问题**: `TypeError: GenerativeServiceAsyncClient.generate_content() got an unexpected keyword argument 'parallel_tool_calls'`\n",
    "- **原因**: Google Gemini API不支持`parallel_tool_calls`参数\n",
    "- **解决方案**: 自动检测Gemini模型并跳过不支持的参数 ✅\n",
    "\n",
    "#### 2. **消息顺序问题** \n",
    "- **问题**: `ChatGoogleGenerativeAIError: function call turn comes immediately after a user turn or after a function response turn`\n",
    "- **原因**: Gemini对消息顺序有严格要求，工具调用必须紧跟在用户消息或工具响应之后\n",
    "- **解决方案**: 重新设计消息序列，确保符合Gemini的严格要求 ✅\n",
    "\n",
    "#### 3. **消息格式错误**\n",
    "- **问题**: `'AIMessage' object has no attribute 'get'`\n",
    "- **原因**: 代码试图在LangChain消息对象上使用字典方法\n",
    "- **解决方案**: 智能检测消息类型并正确处理dict和LangChain对象格式 ✅\n",
    "\n",
    "### 🛠️ **修复详情**\n",
    "\n",
    "1. **智能模型检测**: 自动识别Gemini模型并应用特殊处理\n",
    "2. **消息序列优化**: 过滤和重新排序消息以符合Gemini要求  \n",
    "3. **系统提示处理**: 将系统提示转换为用户指令格式\n",
    "4. **消息格式处理**: 支持dict和LangChain消息对象两种格式\n",
    "5. **调试增强**: 添加详细的消息处理日志\n",
    "6. **兼容性增强**: 保持与其他模型的完全兼容性\n",
    "\n",
    "### 🚀 **备选模型配置**\n",
    "\n",
    "如果仍然遇到问题，可以尝试以下替代配置：\n",
    "\n",
    "```python\n",
    "# 使用Claude模型 (最稳定)\n",
    "config = {\n",
    "    \"thread_id\": str(uuid.uuid4()),\n",
    "    \"search_api\": \"tavily\",\n",
    "    \"supervisor_model\": \"anthropic:claude-3-7-sonnet-latest\",\n",
    "    \"researcher_model\": \"anthropic:claude-3-5-haiku-latest\",\n",
    "}\n",
    "\n",
    "# 或者使用OpenAI模型\n",
    "config = {\n",
    "    \"thread_id\": str(uuid.uuid4()),\n",
    "    \"search_api\": \"tavily\", \n",
    "    \"supervisor_model\": \"openai:gpt-4o\",\n",
    "    \"researcher_model\": \"openai:gpt-4o-mini\",\n",
    "}\n",
    "\n",
    "# 使用Gemini但禁用搜索（更简单的工作流）\n",
    "config = {\n",
    "    \"thread_id\": str(uuid.uuid4()),\n",
    "    \"search_api\": \"none\",  # 禁用搜索工具\n",
    "    \"supervisor_model\": \"google_genai:gemini-2.5-pro-preview-05-06\",\n",
    "    \"researcher_model\": \"google_genai:gemini-2.5-flash\",\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U -q open-deep-research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 开始调试消息处理...\n",
      "✅ 测试消息创建成功: 3 条消息\n",
      "\n",
      "处理消息 [0]:\n",
      "  类型: <class 'dict'>\n",
      "  内容: {'role': 'user', 'content': 'Test message 1'}\n",
      "  ✅ 已添加到过滤列表\n",
      "\n",
      "处理消息 [1]:\n",
      "  类型: <class 'dict'>\n",
      "  内容: {'role': 'assistant', 'content': 'Test response 1'}\n",
      "  ✅ 已添加到过滤列表\n",
      "\n",
      "处理消息 [2]:\n",
      "  类型: <class 'dict'>\n",
      "  内容: {'role': 'user', 'content': 'Test message 2'}\n",
      "  ✅ 已添加到过滤列表\n",
      "\n",
      "📊 处理结果:\n",
      "  原始消息数: 3\n",
      "  过滤后消息数: 3\n",
      "  处理成功率: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# 🔧 调试和诊断工具\n",
    "\n",
    "def debug_message_processing():\n",
    "    \"\"\"调试消息处理过程\"\"\"\n",
    "    print(\"🔍 开始调试消息处理...\")\n",
    "    \n",
    "    # 创建测试消息\n",
    "    test_messages = [\n",
    "        {\"role\": \"user\", \"content\": \"Test message 1\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Test response 1\"},\n",
    "        {\"role\": \"user\", \"content\": \"Test message 2\"}\n",
    "    ]\n",
    "    \n",
    "    print(f\"✅ 测试消息创建成功: {len(test_messages)} 条消息\")\n",
    "    \n",
    "    # 测试消息处理逻辑\n",
    "    filtered_messages = []\n",
    "    for i, msg in enumerate(test_messages):\n",
    "        print(f\"\\n处理消息 [{i}]:\")\n",
    "        print(f\"  类型: {type(msg)}\")\n",
    "        print(f\"  内容: {msg}\")\n",
    "        \n",
    "        if isinstance(msg, dict):\n",
    "            if msg.get(\"role\") in [\"user\", \"assistant\", \"tool\"]:\n",
    "                filtered_messages.append(msg)\n",
    "                print(f\"  ✅ 已添加到过滤列表\")\n",
    "            else:\n",
    "                print(f\"  ❌ 角色不符合要求: {msg.get('role')}\")\n",
    "        elif hasattr(msg, 'role') and hasattr(msg, 'content'):\n",
    "            if msg.role in [\"user\", \"assistant\", \"tool\"]:\n",
    "                message_dict = {\"role\": msg.role, \"content\": msg.content}\n",
    "                filtered_messages.append(message_dict)\n",
    "                print(f\"  ✅ LangChain消息已转换并添加\")\n",
    "            else:\n",
    "                print(f\"  ❌ LangChain消息角色不符合: {msg.role}\")\n",
    "        else:\n",
    "            print(f\"  ❌ 未知消息格式\")\n",
    "    \n",
    "    print(f\"\\n📊 处理结果:\")\n",
    "    print(f\"  原始消息数: {len(test_messages)}\")\n",
    "    print(f\"  过滤后消息数: {len(filtered_messages)}\")\n",
    "    print(f\"  处理成功率: {len(filtered_messages)/len(test_messages)*100:.1f}%\")\n",
    "    \n",
    "    return filtered_messages\n",
    "\n",
    "# 运行调试\n",
    "debug_results = debug_message_processing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! uv pip install -U -q nest-asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the multi-agent graph\n",
    "\n",
    "Next, we'll compile the LangGraph workflow for the multi-agent research approach. This step creates the orchestration layer that manages communication between the supervisor and research agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.15\n"
     ]
    }
   ],
   "source": [
    "import uuid \n",
    "import os, getpass\n",
    "import open_deep_research   \n",
    "print(open_deep_research.__version__) \n",
    "from IPython.display import Image, display, Markdown\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from open_deep_research.multi_agent import supervisor_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MemorySaver for checkpointing the agent's state\n",
    "# This enables tracking and debugging of the multi-agent interaction\n",
    "checkpointer = MemorySaver()\n",
    "agent = supervisor_builder.compile(name=\"research_team\", checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the graph structure\n",
    "# This shows how supervisor and research agents are connected in the workflow\n",
    "display(Image(agent.get_graph(xray=1).draw_mermaid_png(max_retries=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 使用配置: googlesearch 模式\n",
      "✅ 异步环境已配置\n",
      "🚀 开始运行多智能体工作流（googlesearch 模式）...\n",
      "Requesting 5 results for '模型上下文协议（MCP）基本概念和主要用途' from Google API...\n",
      "Requesting 5 results for 'Model Context Protocol (MCP) concepts and uses' from Google API...\n",
      "Fetched full content for 5 results\n",
      "Fetched full content for 5 results\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'google_search_tool', 'arguments': '{\"queries\": [\"MCP \\\\u4f18\\\\u52bf\", \"MCP \\\\u5e94\\\\u7528\\\\u573a\\\\u666f\"]}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--7bad0ad9-69fd-494b-866b-c7d63c938c9d-0', tool_calls=[{'name': 'google_search_tool', 'args': {'queries': ['MCP 优势', 'MCP 应用场景']}, 'id': '4ea5f5d9-b1f6-4081-ac2c-5dba624b074c', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1162, 'output_tokens': 24, 'total_tokens': 1379, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 193}})]}\n",
      "Requesting 5 results for 'MCP 优势' from Google API...\n",
      "Requesting 5 results for 'MCP 应用场景' from Google API...\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'google_search_tool', 'arguments': '{\"queries\": [\"MCP \\\\u67b6\\\\u6784\", \"MCP \\\\u5de5\\\\u4f5c\\\\u539f\\\\u7406\"]}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--3906dac6-3ba3-4243-bd78-c3cd45e05f0d-0', tool_calls=[{'name': 'google_search_tool', 'args': {'queries': ['MCP 架构', 'MCP 工作原理']}, 'id': '8c97444f-d865-4cee-9fb7-470b2b543ac4', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1162, 'output_tokens': 24, 'total_tokens': 1282, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 96}})]}\n",
      "Requesting 5 results for 'MCP 架构' from Google API...\n",
      "Requesting 5 results for 'MCP 工作原理' from Google API...\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'google_search_tool', 'arguments': '{\"queries\": [\"MCP \\\\u4e3b\\\\u8981\\\\u7528\\\\u9014\", \"MCP applications\"]}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--79fcc0e2-f88e-4849-93d9-70200996e813-0', tool_calls=[{'name': 'google_search_tool', 'args': {'queries': ['MCP 主要用途', 'MCP applications']}, 'id': '6efd59bb-1ba2-45e7-b846-046fe8a42394', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1156, 'output_tokens': 23, 'total_tokens': 1526, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 347}})]}\n",
      "Requesting 5 results for 'MCP 主要用途' from Google API...\n",
      "Requesting 5 results for 'MCP applications' from Google API...\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'google_search_tool', 'arguments': '{\"queries\": [\"Model Context Protocol \\\\u672a\\\\u6765\\\\u53d1\\\\u5c55\", \"MCP\\\\u6280\\\\u672f\\\\u8d8b\\\\u52bf \\\\u4eba\\\\u5de5\\\\u667a\\\\u80fd\"]}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--0a771737-e5c5-404d-adc2-f2632008ca37-0', tool_calls=[{'name': 'google_search_tool', 'args': {'queries': ['Model Context Protocol 未来发展', 'MCP技术趋势 人工智能']}, 'id': '3be15457-e7dd-4f61-8ad5-848fcdf50f18', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1158, 'output_tokens': 29, 'total_tokens': 1456, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 269}})]}\n",
      "Requesting 5 results for 'Model Context Protocol 未来发展' from Google API...\n",
      "Requesting 5 results for 'MCP技术趋势 人工智能' from Google API...\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'google_search_tool', 'arguments': '{\"queries\": [\"Model Context Protocol basics\", \"What is Model Context Protocol (MCP)\"]}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--f9ff5c17-99ef-4039-9fb5-514db78ef959-0', tool_calls=[{'name': 'google_search_tool', 'args': {'queries': ['Model Context Protocol basics', 'What is Model Context Protocol (MCP)']}, 'id': '343cb45a-e3c3-4b91-93e6-b8d0990cb8b8', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1166, 'output_tokens': 30, 'total_tokens': 1521, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 325}})]}\n",
      "Requesting 5 results for 'Model Context Protocol basics' from Google API...\n",
      "Requesting 5 results for 'What is Model Context Protocol (MCP)' from Google API...\n",
      "Warning: Failed to fetch content for https://x.com/aigclink/status/1909514004684587267: 400, message='Got more than 8190 bytes (10166) when reading Header value is too long.', url='https://x.com/aigclink/status/1909514004684587267'\n",
      "Warning: Failed to fetch content for https://x.com/op7418/status/1892056670160949578: 400, message='Got more than 8190 bytes (10166) when reading Header value is too long.', url='https://x.com/op7418/status/1892056670160949578'\n",
      "Warning: Failed to fetch content for https://x.com/akshay_pachaar/status/1900456268084764689: 400, message='Got more than 8190 bytes (10166) when reading Header value is too long.', url='https://x.com/akshay_pachaar/status/1900456268084764689'\n",
      "Fetched full content for 5 results\n",
      "Fetched full content for 5 results\n",
      "Fetched full content for 5 results\n",
      "Fetched full content for 5 results\n",
      "Fetched full content for 5 results\n",
      "Fetched full content for 5 results\n",
      "Fetched full content for 5 results\n",
      "[Researcher] 消息序列修复: 在开头插入用户消息以满足提供商要求\n",
      "[Researcher] 消息序列修复: 在开头插入用户消息以满足提供商要求\n",
      "Fetched full content for 5 results\n",
      "[Researcher] 消息序列修复: 在开头插入用户消息以满足提供商要求\n",
      "Fetched full content for 5 results\n",
      "[Researcher] 消息序列修复: 在开头插入用户消息以满足提供商要求\n",
      "Warning: Failed to fetch content for https://www.cnblogs.com/fnng/p/18744210: \n",
      "Fetched full content for 5 results\n",
      "[Researcher] 消息序列修复: 在开头插入用户消息以满足提供商要求\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'FinishResearch', 'arguments': '{}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--590299a1-932d-4bab-bf69-581af4403f7f-0', tool_calls=[{'name': 'Section', 'args': {'name': '模型上下文协议（MCP）的基本概念', 'description': 'A section describing the basic concepts of the Model Context Protocol (MCP), including its purpose, architecture, key features, and adoption.', 'content': '## 模型上下文协议（MCP）的基本概念\\n\\n模型上下文协议（MCP）是由Anthropic于2024年11月推出的一种开放标准和开源框架，旨在标准化AI模型（如大型语言模型LLM）与外部工具、系统和数据源之间的数据集成和共享方式。它的核心目标是解决AI系统面临的信息孤岛和碎片化集成问题，提供一个通用的接口来读取文件、执行功能和处理上下文提示。\\n\\n**核心概念：**\\n*   **标准化集成：** MCP提供了一个统一的协议，取代了为每个数据源定制连接器的复杂性，简化了AI应用与数据的连接。\\n*   **客户端-服务器架构：** MCP遵循客户端-服务器模型。MCP服务器暴露数据源和能力（如文件系统、数据库），而MCP客户端（AI应用或代理）连接到这些服务器以获取所需上下文。\\n*   **主要功能：** MCP定义了三种基本功能：\\n    *   **提示（Prompts）：** 允许服务器提供结构化消息和指令，指导语言模型交互。\\n    *   **资源（Resources）：** 使服务器能够共享数据，如文件或数据库模式，为模型提供额外上下文。\\n    *   **工具（Tools）：** 允许服务器暴露可由语言模型调用的函数，使模型能够与外部系统交互。\\n\\nMCP的广泛采用，包括OpenAI和Google DeepMind等主要AI提供商，凸显了其作为AI系统连接和互操作性通用开放标准的潜力。\\n\\n### Sources\\n1.  https://www.anthropic.com/news/model-context-protocol\\n2.  https://modelcontextprotocol.io/introduction\\n3.  https://en.wikipedia.org/wiki/Model_Context_Protocol\\n4.  https://medium.com/@nimritakoul01/the-model-context-protocol-mcp-a-complete-tutorial-a3abe8a7f4ef'}, 'id': '42b72f76-9da2-40b6-aa69-1016d9792939', 'type': 'tool_call'}, {'name': 'FinishResearch', 'args': {}, 'id': 'ddb6267d-a791-4aa7-86cb-45cf7f641737', 'type': 'tool_call'}], usage_metadata={'input_tokens': 21841, 'output_tokens': 496, 'total_tokens': 22520, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 183}})]}\n",
      "[Researcher] 消息序列修复: 在开头插入用户消息以满足提供商要求, 合并了 2 个连续的 tool 消息, 插入 user 消息以维持交替模式\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'FinishResearch', 'arguments': '{}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--96f8ada8-8540-442c-ab18-ea18ec987b35-0', tool_calls=[{'name': 'FinishResearch', 'args': {}, 'id': '04744631-fe41-43d0-b0a0-0131fe042132', 'type': 'tool_call'}], usage_metadata={'input_tokens': 22840, 'output_tokens': 9, 'total_tokens': 22902, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 53}})]}\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'Section', 'arguments': '{\"name\": \"MCP\\\\u7684\\\\u4f18\\\\u52bf\\\\u548c\\\\u5e94\\\\u7528\\\\u573a\\\\u666f\", \"description\": \"This section details the advantages and application scenarios of Model Context Protocol (MCP).\", \"content\": \"## MCP\\\\u7684\\\\u4f18\\\\u52bf\\\\u548c\\\\u5e94\\\\u7528\\\\u573a\\\\u666f\\\\n\\\\nMCP\\\\uff08\\\\u6a21\\\\u578b\\\\u4e0a\\\\u4e0b\\\\u6587\\\\u534f\\\\u8bae\\\\uff09\\\\u65e8\\\\u5728\\\\u4f5c\\\\u4e3aAI\\\\u6a21\\\\u578b\\\\u7684\\\\u201c\\\\u4e07\\\\u80fd\\\\u8f6c\\\\u63a5\\\\u5934\\\\u201d\\\\uff0c\\\\u4f7f\\\\u5176\\\\u80fd\\\\u591f\\\\u8f7b\\\\u677e\\\\u83b7\\\\u53d6\\\\u6570\\\\u636e\\\\u6216\\\\u8c03\\\\u7528\\\\u5de5\\\\u5177\\\\u3002\\\\u5176\\\\u4e3b\\\\u8981\\\\u4f18\\\\u52bf\\\\u5305\\\\u62ec\\\\uff1a\\\\n\\\\n*   **\\\\u751f\\\\u6001\\\\u4e0e\\\\u6548\\\\u7387\\\\uff1a** \\\\u63d0\\\\u4f9b\\\\u5927\\\\u91cf\\\\u73b0\\\\u6709\\\\u63d2\\\\u4ef6\\\\uff0c\\\\u907f\\\\u514d\\\\u91cd\\\\u590d\\\\u5f00\\\\u53d1\\\\uff0c\\\\u5e76\\\\u652f\\\\u6301\\\\u5f00\\\\u653e\\\\u6807\\\\u51c6\\\\uff0c\\\\u5229\\\\u4e8e\\\\u670d\\\\u52a1\\\\u5546\\\\u5f00\\\\u53d1API\\\\u3002\\\\n*   **\\\\u7cbe\\\\u7ec6\\\\u63a7\\\\u5236\\\\uff1a** \\\\u5bf9AI\\\\u601d\\\\u7ef4\\\\u8fc7\\\\u7a0b\\\\u63d0\\\\u4f9b\\\\u66f4\\\\u7cbe\\\\u7ec6\\\\u7684\\\\u63a7\\\\u5236\\\\uff0c\\\\u5b9e\\\\u73b0\\\\u66f4\\\\u7cbe\\\\u51c6\\\\u7684\\\\u64cd\\\\u4f5c\\\\u3002\\\\n*   **\\\\u6613\\\\u7528\\\\u6027\\\\u4e0e\\\\u5b89\\\\u5168\\\\u6027\\\\uff1a** \\\\u63d0\\\\u4f9b\\\\u5f00\\\\u7bb1\\\\u5373\\\\u7528\\\\u7684\\\\u670d\\\\u52a1\\\\u5668\\\\u548c\\\\u53ef\\\\u89c6\\\\u5316\\\\u914d\\\\u7f6e\\\\uff0c\\\\u65e0\\\\u9700\\\\u7f16\\\\u7801\\\\uff0c\\\\u540c\\\\u65f6\\\\u672c\\\\u5730\\\\u670d\\\\u52a1\\\\u5668\\\\u8fdb\\\\u7a0b\\\\u63a7\\\\u5236\\\\u6570\\\\u636e\\\\u8bbf\\\\u95ee\\\\uff0c\\\\u4fdd\\\\u969c\\\\u5b89\\\\u5168\\\\u6027\\\\u3002\\\\n*   **\\\\u6269\\\\u5c55\\\\u6027\\\\uff1a** \\\\u80fd\\\\u591f\\\\u65e0\\\\u7f1d\\\\u63a5\\\\u5165\\\\u4f01\\\\u4e1a\\\\u77e5\\\\u8bc6\\\\u5e93\\\\uff0c\\\\u5982\\\\u8bbe\\\\u8ba1\\\\u8ba8\\\\u8bba\\\\u548c\\\\u5185\\\\u90e8\\\\u6587\\\\u6863\\\\u3002\\\\n\\\\nMCP\\\\u7684\\\\u5e94\\\\u7528\\\\u573a\\\\u666f\\\\u5e7f\\\\u6cdb\\\\uff0c\\\\u5305\\\\u62ec\\\\uff1a\\\\n\\\\n*   **\\\\u884c\\\\u7a0b\\\\u89c4\\\\u5212\\\\u52a9\\\\u624b\\\\uff1a** AI\\\\u52a9\\\\u624b\\\\u53ef\\\\u65e0\\\\u7f1d\\\\u68c0\\\\u67e5\\\\u65e5\\\\u5386\\\\u3001\\\\u9884\\\\u8ba2\\\\u822a\\\\u73ed\\\\u3001\\\\u53d1\\\\u9001\\\\u786e\\\\u8ba4\\\\u90ae\\\\u4ef6\\\\u3002\\\\n*   **\\\\u9ad8\\\\u7ea7IDE\\\\uff1a** \\\\u667a\\\\u80fd\\\\u4ee3\\\\u7801\\\\u7f16\\\\u8f91\\\\u5668\\\\u901a\\\\u8fc7MCP\\\\u8fde\\\\u63a5\\\\u6587\\\\u4ef6\\\\u7cfb\\\\u7edf\\\\u3001\\\\u7248\\\\u672c\\\\u63a7\\\\u5236\\\\u7b49\\\\uff0c\\\\u63d0\\\\u4f9b\\\\u66f4\\\\u667a\\\\u80fd\\\\u7684\\\\u4ee3\\\\u7801\\\\u5efa\\\\u8bae\\\\u3002\\\\n*   **\\\\u590d\\\\u6742\\\\u6570\\\\u636e\\\\u5206\\\\u6790\\\\uff1a** AI\\\\u5206\\\\u6790\\\\u5e73\\\\u53f0\\\\u53ef\\\\u81ea\\\\u52a8\\\\u53d1\\\\u73b0\\\\u5e76\\\\u4e0e\\\\u591a\\\\u4e2a\\\\u6570\\\\u636e\\\\u5e93\\\\u548c\\\\u53ef\\\\u89c6\\\\u5316\\\\u5de5\\\\u5177\\\\u4ea4\\\\u4e92\\\\u3002\\\\n*   **AI\\\\u5916\\\\u547c\\\\u670d\\\\u52a1\\\\uff1a** \\\\u964d\\\\u4f4e\\\\u8bed\\\\u97f3\\\\u514b\\\\u9686\\\\u670d\\\\u52a1\\\\u95e8\\\\u69db\\\\uff0c\\\\u5b9e\\\\u73b0\\\\u81ea\\\\u7136\\\\u8bed\\\\u8a00\\\\u6784\\\\u5efaAI\\\\u5916\\\\u547c\\\\u3002\\\\n*   **\\\\u4ee3\\\\u7801\\\\u5f00\\\\u53d1\\\\uff1a** \\\\u5141\\\\u8bb8LLM\\\\u5b8c\\\\u5168\\\\u8bbf\\\\u95eeIDE\\\\uff0c\\\\u5b9e\\\\u73b0\\\\u4ece\\\\u9700\\\\u6c42\\\\u5206\\\\u6790\\\\u5230\\\\u529f\\\\u80fd\\\\u4e0a\\\\u7ebf\\\\u7684\\\\u5168\\\\u6d41\\\\u7a0b\\\\u81ea\\\\u52a8\\\\u5316\\\\u3002\\\\n\\\\n### Sources\\\\n1.  https://zhuanlan.zhihu.com/p/29001189476\\\\n2.  https://www.reddit.com/r/ClaudeAI/comments/1j6zi68/sequential_thinking_mcp_vs_claude_37_extended/?tl=zh-hans\\\\n3.  https://zhuanlan.zhihu.com/p/27327515233\\\\n4.  https://x.com/op7418/status/1892056670160949578\\\\n5.  https://zhuanlan.zhihu.com/p/1890326398637417398\\\\n6.  https://x.com/aigclink/status/1909514004684587267\\\\n7.  https://www.reddit.com/r/mcp/comments/1j6y8ct/what_are_some_of_the_best_current_use_cases_for/?tl=zh-hans\\\\n8.  https://www.cnblogs.com/ytkah/p/18763320/mcp\\\\n9.  https://github.com/yzfly/Awesome-MCP-ZH\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--45e6c782-c591-4887-90b1-3d8f25fdb0a8-0', tool_calls=[{'name': 'Section', 'args': {'name': 'MCP的优势和应用场景', 'description': 'This section details the advantages and application scenarios of Model Context Protocol (MCP).', 'content': '## MCP的优势和应用场景\\n\\nMCP（模型上下文协议）旨在作为AI模型的“万能转接头”，使其能够轻松获取数据或调用工具。其主要优势包括：\\n\\n*   **生态与效率：** 提供大量现有插件，避免重复开发，并支持开放标准，利于服务商开发API。\\n*   **精细控制：** 对AI思维过程提供更精细的控制，实现更精准的操作。\\n*   **易用性与安全性：** 提供开箱即用的服务器和可视化配置，无需编码，同时本地服务器进程控制数据访问，保障安全性。\\n*   **扩展性：** 能够无缝接入企业知识库，如设计讨论和内部文档。\\n\\nMCP的应用场景广泛，包括：\\n\\n*   **行程规划助手：** AI助手可无缝检查日历、预订航班、发送确认邮件。\\n*   **高级IDE：** 智能代码编辑器通过MCP连接文件系统、版本控制等，提供更智能的代码建议。\\n*   **复杂数据分析：** AI分析平台可自动发现并与多个数据库和可视化工具交互。\\n*   **AI外呼服务：** 降低语音克隆服务门槛，实现自然语言构建AI外呼。\\n*   **代码开发：** 允许LLM完全访问IDE，实现从需求分析到功能上线的全流程自动化。\\n\\n### Sources\\n1.  https://zhuanlan.zhihu.com/p/29001189476\\n2.  https://www.reddit.com/r/ClaudeAI/comments/1j6zi68/sequential_thinking_mcp_vs_claude_37_extended/?tl=zh-hans\\n3.  https://zhuanlan.zhihu.com/p/27327515233\\n4.  https://x.com/op7418/status/1892056670160949578\\n5.  https://zhuanlan.zhihu.com/p/1890326398637417398\\n6.  https://x.com/aigclink/status/1909514004684587267\\n7.  https://www.reddit.com/r/mcp/comments/1j6y8ct/what_are_some_of_the_best_current_use_cases_for/?tl=zh-hans\\n8.  https://www.cnblogs.com/ytkah/p/18763320/mcp\\n9.  https://github.com/yzfly/Awesome-MCP-ZH'}, 'id': 'b6f232ab-83c9-4fed-aa3e-9ca05ae95601', 'type': 'tool_call'}], usage_metadata={'input_tokens': 27549, 'output_tokens': 668, 'total_tokens': 28404, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 187}})]}\n",
      "[Researcher] 消息序列修复: 在开头插入用户消息以满足提供商要求, 插入 user 消息以维持交替模式\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'FinishResearch', 'arguments': '{}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--4d520d99-ab72-4af4-8411-e26cafdd904a-0', tool_calls=[{'name': 'Section', 'args': {'name': 'MCP的主要用途', 'description': '本节介绍了模型上下文协议（MCP）的主要用途，包括连接LLM与外部数据和工具、增强AI助手和Agentic AI应用、标准化集成以及AI辅助编程。', 'content': '## MCP的主要用途\\n\\n模型上下文协议 (MCP) 是一个开放标准，旨在彻底改变大型语言模型 (LLM) 与外部世界互动的方式。其主要用途和应用包括：\\n\\n*   **连接LLM与外部数据和工具**：MCP 充当 AI 应用程序的“USB-C 接口”，使 LLM 能够无缝访问各种数据源（如代码库、业务工具）和执行操作（如网络搜索、文件管理）。\\n*   **增强AI助手和Agentic AI应用**：通过提供上下文信息，MCP 帮助 AI 助手和代理执行任务、调用外部 API 并提供更准确和相关的响应，超越其训练数据。这简化了复杂 AI 工作流和多代理系统的构建和管理。\\n*   **标准化集成**：MCP 提供了一种统一的方法来连接 AI 系统与数据源，取代了碎片化的定制集成，从而实现更简单、更可靠的 AI 应用架构。\\n*   **AI辅助编程**：在开发环境中，MCP 助力 AI 代理检索相关信息，理解编码任务，并生成更精确的代码。\\n\\n### Sources\\n1.  https://github.com/liaokongVFX/MCP-Chinese-Getting-Started-Guide\\n2.  https://www.threads.com/@mr.__.l/post/DHIHf5PBtyb/%E4%B8%BB%E8%A6%81%E7%94%A8%E9%80%94%E8%88%87%E6%87%89%E7%94%A8%E5%A0%B4%E6%99%AFmcp-%E7%9A%84%E7%94%A8%E9%80%94%E5%9C%A8%E6%96%BC%E7%82%BA-ai-%E9%96%8B%E7%99%BC%E8%80%85%E8%88%87%E7%B3%BB%E7%B5%B1%E6%95%B4%E5%90%88%E6%8F%90%E4%BE%9B%E4%BE%BF%E5%88%A9%E8%97%89%E7%94%B1-mcp%E9%96%8B%E7%99%BC%E8%80%85%E5%8F%AF%E4%BB%A5%E8%AE%93%E4%BB%BB%E6%84%8F%E7%AC%A6%E5%90%88%E8%A6%8F%E7%AF%84%E7%9A%84-llm-%E7%8D%B7%E5%BE%97%E5%8B%95%E6%89%8B%E8%83%BD%E5%8A%9B%E5%9C%A8%E5%90%84%E7%A8%AE%E7%92%B0%E5%A2%83%E4%B8%AD%E5%9F%B7%E8%A1%8C%E6%93%8D%E4%BD%9C%E6%88%96%E8%AA%BF%E7%94%A8%E8%B3%87%E6%96%99\\n3.  https://aws.amazon.com/cn/blogs/china/practical-guide-to-building-agentic-ai-applications-for-aws-china-region/\\n4.  https://medium.com/@amosgyamfi/the-top-7-mcp-supported-ai-frameworks-a8e5030c87ab\\n5.  https://www.anthropic.com/news/model-context-protocol\\n6.  https://docs.anthropic.com/en/docs/agents-and-tools/mcp'}, 'id': 'c0316cb9-35e8-4561-a5b0-4b135253401a', 'type': 'tool_call'}, {'name': 'FinishResearch', 'args': {}, 'id': 'a5f122f7-9bb5-429f-81ff-4a2dc7e640e3', 'type': 'tool_call'}], usage_metadata={'input_tokens': 39048, 'output_tokens': 1050, 'total_tokens': 40673, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 575}})]}\n",
      "[Researcher] 消息序列修复: 在开头插入用户消息以满足提供商要求, 合并了 2 个连续的 tool 消息, 插入 user 消息以维持交替模式\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'Section', 'arguments': '{\"name\": \"MCP\\\\u7684\\\\u67b6\\\\u6784\\\\u548c\\\\u5de5\\\\u4f5c\\\\u539f\\\\u7406\", \"description\": \"\\\\u672c\\\\u8282\\\\u4ecb\\\\u7ecd\\\\u4e86\\\\u6a21\\\\u578b\\\\u4e0a\\\\u4e0b\\\\u6587\\\\u534f\\\\u8bae\\\\uff08MCP\\\\uff09\\\\u7684\\\\u67b6\\\\u6784\\\\u548c\\\\u5de5\\\\u4f5c\\\\u539f\\\\u7406\\\\uff0c\\\\u5305\\\\u62ec\\\\u5176\\\\u5ba2\\\\u6237\\\\u7aef-\\\\u670d\\\\u52a1\\\\u5668\\\\u6a21\\\\u578b\\\\u548cLLM\\\\u5982\\\\u4f55\\\\u901a\\\\u8fc7MCP\\\\u4e0e\\\\u5916\\\\u90e8\\\\u6570\\\\u636e\\\\u548c\\\\u5de5\\\\u5177\\\\u4ea4\\\\u4e92\\\\u3002\", \"content\": \"## MCP\\\\u7684\\\\u67b6\\\\u6784\\\\u548c\\\\u5de5\\\\u4f5c\\\\u539f\\\\u7406\\\\n\\\\n\\\\u6a21\\\\u578b\\\\u4e0a\\\\u4e0b\\\\u6587\\\\u534f\\\\u8bae\\\\uff08MCP\\\\uff09\\\\u662f\\\\u4e00\\\\u4e2a\\\\u5f00\\\\u653e\\\\u534f\\\\u8bae\\\\uff0c\\\\u65e8\\\\u5728\\\\u6807\\\\u51c6\\\\u5316\\\\u5e94\\\\u7528\\\\u7a0b\\\\u5e8f\\\\u5982\\\\u4f55\\\\u5411\\\\u5927\\\\u578b\\\\u8bed\\\\u8a00\\\\u6a21\\\\u578b\\\\uff08LLM\\\\uff09\\\\u63d0\\\\u4f9b\\\\u4e0a\\\\u4e0b\\\\u6587\\\\u4fe1\\\\u606f\\\\u3002\\\\u5b83\\\\u7c7b\\\\u4f3c\\\\u4e8eAI\\\\u4e16\\\\u754c\\\\u7684USB-C\\\\u7aef\\\\u53e3\\\\uff0c\\\\u5141\\\\u8bb8AI\\\\u6a21\\\\u578b\\\\u4e0e\\\\u5404\\\\u79cd\\\\u6570\\\\u636e\\\\u6e90\\\\u548c\\\\u5de5\\\\u5177\\\\u8fdb\\\\u884c\\\\u6807\\\\u51c6\\\\u5316\\\\u8fde\\\\u63a5\\\\u3002\\\\n\\\\n### \\\\u67b6\\\\u6784\\\\nMCP\\\\u9075\\\\u5faa\\\\u5ba2\\\\u6237\\\\u7aef-\\\\u670d\\\\u52a1\\\\u5668\\\\u67b6\\\\u6784\\\\uff0c\\\\u4e3b\\\\u8981\\\\u89d2\\\\u8272\\\\u5305\\\\u62ec\\\\uff1a\\\\n*   **MCP\\\\u4e3b\\\\u673a\\\\uff1a** \\\\u5e0c\\\\u671b\\\\u901a\\\\u8fc7MCP\\\\u8bbf\\\\u95ee\\\\u6570\\\\u636e\\\\u7684\\\\u5e94\\\\u7528\\\\u7a0b\\\\u5e8f\\\\uff08\\\\u5982AI\\\\u5de5\\\\u5177\\\\u3001IDE\\\\uff09\\\\u3002\\\\n*   **MCP\\\\u5ba2\\\\u6237\\\\u7aef\\\\uff1a** \\\\u5d4c\\\\u5165\\\\u5728LLM\\\\u4e2d\\\\uff0c\\\\u4e0eMCP\\\\u670d\\\\u52a1\\\\u5668\\\\u4fdd\\\\u6301\\\\u8fde\\\\u63a5\\\\uff0c\\\\u8d1f\\\\u8d23\\\\u8f6c\\\\u53d1\\\\u8bf7\\\\u6c42\\\\u548c\\\\u8fd4\\\\u56de\\\\u7ed3\\\\u679c\\\\u3002\\\\n*   **MCP\\\\u670d\\\\u52a1\\\\u5668\\\\uff1a** \\\\u8f7b\\\\u91cf\\\\u7ea7\\\\u7a0b\\\\u5e8f\\\\uff0c\\\\u901a\\\\u8fc7\\\\u6807\\\\u51c6\\\\u5316\\\\u534f\\\\u8bae\\\\u66b4\\\\u9732\\\\u7279\\\\u5b9a\\\\u529f\\\\u80fd\\\\uff08\\\\u5de5\\\\u5177\\\\u548c\\\\u6570\\\\u636e\\\\u8bbf\\\\u95ee\\\\uff09\\\\uff0c\\\\u53ef\\\\u8bbf\\\\u95ee\\\\u672c\\\\u5730\\\\u6570\\\\u636e\\\\u6e90\\\\u6216\\\\u8fdc\\\\u7a0b\\\\u670d\\\\u52a1\\\\u3002\\\\n\\\\n### \\\\u5de5\\\\u4f5c\\\\u539f\\\\u7406\\\\n\\\\u5f53LLM\\\\u9700\\\\u8981\\\\u5916\\\\u90e8\\\\u4fe1\\\\u606f\\\\u6216\\\\u6267\\\\u884c\\\\u7279\\\\u5b9a\\\\u64cd\\\\u4f5c\\\\u65f6\\\\uff0c\\\\u5b83\\\\u4f1a\\\\u901a\\\\u8fc7MCP\\\\u5ba2\\\\u6237\\\\u7aef\\\\u8c03\\\\u7528MCP\\\\u670d\\\\u52a1\\\\u5668\\\\u4e0a\\\\u5b9a\\\\u4e49\\\\u7684\\\\u201c\\\\u5de5\\\\u5177\\\\u201d\\\\u3002\\\\u8fd9\\\\u4e9b\\\\u5de5\\\\u5177\\\\u662f\\\\u53ef\\\\u6267\\\\u884c\\\\u51fd\\\\u6570\\\\uff0c\\\\u80fd\\\\u591f\\\\u4ece\\\\u672c\\\\u5730\\\\u6587\\\\u4ef6\\\\u3001\\\\u6570\\\\u636e\\\\u5e93\\\\u6216\\\\u8fdc\\\\u7a0bAPI\\\\u4e2d\\\\u68c0\\\\u7d22\\\\u6570\\\\u636e\\\\u6216\\\\u6267\\\\u884c\\\\u64cd\\\\u4f5c\\\\u3002\\\\u670d\\\\u52a1\\\\u5668\\\\u5904\\\\u7406\\\\u8bf7\\\\u6c42\\\\u5e76\\\\u5c06\\\\u7ed3\\\\u679c\\\\u8fd4\\\\u56de\\\\u7ed9LLM\\\\uff0cLLM\\\\u518d\\\\u7ed3\\\\u5408\\\\u8fd9\\\\u4e9b\\\\u7ed3\\\\u679c\\\\u751f\\\\u6210\\\\u54cd\\\\u5e94\\\\u3002\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--6f42f294-1a62-4cd1-a21d-b42e3e83a97a-0', tool_calls=[{'name': 'Section', 'args': {'name': 'MCP的架构和工作原理', 'description': '本节介绍了模型上下文协议（MCP）的架构和工作原理，包括其客户端-服务器模型和LLM如何通过MCP与外部数据和工具交互。', 'content': '## MCP的架构和工作原理\\n\\n模型上下文协议（MCP）是一个开放协议，旨在标准化应用程序如何向大型语言模型（LLM）提供上下文信息。它类似于AI世界的USB-C端口，允许AI模型与各种数据源和工具进行标准化连接。\\n\\n### 架构\\nMCP遵循客户端-服务器架构，主要角色包括：\\n*   **MCP主机：** 希望通过MCP访问数据的应用程序（如AI工具、IDE）。\\n*   **MCP客户端：** 嵌入在LLM中，与MCP服务器保持连接，负责转发请求和返回结果。\\n*   **MCP服务器：** 轻量级程序，通过标准化协议暴露特定功能（工具和数据访问），可访问本地数据源或远程服务。\\n\\n### 工作原理\\n当LLM需要外部信息或执行特定操作时，它会通过MCP客户端调用MCP服务器上定义的“工具”。这些工具是可执行函数，能够从本地文件、数据库或远程API中检索数据或执行操作。服务器处理请求并将结果返回给LLM，LLM再结合这些结果生成响应。'}, 'id': '437a19a7-9223-4bcb-bb2b-8b9bd2a4aa27', 'type': 'tool_call'}], usage_metadata={'input_tokens': 15328, 'output_tokens': 296, 'total_tokens': 15986, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 362}})]}\n",
      "[Researcher] 消息序列修复: 在开头插入用户消息以满足提供商要求, 插入 user 消息以维持交替模式\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'FinishResearch', 'arguments': '{}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--7fb81110-bed2-44f4-a86a-18420284a78c-0', tool_calls=[{'name': 'FinishResearch', 'args': {}, 'id': '0d3d6ec2-7469-4e32-b93c-301fcede51ee', 'type': 'tool_call'}], usage_metadata={'input_tokens': 28902, 'output_tokens': 9, 'total_tokens': 28965, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 54}})]}\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'FinishResearch', 'arguments': '{}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--29cd09d8-7d54-4488-acc7-370435ce7e0b-0', tool_calls=[{'name': 'FinishResearch', 'args': {}, 'id': '0f590096-d0ea-4226-a00b-57ba537729d5', 'type': 'tool_call'}], usage_metadata={'input_tokens': 15933, 'output_tokens': 9, 'total_tokens': 16022, 'input_token_details': {'cache_read': 15147}, 'output_token_details': {'reasoning': 80}})]}\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'FinishResearch', 'arguments': '{}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--5df80657-8e9a-4617-9a0b-eaea360bfe82-0', tool_calls=[{'name': 'FinishResearch', 'args': {}, 'id': '6322bbbf-ba7b-4233-b8c4-183b4b3f7c34', 'type': 'tool_call'}], usage_metadata={'input_tokens': 41152, 'output_tokens': 9, 'total_tokens': 41226, 'input_token_details': {'cache_read': 38693}, 'output_token_details': {'reasoning': 65}})]}\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'Section', 'arguments': '{\"name\": \"MCP\\\\u7684\\\\u672a\\\\u6765\\\\u53d1\\\\u5c55\", \"description\": \"\\\\u7814\\\\u7a76\\\\u4e86\\\\u6a21\\\\u578b\\\\u4e0a\\\\u4e0b\\\\u6587\\\\u534f\\\\u8bae\\\\uff08MCP\\\\uff09\\\\u5728\\\\u4eba\\\\u5de5\\\\u667a\\\\u80fd\\\\u9886\\\\u57df\\\\u7684\\\\u672a\\\\u6765\\\\u53d1\\\\u5c55\\\\u8d8b\\\\u52bf\\\\uff0c\\\\u5305\\\\u62ec\\\\u884c\\\\u4e1a\\\\u5e94\\\\u7528\\\\u3001\\\\u670d\\\\u52a1\\\\u5316\\\\u3001\\\\u4e91\\\\u5e73\\\\u53f0\\\\u96c6\\\\u6210\\\\u3001AI\\\\u4ee3\\\\u7406\\\\u80fd\\\\u529b\\\\u589e\\\\u5f3a\\\\u4ee5\\\\u53ca\\\\u6807\\\\u51c6\\\\u5316\\\\u548c\\\\u5b89\\\\u5168\\\\u6027\\\\u3002\", \"content\": \"## MCP\\\\u7684\\\\u672a\\\\u6765\\\\u53d1\\\\u5c55\\\\n\\\\n\\\\u6a21\\\\u578b\\\\u4e0a\\\\u4e0b\\\\u6587\\\\u534f\\\\u8bae (MCP) \\\\u4f5c\\\\u4e3aAI\\\\u4ee3\\\\u7406\\\\u6280\\\\u672f\\\\u7684\\\\u91cd\\\\u8981\\\\u7a81\\\\u7834\\\\uff0c\\\\u6b63\\\\u5f7b\\\\u5e95\\\\u6539\\\\u53d8AI\\\\u4e0e\\\\u5916\\\\u90e8\\\\u4e16\\\\u754c\\\\u7684\\\\u4e92\\\\u52a8\\\\u65b9\\\\u5f0f\\\\u3002\\\\u5176\\\\u672a\\\\u6765\\\\u53d1\\\\u5c55\\\\u8d8b\\\\u52bf\\\\u4e3b\\\\u8981\\\\u4f53\\\\u73b0\\\\u5728\\\\u4ee5\\\\u4e0b\\\\u51e0\\\\u4e2a\\\\u65b9\\\\u9762\\\\uff1a\\\\n\\\\n*   **\\\\u5e7f\\\\u6cdb\\\\u7684\\\\u884c\\\\u4e1a\\\\u5e94\\\\u7528\\\\uff1a** \\\\u5c06\\\\u51fa\\\\u73b0\\\\u9488\\\\u5bf9\\\\u533b\\\\u7597\\\\u3001\\\\u91d1\\\\u878d\\\\u3001\\\\u6559\\\\u80b2\\\\u7b49\\\\u7279\\\\u5b9a\\\\u884c\\\\u4e1a\\\\u7684MCP\\\\u670d\\\\u52a1\\\\u5668\\\\uff0c\\\\u5b9e\\\\u73b0AI\\\\u4e0e\\\\u5782\\\\u76f4\\\\u9886\\\\u57df\\\\u6570\\\\u636e\\\\u7684\\\\u6df1\\\\u5ea6\\\\u878d\\\\u5408\\\\u3002\\\\n*   **\\\\u670d\\\\u52a1\\\\u5316\\\\u4e0e\\\\u751f\\\\u6001\\\\u6269\\\\u5c55\\\\uff1a** \\\\u9884\\\\u8ba1\\\\u5c06\\\\u6d8c\\\\u73b0MCP\\\\u5373\\\\u670d\\\\u52a1\\\\uff08MCP-as-a-Service\\\\uff09\\\\u7684\\\\u5546\\\\u4e1a\\\\u6a21\\\\u5f0f\\\\uff0c\\\\u5e76\\\\u5f62\\\\u6210\\\\u591a\\\\u516c\\\\u53f8\\\\u8054\\\\u76df\\\\u5171\\\\u540c\\\\u63a8\\\\u52a8\\\\u534f\\\\u8bae\\\\u7684\\\\u8fdb\\\\u4e00\\\\u6b65\\\\u53d1\\\\u5c55\\\\u3002AIbase\\\\u7b49\\\\u5e73\\\\u53f0\\\\u6b63\\\\u5728\\\\u6784\\\\u5efaMCP\\\\u8d44\\\\u6e90\\\\u4ed3\\\\u5e93\\\\uff0c\\\\u4fc3\\\\u8fdb\\\\u751f\\\\u6001\\\\u7cfb\\\\u7edf\\\\u5efa\\\\u8bbe\\\\u3002\\\\n*   **\\\\u4e91\\\\u5e73\\\\u53f0\\\\u6df1\\\\u5ea6\\\\u96c6\\\\u6210\\\\uff1a** \\\\u4e3b\\\\u8981AI\\\\u5e73\\\\u53f0\\\\u548c\\\\u4e91\\\\u8ba1\\\\u7b97\\\\u63d0\\\\u4f9b\\\\u5546\\\\uff08\\\\u5982AWS\\\\u3001Cloudflare\\\\uff09\\\\u5c06\\\\u66f4\\\\u5e7f\\\\u6cdb\\\\u5730\\\\u96c6\\\\u6210MCP\\\\uff0c\\\\u652f\\\\u6301\\\\u8fdc\\\\u7a0bMCP\\\\u670d\\\\u52a1\\\\u5668\\\\u90e8\\\\u7f72\\\\uff0c\\\\u4f7fAI\\\\u4ee3\\\\u7406\\\\u80fd\\\\u5b89\\\\u5168\\\\u3001\\\\u9ad8\\\\u6548\\\\u5730\\\\u8bbf\\\\u95ee\\\\u548c\\\\u64cd\\\\u4f5c\\\\u5916\\\\u90e8\\\\u670d\\\\u52a1\\\\u3002\\\\n*   **\\\\u589e\\\\u5f3a\\\\u578bAI\\\\u4ee3\\\\u7406\\\\u80fd\\\\u529b\\\\uff1a** MCP\\\\u5c06\\\\u4fc3\\\\u4f7fAI\\\\u4ece\\\\u88ab\\\\u52a8\\\\u54cd\\\\u5e94\\\\u8005\\\\u8f6c\\\\u53d8\\\\u4e3a\\\\u4e3b\\\\u52a8\\\\u4efb\\\\u52a1\\\\u6267\\\\u884c\\\\u8005\\\\uff0c\\\\u5b9e\\\\u73b0\\\\u8de8\\\\u5de5\\\\u5177\\\\u534f\\\\u4f5c\\\\u3001\\\\u81ea\\\\u4e3b\\\\u4efb\\\\u52a1\\\\u6267\\\\u884c\\\\u548c\\\\u5f39\\\\u6027\\\\u6a21\\\\u5757\\\\u96c6\\\\u6210\\\\uff0c\\\\u4f8b\\\\u5982\\\\u81ea\\\\u52a8\\\\u5316\\\\u62a5\\\\u544a\\\\u3001\\\\u667a\\\\u80fd\\\\u5bb6\\\\u5c45\\\\u63a7\\\\u5236\\\\u548c\\\\u4e2a\\\\u4ebaAI\\\\u79d8\\\\u4e66\\\\u7b49\\\\u3002\\\\n*   **\\\\u6807\\\\u51c6\\\\u5316\\\\u4e0e\\\\u5b89\\\\u5168\\\\u6027\\\\uff1a** \\\\u4f5c\\\\u4e3a\\\\u5f00\\\\u653e\\\\u6807\\\\u51c6\\\\uff0cMCP\\\\u5c06\\\\u6301\\\\u7eed\\\\u63a8\\\\u52a8AI\\\\u4e0e\\\\u5916\\\\u90e8\\\\u8d44\\\\u6e90\\\\u901a\\\\u4fe1\\\\u7684\\\\u6807\\\\u51c6\\\\u5316\\\\uff0c\\\\u540c\\\\u65f6\\\\u901a\\\\u8fc7\\\\u5185\\\\u7f6e\\\\u7684\\\\u8bbf\\\\u95ee\\\\u63a7\\\\u5236\\\\u548c\\\\u6388\\\\u6743\\\\u673a\\\\u5236\\\\uff0c\\\\u786e\\\\u4fdd\\\\u6570\\\\u636e\\\\u5b89\\\\u5168\\\\u548c\\\\u9690\\\\u79c1\\\\u3002\\\\n\\\\n### Sources\\\\n1. https://ikala.ai/zh-tw/blog/ikala-ai-insight/what-is-model-context-protocol-mcp/\\\\n2. https://www.ibest.com.tw/news-detail/what-is-mcp/\\\\n3. https://www.cloudflare.com/zh-cn/press-releases/2025/cloudflare-accelerates-ai-agent-development-remote-mcp/\\\\n4. https://aws.amazon.com/cn/blogs/china/practical-guide-to-building-agentic-ai-applications-for-aws-china-region/\\\\n5. https://news.qq.com/rain/a/20250523A05RLD00\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--2d00d2d1-c900-4786-b6e0-fc2bf10191a8-0', tool_calls=[{'name': 'Section', 'args': {'name': 'MCP的未来发展', 'description': '研究了模型上下文协议（MCP）在人工智能领域的未来发展趋势，包括行业应用、服务化、云平台集成、AI代理能力增强以及标准化和安全性。', 'content': '## MCP的未来发展\\n\\n模型上下文协议 (MCP) 作为AI代理技术的重要突破，正彻底改变AI与外部世界的互动方式。其未来发展趋势主要体现在以下几个方面：\\n\\n*   **广泛的行业应用：** 将出现针对医疗、金融、教育等特定行业的MCP服务器，实现AI与垂直领域数据的深度融合。\\n*   **服务化与生态扩展：** 预计将涌现MCP即服务（MCP-as-a-Service）的商业模式，并形成多公司联盟共同推动协议的进一步发展。AIbase等平台正在构建MCP资源仓库，促进生态系统建设。\\n*   **云平台深度集成：** 主要AI平台和云计算提供商（如AWS、Cloudflare）将更广泛地集成MCP，支持远程MCP服务器部署，使AI代理能安全、高效地访问和操作外部服务。\\n*   **增强型AI代理能力：** MCP将促使AI从被动响应者转变为主动任务执行者，实现跨工具协作、自主任务执行和弹性模块集成，例如自动化报告、智能家居控制和个人AI秘书等。\\n*   **标准化与安全性：** 作为开放标准，MCP将持续推动AI与外部资源通信的标准化，同时通过内置的访问控制和授权机制，确保数据安全和隐私。\\n\\n### Sources\\n1. https://ikala.ai/zh-tw/blog/ikala-ai-insight/what-is-model-context-protocol-mcp/\\n2. https://www.ibest.com.tw/news-detail/what-is-mcp/\\n3. https://www.cloudflare.com/zh-cn/press-releases/2025/cloudflare-accelerates-ai-agent-development-remote-mcp/\\n4. https://aws.amazon.com/cn/blogs/china/practical-guide-to-building-agentic-ai-applications-for-aws-china-region/\\n5. https://news.qq.com/rain/a/20250523A05RLD00'}, 'id': '4db06222-902c-44aa-92ec-cc1c6e72f0cb', 'type': 'tool_call'}], usage_metadata={'input_tokens': 38140, 'output_tokens': 517, 'total_tokens': 40065, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1408}})]}\n",
      "[Researcher] 消息序列修复: 在开头插入用户消息以满足提供商要求, 插入 user 消息以维持交替模式\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'FinishResearch', 'arguments': '{}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--0964ee42-7aed-4d26-b977-642c2a50aefb-0', tool_calls=[{'name': 'FinishResearch', 'args': {}, 'id': '400848ed-6e47-474f-b190-8aeb0bcd496a', 'type': 'tool_call'}], usage_metadata={'input_tokens': 39192, 'output_tokens': 9, 'total_tokens': 39278, 'input_token_details': {'cache_read': 37665}, 'output_token_details': {'reasoning': 77}})]}\n",
      "[Supervisor] 消息序列修复: 插入 user 消息以维持交替模式\n",
      "[Supervisor] 消息序列修复: 插入 user 消息以维持交替模式, 插入 user 消息以维持交替模式\n",
      "[Supervisor] 消息序列修复: 插入 user 消息以维持交替模式, 插入 user 消息以维持交替模式\n",
      "✅ 工作流执行成功！网络连接问题已修复\n",
      "response:  {'messages': [HumanMessage(content='请简要介绍模型上下文协议（MCP）的基本概念和主要用途。', additional_kwargs={}, response_metadata={}, id='237d97ab-1ec4-4281-a1a2-bed964fb0ea0'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'google_search_tool', 'arguments': '{\"queries\": [\"\\\\u6a21\\\\u578b\\\\u4e0a\\\\u4e0b\\\\u6587\\\\u534f\\\\u8bae\\\\uff08MCP\\\\uff09\\\\u57fa\\\\u672c\\\\u6982\\\\u5ff5\\\\u548c\\\\u4e3b\\\\u8981\\\\u7528\\\\u9014\", \"Model Context Protocol (MCP) concepts and uses\"]}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite-preview-06-17', 'safety_ratings': []}, id='run--363e42f3-e906-4017-a40d-eb861c75b9c4-0', tool_calls=[{'name': 'google_search_tool', 'args': {'queries': ['模型上下文协议（MCP）基本概念和主要用途', 'Model Context Protocol (MCP) concepts and uses']}, 'id': 'e9c2a61f-0e2b-400b-a1b1-8c8e50caae42', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1068, 'output_tokens': 38, 'total_tokens': 1106, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Content from sources:\\n================================================================================\\nSource: Introducing the Model Context Protocol \\\\ Anthropic\\n--------------------------------------------------------------------------------\\nURL: https://www.anthropic.com/news/model-context-protocol\\n===\\nMost relevant content from source: Nov 25, 2024 ... The Model Context Protocol (MCP) is an open standard for connecting AI assistants to the systems where data lives, including content\\xa0...\\n===\\nFull source content limited to 4000 tokens: Introducing the Model Context Protocol \\\\ AnthropicSkip to main contentSkip to footerClaudeAPISolutionsResearchCommitmentsLearnNewsTry ClaudeAnnouncementsIntroducing the Model Context ProtocolNov 25, 2024●3 min readToday, we\\'re open-sourcing the Model Context Protocol (MCP), a new standard for connecting AI assistants to the systems where data lives, including content repositories, business tools, and development environments. Its aim is to help frontier models produce better, more relevant responses.As AI assistants gain mainstream adoption, the industry has invested heavily in model capabilities, achieving rapid advances in reasoning and quality. Yet even the most sophisticated models are constrained by their isolation from data—trapped behind information silos and legacy systems. Every new data source requires its own custom implementation, making truly connected systems difficult to scale.MCP addresses this challenge. It provides a universal, open standard for connecting AI systems with data sources, replacing fragmented integrations with a single protocol. The result is a simpler, more reliable way to give AI systems access to the data they need.Model Context ProtocolThe Model Context Protocol is an open standard that enables developers to build secure, two-way connections between their data sources and AI-powered tools. The architecture is straightforward: developers can either expose their data through MCP servers or build AI applications (MCP clients) that connect to these servers.Today, we\\'re introducing three major components of the Model Context Protocol for developers:The Model Context Protocol specification and SDKsLocal MCP server support in the Claude Desktop appsAn open-source repository of MCP serversClaude 3.5 Sonnet is adept at quickly building MCP server implementations, making it easy for organizations and individuals to rapidly connect their most important datasets with a range of AI-powered tools. To help developers start exploring, we’re sharing pre-built MCP servers for popular enterprise systems like Google Drive, Slack, GitHub, Git, Postgres, and Puppeteer.Early adopters like Block and Apollo have integrated MCP into their systems, while development tools companies including Zed, Replit, Codeium, and Sourcegraph are working with MCP to enhance their platforms—enabling AI agents to better retrieve relevant information to further understand the context around a coding task and produce more nuanced and functional code with fewer attempts.\"At Block, open source is more than a development model—it’s the foundation of our work and a commitment to creating technology that drives meaningful change and serves as a public good for all,” said Dhanji R. Prasanna, Chief Technology Officer at Block. “Open technologies like the Model Context Protocol are the bridges that connect AI to real-world applications, ensuring innovation is accessible, transparent, and rooted in collaboration. We are excited to partner on a protocol and use it to build agentic systems, which remove the burden of the mechanical so people can focus on the creative.”Instead of maintaining separate connectors for each data source, developers can now build against a standard protocol. As the ecosystem matures, AI systems will maintain context as they move between different tools and datasets, replacing today\\'s fragmented integrations with a more sustainable architecture.Getting startedDevelopers can start building and testing MCP connectors today. All Claude.ai plans support connecting MCP servers to the Claude Desktop app.Claude for Work customers can begin testing MCP servers locally, connecting Claude to internal systems and datasets. We\\'ll soon provide developer toolkits for deploying remote production MCP servers that can serve your entire Claude for Work organization.To start building:Install pre-built MCP servers through the Claude Desktop appFollow our quickstart guide to build your first MCP serverContribute to our open-source repositories of connectors and implementationsAn open communityWe’re committed to building MCP as a collaborative, open-source project and ecosystem, and we’re eager to hear your feedback. Whether you’re an AI tool developer, an enterprise looking to leverage existing data, or an early adopter exploring the frontier, we invite you to build the future of context-aware AI together.NewsRemote MCP support in Claude CodeJun 18, 2025NewsClaude in Amazon Bedrock: Approved for Use in FedRAMP High and DoD IL4/5 WorkloadsJun 11, 2025NewsNational Security Expert Richard Fontaine appointed to Anthropic’s Long-Term Benefit Trust Jun 07, 2025ProductClaude overviewClaude CodeClaude team planClaude enterprise planClaude education planDownload Claude appsClaude.ai pricing plans Claude.ai loginAPI PlatformAPI overview Developer docsClaude in Amazon BedrockClaude on Google Cloud\\'s Vertex AI PricingConsole loginResearchResearch overviewEconomic IndexClaude modelsClaude Opus 4Claude Sonnet 4Claude Haiku 3.5Commitments TransparencyResponsible scaling policySecurity and complianceSolutionsAI agentsCodingCustomer supportLearnAnthropic AcademyCustomer storiesEngineering at AnthropicMCP IntegrationsExploreAbout usBecome a partnerCareersEventsNewsStartups programHelp and securityStatusAvailabilitySupport centerTerms and policiesPrivacy choicesPrivacy policyResponsible disclosure policyTerms of service - consumerTerms of service - commercialUsage policy© 2025 Anthropic PBC\\n\\n================================================================================\\n\\n================================================================================\\nSource: Model Context Protocol: Introduction\\n--------------------------------------------------------------------------------\\nURL: https://modelcontextprotocol.io/introduction\\n===\\nMost relevant content from source: MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications.\\n===\\nFull source content limited to 4000 tokens: Introduction - Model Context ProtocolModel Context Protocol home pageVersion 2025-06-18 (latest)Search...⌘KGitHubSearch...NavigationUser GuideIntroductionUser GuideIntroductionQuickstartConceptsExamplesTutorialsFAQsProtocolSpecificationKey ChangesArchitectureBase ProtocolClient FeaturesServer FeaturesDevelopmentVersioningRoadmapContributingSDKsC# SDKJava SDKKotlin SDKPython SDKRuby SDKSwift SDKTypeScript SDKUser GuideCopy pageIntroductionGet started with the Model Context Protocol (MCP)MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.\\n\\u200bWhy MCP?\\nMCP helps you build agents and complex workflows on top of LLMs. LLMs frequently need to integrate with data and tools, and MCP provides:\\n\\nA growing list of pre-built integrations that your LLM can directly plug into\\nThe flexibility to switch between LLM providers and vendors\\nBest practices for securing your data within your infrastructure\\n\\n\\u200bGeneral architecture\\nAt its core, MCP follows a client-server architecture where a host application can connect to multiple servers:\\n\\n\\nMCP Hosts: Programs like Claude Desktop, IDEs, or AI tools that want to access data through MCP\\nMCP Clients: Protocol clients that maintain 1:1 connections with servers\\nMCP Servers: Lightweight programs that each expose specific capabilities through the standardized Model Context Protocol\\nLocal Data Sources: Your computer’s files, databases, and services that MCP servers can securely access\\nRemote Services: External systems available over the internet (e.g., through APIs) that MCP servers can connect to\\n\\n\\u200bGet started\\nChoose the path that best fits your needs:\\n\\u200bQuick Starts\\nFor Server DevelopersGet started building your own server to use in Claude for Desktop and other\\nclientsFor Client DevelopersGet started building your own client that can integrate with all MCP serversFor Claude Desktop UsersGet started using pre-built servers in Claude for Desktop\\n\\u200bExamples\\nExample ServersCheck out our gallery of official MCP servers and implementationsExample ClientsView the list of clients that support MCP integrations\\n\\u200bTutorials\\nBuilding MCP with LLMsLearn how to use LLMs like Claude to speed up your MCP developmentDebugging GuideLearn how to effectively debug MCP servers and integrationsMCP InspectorTest and inspect your MCP servers with our interactive debugging toolMCP Workshop (Video, 2hr)\\n\\u200bExplore MCP\\nDive deeper into MCP’s core concepts and capabilities:\\nCore architectureUnderstand how MCP connects clients, servers, and LLMsResourcesExpose data and content from your servers to LLMsPromptsCreate reusable prompt templates and workflowsToolsEnable LLMs to perform actions through your serverSamplingLet your servers request completions from LLMsTransportsLearn about MCP’s communication mechanism\\n\\u200bContributing\\nWant to contribute? Check out our Contributing Guide to learn how you can help improve MCP.\\n\\u200bSupport and Feedback\\nHere’s how to get help or provide feedback:\\n\\nFor bug reports and feature requests related to the MCP specification, SDKs, or documentation (open source), please create a GitHub issue\\nFor discussions or Q&A about the MCP specification, use the specification discussions\\nFor discussions or Q&A about other MCP open source components, use the organization discussions\\nFor bug reports, feature requests, and questions related to Claude.app and claude.ai’s MCP integration, please see Anthropic’s guide on How to Get Support\\nWas this page helpful?YesNoFor Server DevelopersgithubOn this pageWhy MCP?General architectureGet startedQuick StartsExamplesTutorialsExplore MCPContributingSupport and FeedbackAssistantResponses are generated using AI and may contain mistakes.\\n\\n================================================================================\\n\\n================================================================================\\nSource: 我用Go 语言为模型上下文协议(MCP) 搭建了一个Web UI - 征求反馈和 ...\\n--------------------------------------------------------------------------------\\nURL: https://www.reddit.com/r/golang/comments/1j219pd/i_built_a_gobased_web_ui_for_the_model_context/?tl=zh-hans\\n===\\nMost relevant content from source: Mar 2, 2025 ... 这是一个基于Go 语言的Web 应用程序，提供了一个灵活的聊天界面，支持多个LLM 提供商，比如Anthropic、 OpenAI、Ollama 和OpenRouter。 主要功能：. 流\\xa0...\\n===\\nFull source content limited to 4000 tokens: \\n\\n\\n\\nReddit - The heart of the internet\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\nSkip to main content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOpen menu\\n\\nOpen navigation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\nGo to Reddit Home\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nr/golang\\n\\nA chip\\n\\n\\n\\n\\n\\n\\n\\n\\nA close button\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGet App\\n\\n\\n\\n\\n\\n        Get the Reddit app\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\nLog In\\n\\nLog in to Reddit\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nExpand user menu\\nOpen settings menu\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n              Go to golang\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nr/golang\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nr/golang\\n\\n\\n\\n\\n\\n\\n\\n              问问题，发帖讨论Go编程语言以及相关的工具、活动等等。\\n            \\n\\n\\n\\n\\n\\n\\nMembers\\n\\n\\n\\n\\n\\n\\nOnline\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n•\\n\\n\\n\\n\\n\\n\\nMegaGrindStone\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n       我用 Go 语言为模型上下文协议 (MCP) 搭建了一个 Web UI - 征求反馈和贡献\\n \\n\\n\\n\\n\\n\\n\\n    show & tell\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n      嘿，Go 爱好者们！👋\\n    \\n      我一直在开发一个用于模型上下文协议 (MCP) 的开源 Web UI，想和大家分享一下。这是一个基于 Go 语言的 Web 应用程序，提供了一个灵活的聊天界面，支持多个 LLM 提供商，比如 Anthropic、 OpenAI、Ollama 和 OpenRouter。\\n    \\n主要功能：\\n\\n\\n\\n      流式聊天界面\\n    \\n\\n\\n      使用 BoltDB 实现持久化聊天记录\\n    \\n\\n\\n      支持多个 LLM 提供商\\n    \\n\\n\\n      使用 Server-Sent Events (SSE) 实现实时更新\\n    \\n\\n\\n      可通过 YAML 进行配置\\n    \\n\\n\\n当前状态： 这个项目是可用的，但肯定还有改进的空间，尤其是在前端方面。我用的是基本的 Bootstrap 样式，所以 UI/UX 方面还有很大的提升潜力。\\n    \\n贡献机会：\\n\\n\\n\\n      前端样式和设计\\n    \\n\\n\\n      添加新的 LLM 提供商集成\\n    \\n\\n\\n      改进配置管理\\n    \\n\\n\\n      编写测试\\n    \\n\\n\\n      改进文档\\n    \\n\\n\\n维护合作： 我正在积极寻找有兴趣帮助维护和发展这个项目的开发者。如果你对 Go、AI 界面感兴趣，或者只是想为开源项目做贡献，我很乐意欢迎你加入！\\n    \\n加入我们的社区： 我建立了一个 Discord 服务器，用于协作和讨论。欢迎加入：  https://discord.gg/kzvxBqHQ \\n\\n      这个项目还很年轻，我欢迎反馈、建议和贡献。即使你只想分享你的想法或指出潜在的改进，我也很乐意倾听！\\n    \\n      GitHub:  https://github.com/MegaGrindStone/mcp-web-ui \\n\\n      干杯，祝你编码愉快！🚀\\n    \\n\\n\\n\\n      Read more\\n       \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Share \\n\\n\\n          Share\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNew to Reddit?\\n          \\n\\n                  Create your account and connect with a world of communities.\\n                \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n        Continue with Email\\n      \\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n              Continue With Phone Number\\n            \\n\\n\\n\\n\\n\\nBy continuing, you agree to our\\nUser Agreement\\nand acknowledge that you understand the\\nPrivacy Policy.\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Public\\n   Anyone can view, post, and comment to this community\\n\\n\\n\\n\\n\\n\\nReddit Rules\\n\\n\\n\\nPrivacy Policy\\n\\n\\n\\nUser Agreement\\n\\n\\nReddit, Inc. © 2025. All rights reserved.\\n\\n\\n\\n\\n\\n\\n\\n\\nExpand Navigation\\n\\n\\n\\n\\n\\n\\n\\n\\nCollapse Navigation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n&nbsp;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n&nbsp;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n&nbsp;\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTOPICS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nInternet Culture (Viral)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAmazing\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnimals & Pets\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCringe & Facepalm\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFunny\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nInteresting\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMemes\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOddly Satisfying\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReddit Meta\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWholesome & Heartwarming\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGames\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAction Games\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAdventure Games\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEsports\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGaming Consoles & Gear\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGaming News & Discussion\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMobile Games\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOther Games\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRole-Playing Games\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSimulation Games\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSports & Racing Games\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nStrategy Games\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTabletop Games\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nQ&As\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nQ&As\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nStories & Confessions\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTechnology\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n3D Printing\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nArtificial Intelligence & Machine Learning\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nComputers & Hardware\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nConsumer Electronics\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDIY Electronics\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProgramming\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSoftware & Apps\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nStreaming Services\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTech News & Discussion\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nVirtual & Augmented Reality\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPop Culture\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCelebrities\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCreators & Influencers\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGenerations & Nostalgia\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPodcasts\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nStreamers\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTarot & Astrology\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMovies & TV\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAction Movies & Series\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnimated Movies & Series\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nComedy Movies & Series\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCrime, Mystery, & Thriller Movies & Series\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentary Movies & Series\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDrama Movies & Series\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFantasy Movies & Series\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHorror Movies & Series\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMovie News & Discussion\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReality TV\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRomance Movies & Series\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSci-Fi Movies & Series\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuperhero Movies & Series\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTV News & Discussion\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\nRESOURCES\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout Reddit\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAdvertise\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReddit Pro\\n\\n      BETA\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBlog\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCareers\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPress\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCommunities\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBest of Reddit\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTopics\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n     \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n================================================================================\\n\\n================================================================================\\nSource: 模型上下文协议(MCP) - Anthropic\\n--------------------------------------------------------------------------------\\nURL: https://docs.anthropic.com/zh-CN/docs/claude-code/mcp\\n===\\nMost relevant content from source: 添加MCP HTTP 服务器. Copy. # 基本语法 claude mcp add --transport http <name> ... MCP 服务器可以在三个不同的作用域级别进行配置，每个级别都有不同的用途来管理服务器可\\xa0...\\n===\\nFull source content limited to 4000 tokens: 模型上下文协议 (MCP) - AnthropicAnthropic home page简体中文Search...ResearchLoginSign upSign upSearch...Navigation使用 Claude 构建模型上下文协议 (MCP)开发者指南欢迎API 指南Claude 代码资源发布说明Documentation开发者 Discord支持入门概述设置快速开始内存管理常见工作流程使用 Claude 构建将 Claude Code 添加到您的 IDE模型上下文协议 (MCP)Claude Code GitHub ActionsClaude Code SDK故障排除部署概述Amazon BedrockGoogle Vertex AI企业代理LLM 网关开发容器管理身份和访问管理安全性监控成本参考CLI 参考交互模式斜杠命令设置资源数据使用法律和合规使用 Claude 构建模型上下文协议 (MCP)Copy page了解如何在 Claude Code 中设置 MCP。模型上下文协议 (MCP) 是一个开放协议，使 LLM 能够访问外部工具和数据源。有关 MCP 的更多详细信息，请参阅 MCP 文档。\\n使用第三方 MCP 服务器需要您自担风险。确保您信任 MCP\\n服务器，在使用与互联网通信的 MCP 服务器时要特别小心，\\n因为这些可能会使您面临提示注入风险。\\n\\u200b配置 MCP 服务器\\n1添加 MCP stdio 服务器# 基本语法\\nclaude mcp add <name> <command> [args...]\\n\\n# 示例：添加本地服务器\\nclaude mcp add my-server -e API_KEY=123 -- /path/to/server arg1 arg2\\n2添加 MCP SSE 服务器# 基本语法\\nclaude mcp add --transport sse <name> <url>\\n\\n# 示例：添加 SSE 服务器\\nclaude mcp add --transport sse sse-server https://example.com/sse-endpoint\\n\\n# 示例：添加带有自定义标头的 SSE 服务器\\nclaude mcp add --transport sse api-server https://api.example.com/mcp -e X-API-Key=your-key\\n3添加 MCP HTTP 服务器# 基本语法\\nclaude mcp add --transport http <name> <url>\\n\\n# 示例：添加可流式传输的 HTTP 服务器\\nclaude mcp add --transport http http-server https://example.com/mcp\\n\\n# 示例：添加带有身份验证标头的 HTTP 服务器\\nclaude mcp add --transport http secure-server https://api.example.com/mcp -e Authorization=\"Bearer your-token\"\\n4管理您的 MCP 服务器# 列出所有已配置的服务器\\nclaude mcp list\\n\\n# 获取特定服务器的详细信息\\nclaude mcp get my-server\\n\\n# 删除服务器\\nclaude mcp remove my-server\\n\\n提示：\\n使用 -s 或 --scope 标志指定配置存储位置：\\n\\nlocal（默认）：仅在当前项目中对您可用（在旧版本中称为 project）\\nproject：通过 .mcp.json 文件与项目中的每个人共享\\nuser：在所有项目中对您可用（在旧版本中称为 global）\\n\\n\\n使用 -e 或 --env 标志设置环境变量（例如，-e KEY=value）\\n使用 MCP_TIMEOUT 环境变量配置 MCP 服务器启动超时（例如，MCP_TIMEOUT=10000 claude 设置 10 秒超时）\\n随时使用 Claude Code 中的 /mcp 命令检查 MCP 服务器状态\\nMCP 遵循客户端-服务器架构，其中 Claude Code（客户端）可以连接到多个专用服务器\\nClaude Code 支持 SSE（服务器发送事件）和可流式传输的 HTTP 服务器进行实时通信\\n使用 /mcp 与需要 OAuth 2.0 身份验证的远程服务器进行身份验证\\n\\n\\u200b了解 MCP 服务器作用域\\nMCP 服务器可以在三个不同的作用域级别进行配置，每个级别都有不同的用途来管理服务器可访问性和共享。了解这些作用域有助于您确定为特定需求配置服务器的最佳方式。\\n\\u200b作用域层次结构和优先级\\nMCP 服务器配置遵循清晰的优先级层次结构。当同名服务器存在于多个作用域时，系统通过优先考虑本地作用域服务器，然后是项目作用域服务器，最后是用户作用域服务器来解决冲突。这种设计确保个人配置可以在需要时覆盖共享配置。\\n\\u200b本地作用域\\n本地作用域服务器代表默认配置级别，存储在您的项目特定用户设置中。这些服务器对您保持私有，仅在当前项目目录中工作时可访问。此作用域非常适合个人开发服务器、实验性配置或包含不应共享的敏感凭据的服务器。\\n# 添加本地作用域服务器（默认）\\nclaude mcp add my-private-server /path/to/server\\n\\n# 显式指定本地作用域\\nclaude mcp add my-private-server -s local /path/to/server\\n\\n\\u200b项目作用域\\n项目作用域服务器通过将配置存储在项目根目录的 .mcp.json 文件中来实现团队协作。此文件设计为检入版本控制，确保所有团队成员都能访问相同的 MCP 工具和服务。当您添加项目作用域服务器时，Claude Code 会自动创建或更新此文件，使用适当的配置结构。\\n# 添加项目作用域服务器\\nclaude mcp add shared-server -s project /path/to/server\\n\\n生成的 .mcp.json 文件遵循标准化格式：\\n{\\n  \"mcpServers\": {\\n    \"shared-server\": {\\n      \"command\": \"/path/to/server\",\\n      \"args\": [],\\n      \"env\": {}\\n    }\\n  }\\n}\\n\\n出于安全原因，Claude Code 在使用来自 .mcp.json 文件的项目作用域服务器之前会提示批准。如果您需要重置这些批准选择，请使用 claude mcp reset-project-choices 命令。\\n\\u200b用户作用域\\n用户作用域服务器提供跨项目可访问性，使它们在您机器上的所有项目中可用，同时对您的用户帐户保持私有。此作用域适用于个人实用程序服务器、开发工具或您在不同项目中经常使用的服务。\\n# 添加用户服务器\\nclaude mcp add my-user-server -s user /path/to/server\\n\\n\\u200b选择正确的作用域\\n根据以下条件选择您的作用域：\\n\\n本地作用域：个人服务器、实验性配置或特定于一个项目的敏感凭据\\n项目作用域：团队共享服务器、项目特定工具或协作所需的服务\\n用户作用域：多个项目中需要的个人实用程序、开发工具或经常使用的服务\\n\\n\\u200b与远程 MCP 服务器进行身份验证\\n许多远程 MCP 服务器需要身份验证。Claude Code 支持 OAuth 2.0 身份验证流程，以安全连接到这些服务器。\\n1添加需要身份验证的远程服务器# 添加需要 OAuth 的 SSE 或 HTTP 服务器\\nclaude mcp add --transport sse github-server https://api.github.com/mcp\\n2使用 /mcp 命令进行身份验证在 Claude Code 中，使用 /mcp 命令管理身份验证：> /mcp\\n这会打开一个交互式菜单，您可以：\\n查看所有服务器的连接状态\\n与需要 OAuth 的服务器进行身份验证\\n清除现有身份验证\\n查看服务器功能\\n3完成 OAuth 流程当您为服务器选择”身份验证”时：\\n您的浏览器会自动打开到 OAuth 提供商\\n在浏览器中完成身份验证\\nClaude Code 接收并安全存储访问令牌\\n服务器连接变为活动状态\\n\\n提示：\\n身份验证令牌安全存储并自动刷新\\n在 /mcp 菜单中使用”清除身份验证”来撤销访问\\n如果您的浏览器没有自动打开，请复制提供的 URL\\nOAuth 身份验证适用于 SSE 和 HTTP 传输\\n\\n\\u200b连接到 Postgres MCP 服务器\\n假设您想要给 Claude 对 PostgreSQL 数据库的只读访问权限，用于查询和模式检查。\\n1添加 Postgres MCP 服务器claude mcp add postgres-server /path/to/postgres-mcp-server --connection-string \"postgresql://user:pass@localhost:5432/mydb\"\\n2使用 Claude 查询您的数据库> describe the schema of our users table\\n> what are the most recent orders in the system?\\n> show me the relationship between customers and invoices\\n\\n提示：\\nPostgres MCP 服务器为安全起见提供只读访问\\nClaude 可以帮助您探索数据库结构并运行分析查询\\n您可以使用此功能快速了解不熟悉项目中的数据库模式\\n确保您的连接字符串使用具有最小所需权限的适当凭据\\n\\n\\u200b从 JSON 配置添加 MCP 服务器\\n假设您有一个单个 MCP 服务器的 JSON 配置，您想要将其添加到 Claude Code。\\n1从 JSON 添加 MCP 服务器# 基本语法\\nclaude mcp add-json <name> \\'<json>\\'\\n\\n# 示例：使用 JSON 配置添加 stdio 服务器\\nclaude mcp add-json weather-api \\'{\"type\":\"stdio\",\"command\":\"/path/to/weather-cli\",\"args\":[\"--api-key\",\"abc123\"],\"env\":{\"CACHE_DIR\":\"/tmp\"}}\\'\\n2验证服务器已添加claude mcp get weather-api\\n\\n提示：\\n确保 JSON 在您的 shell 中正确转义\\nJSON 必须符合 MCP 服务器配置模式\\n您可以使用 -s global 将服务器添加到您的全局配置而不是项目特定配置\\n\\n\\u200b从 Claude Desktop 导入 MCP 服务器\\n假设您已经在 Claude Desktop 中配置了 MCP 服务器，并希望在 Claude Code 中使用相同的服务器，而无需手动重新配置它们。\\n1从 Claude Desktop 导入服务器# 基本语法 \\nclaude mcp add-from-claude-desktop \\n2选择要导入的服务器运行命令后，您将看到一个交互式对话框，允许您选择要导入的服务器。3验证服务器已导入claude mcp list \\n\\n提示：\\n此功能仅在 macOS 和 Windows 子系统 Linux (WSL) 上有效\\n它从这些平台上的标准位置读取 Claude Desktop 配置文件\\n使用 -s global 标志将服务器添加到您的全局配置\\n导入的服务器将与 Claude Desktop 中的名称相同\\n如果已存在同名服务器，它们将获得数字后缀（例如，server_1）\\n\\n\\u200b将 Claude Code 用作 MCP 服务器\\n假设您想要将 Claude Code 本身用作其他应用程序可以连接的 MCP 服务器，为它们提供 Claude 的工具和功能。\\n1将 Claude 启动为 MCP 服务器# 基本语法\\nclaude mcp serve\\n2从另一个应用程序连接您可以从任何 MCP 客户端连接到 Claude Code MCP 服务器，例如 Claude Desktop。如果您使用 Claude Desktop，可以使用此配置添加 Claude Code MCP 服务器：{\\n  \"command\": \"claude\",\\n  \"args\": [\"mcp\", \"serve\"],\\n  \"env\": {}\\n}\\n\\n提示：\\n服务器提供对 Claude 工具的访问，如 View、Edit、LS 等\\n在 Claude Desktop 中，尝试要求 Claude 读取目录中的文件、进行编辑等\\n请注意，此 MCP 服务器只是将 Claude Code 的工具暴露给您的 MCP 客户端，因此您自己的客户端负责为单个工具调用实现用户确认\\n\\n\\u200b使用 MCP 资源\\nMCP 服务器可以公开您可以使用 @ 提及引用的资源，类似于您引用文件的方式。\\n\\u200b引用 MCP 资源\\n1列出可用资源在您的提示中键入 @ 以查看来自所有连接的 MCP 服务器的可用资源。资源与文件一起出现在自动完成菜单中。2引用特定资源使用格式 @server:protocol://resource/path 引用资源：> Can you analyze @github:issue://123 and suggest a fix?\\n> Please review the API documentation at @docs:file://api/authentication\\n3多个资源引用您可以在单个提示中引用多个资源：> Compare @postgres:schema://users with @docs:file://database/user-model\\n\\n提示：\\n资源在引用时会自动获取并作为附件包含\\n资源路径在 @ 提及自动完成中可进行模糊搜索\\n当服务器支持时，Claude Code 会自动提供列出和读取 MCP 资源的工具\\n资源可以包含 MCP 服务器提供的任何类型的内容（文本、JSON、结构化数据等）\\n\\n\\u200b将 MCP 提示用作斜杠命令\\nMCP 服务器可以公开在 Claude Code 中作为斜杠命令可用的提示。\\n\\u200b执行 MCP 提示\\n1发现可用提示键入 / 以查看所有可用命令，包括来自 MCP 服务器的命令。MCP 提示以格式 /mcp__servername__promptname 出现。2执行不带参数的提示> /mcp__github__list_prs\\n3执行带参数的提示许多提示接受参数。在命令后以空格分隔传递它们：> /mcp__github__pr_review 456\\n> /mcp__jira__create_issue \"Bug in login flow\" high\\n\\n提示：\\nMCP 提示从连接的服务器动态发现\\n参数根据提示的定义参数进行解析\\n提示结果直接注入到对话中\\n服务器和提示名称已标准化（空格变为下划线）\\nWas this page helpful?YesNo将 Claude Code 添加到您的 IDEClaude Code GitHub ActionsxlinkedindiscordOn this page配置 MCP 服务器了解 MCP 服务器作用域作用域层次结构和优先级本地作用域项目作用域用户作用域选择正确的作用域与远程 MCP 服务器进行身份验证连接到 Postgres MCP 服务器从 JSON 配置添加 MCP 服务器从 Claude Desktop 导入 MCP 服务器将 Claude Code 用作 MCP 服务器使用 MCP 资源引用 MCP 资源将 MCP 提示用作斜杠命令执行 MCP 提示\\n\\n================================================================================\\n\\n================================================================================\\nSource: Extending Copilot coding agent with the Model Context Protocol ...\\n--------------------------------------------------------------------------------\\nURL: https://docs.github.com/zh/copilot/customizing-copilot/extending-copilot-coding-agent-with-mcp\\n===\\nMost relevant content from source: Staying safe with MCP servers. Once you\\'ve configured an MCP server, Copilot will be able to use the tools provided by the server autonomously, and will not\\xa0...\\n===\\nFull source content limited to 4000 tokens: 使用模型上下文协议 (MCP) 扩展 Copilot 编码助手 - GitHub 文档Skip to main contentGitHub 文档Version: Free, Pro, & TeamOpen SidebarGitHub Copilot/使用 GitHub Copilot/编码智能体/使用 MCP 扩展编码代理主页GitHub Copilot快速入门关于 GitHub Copilot什么是 GitHub Copilot？Copilot 功能适用于 Copilot 的计划设置为组织设置为自己设置使用 GitHub Copilot最佳做法获取代码建议Copilot Chat在 ID 中聊天在 GitHub 中聊天在 Mobile 中聊天聊天入门语义索引提示设计Copilot Chat 速查表Copilot Spaces组织和共享上下文创建和使用 Copilot Spaces加快开发工作与您的团队协作编码智能体关于将任务分配到 Copilot最佳做法启用 Copilot 编码智能体使用 Copilot 处理议题根据聊天创建 PR评审 Copilot PR使用 Copilot 日志使用 MCP 扩展编码代理故障排除Windows 终端中的 Copilot代码评审使用代码评审配置编码准则自动代码评审使用 Copilot（CLI 版）使用 Copilot 扩展AI 模型Supported AI models in Copilot选择合适的 AI 模型AI 模型比较示例Configure access to AI models更改聊天模型更改补全模型查找匹配代码Create a PR summary使用 Copilot 创建问题Guides选择合适的 AI 工具探索代码库重构代码迁移项目编写测试实现旧代码现代化升级 Java 项目Copilot 聊天指南All prompts调试错误调试无效的 JSON处理 API 速率限制功能分析和功能建议探索功能实现包含反馈重构代码提高代码可读性Lint 错误性能优化设计模式数据访问层分离业务逻辑横切关注点简化继承层次结构修复数据库死锁翻译代码记录代码记录旧代码解释旧代码解释复杂逻辑同步文档撰写讨论或博客文章测试代码生成单元测试创建模拟对象创建端到端测试安全分析查找漏洞管理 Copilot为个人管理Copilot 入门关于个人版 Copilot 计划和权益开始使用 Copilot 计划免费访问 Copilot Pro管理 Copilot 计划查看和更改 Copilot 计划禁用 Copilot Free管理策略为个人安装扩展计费与付款关于计费组织管理管理计划关于计费订阅Cancel管理策略允许 Copilot 流量管理访问权限授予访问权限管理访问请求撤销访问权限管理网络访问权限添加 Copilot 编码智能体检查活动用户活动数据审核日志为企业管理管理计划关于计费了解和管理使用情况了解和管理请求Monitor your Copilot usage and entitlements配置个人设置安装 Copilot 扩展在 CLI 中安装 Copilot网络设置在环境中配置在 CLI 中配置向 GHE.com 进行身份验证配置内容排除从 Copilot 中排除内容内容排除项更改大规模推出 Copilot规划推出选择计划分配许可证自助许可证提醒非活动用户管理支出赋能开发人员推动 Copilot 采用集成 AI 代理使用 Copilot 编码智能体衡量采用情况分析一段时间内的使用情况自定义 Copilot使用 MCP使用 MCP 扩展 Copilot 对话助手使用 GitHub MCP 服务器安装扩展About customizing Copilot responses个人自定义说明存储库自定义说明自定义代理环境Customize the agent firewall生成 Copilot 扩展关于生成扩展扩展快速入门设置 Copilot 扩展创建 Copilot 扩展托管扩展创建 GitHub 应用为扩展配置应用生成 Copilot 代理关于 Copilot 代理与 Copilot 平台通信与 GitHub 通信上下文传递使用 Copilot 的 LLM构建 Copilot 技能集关于 Copilot 技能集构建 Copilot 技能集使用 OIDC调试 Copilot 扩展管理扩展可用性扩展常见问题解答扩展术语表故障排除GitHub Copilot 的常见问题Rate limits查看日志连接安全性设置网络错误Copilot 对话助手负责任使用Copilot code completion在 IDE 中聊天在 GitHub 中聊天在 GitHub Mobile 中聊天Copilot（CLI 版）Windows 终端中的 CopilotGitHub Desktop 中的 Copilot拉取请求摘要Copilot 文本补全代码评审Copilot 编码代理GitHub Copilot/使用 GitHub Copilot/编码智能体/使用 MCP 扩展编码代理使用模型上下文协议 (MCP) 扩展 Copilot 编码助手了解如何使用模型上下文协议 (MCP) 扩展 Copilot 编码智能体 的功能。本文内容通过使用 MCP 服务器保持安全性关于在存储库中设置 MCP 服务器示例配置从 Visual Studio Code将配置添加到存储库验证 MCP 配置自定义内置 GitHub MCP 服务器最佳做法其他阅读材料注意\\n\\nCopilot 编码智能体 is in 公共预览版 and subject to change. During the preview, use of the feature is subject to GitHub 预发行许可条款.\\n有关 Copilot 编码智能体 的详细信息，请参阅 关于将任务分配到 Copilot。\\n\\nThe Model Context Protocol (MCP) is an open standard that defines how applications share context with large language models (LLMs). MCP provides a standardized way to connect AI models to different data sources and tools, enabling them to work together more effectively.\\nYou can use MCP to extend the capabilities of Copilot 编码智能体 by connecting it to other tools and services.\\n该代理可使用本地 MCP 服务器提供的工具。 例如，Playwright MCP 服务器提供了一些工具，用于在执行请求的任务时与网页进行交互，并获取额外的上下文信息。\\n有关 MCP 的详细信息，请参阅官方 MCP 文档。 有关一些当前可用的 MCP 服务器的信息，请参阅 MCP 服务器存储库。\\n注意\\n\\n\\nCopilot 编码智能体 仅支持 MCP 服务器提供的工具。 它不支持资源或提示。\\nCopilot 编码智能体 目前仅支持本地 MCP 服务器。 若要了解有关传输类型的详细信息，请参阅官方 MCP 文档。\\n\\n\\n通过使用 MCP 服务器保持安全性\\n配置 MCP 服务器后，Copilot 将能够自主使用服务器提供的工具且在使用之前不请求批准。\\n建议将服务器权限限制为仅可使用只读工具。 可通过使用 tools 配置选项，仅向 Copilot 公开已知且安全的工具。\\n关于在存储库中设置 MCP 服务器\\n作为存储库管理员，可以将 MCP 服务器配置为在存储库中使用。 这通过 JSON 格式的配置完成，该配置指定要使用的 MCP 服务器的详细信息。 直接在 GitHub.com 上的存储库的设置中输入 JSON 配置。\\n将 MCP 服务器配置为在存储库中使用后，配置中指定的工具将可供 Copilot 编码智能体 用于每个分配的任务。\\n创建 JSON MCP 配置\\n使用专用 JSON 格式配置 MCP 服务器。 JSON 必须包含一个 mcpServers 对象，其中键是 MCP 服务器的名称（例如 playwright），值是具有该 MCP 服务器的配置的对象。\\nJSON{\\n  \"mcpServers\": {\\n    \"MCP SERVER 1\": {\\n      \"command\": \"VALUE\",\\n      \"args\": [ VALUES ],\\n      ...\\n    },\\n    \"MCP SERVER 2\": {\\n      \"command\": \"VALUE\",\\n      \"args\": [ VALUES ],\\n      ...\\n    },\\n    ...\\n  }\\n}\\n{\\n  \"mcpServers\": {\\n    \"MCP SERVER 1\": {\\n      \"command\": \"VALUE\",\\n      \"args\": [ VALUES ],\\n      ...\\n    },\\n    \"MCP SERVER 2\": {\\n      \"command\": \"VALUE\",\\n      \"args\": [ VALUES ],\\n      ...\\n    },\\n    ...\\n  }\\n}\\n\\n配置对象可包含以下键：\\n\\ncommand (string)：启动 MCP 服务器时要运行的命令。\\nargs (string[])：要传递给 command 的参数。\\ntools (string[])：要启用的 MCP 服务器中的工具。 有可能在服务器的文档或代码中找到工具列表。 建议将特定工具列入允许列表，但也可以通过在 * 数组中包含所有工具来启用这些工具。\\ntype (string)：可选字段。 Copilot 编码智能体 仅接受 \"local\"。\\nenv (object)：要传递给服务器的环境变量。 此对象须将应向 MCP 服务器公开的环境变量的名称映射到以下任一项：\\n\\n已配置的 GitHub Actions 机密的名称，以 COPILOT_MCP_ 开头。\\n字符串值。\\n\\n\\n\\n示例配置\\n示例：Playwright\\nPlaywright MCP 服务器提供了使 Copilot 能够浏览 Internet 的工具。\\nJSON{\\n  \"mcpServers\": {\\n    \"playwright\": {\\n      \"command\": \"docker\",\\n      \"args\": [\"run\", \"-i\", \"--rm\", \"--init\", \"mcp/playwright\"],\\n      \"tools\": [\"*\"]\\n    }\\n  }\\n}\\n{\\n  \"mcpServers\": {\\n    \"playwright\": {\\n      \"command\": \"docker\",\\n      \"args\": [\"run\", \"-i\", \"--rm\", \"--init\", \"mcp/playwright\"],\\n      \"tools\": [\"*\"]\\n    }\\n  }\\n}\\n\\n示例：Sentry\\nSentry MCP 服务器为 Copilot 提供经身份验证的访问权限，从而能够访问 Sentry 中记录的异常。\\nJavaScript// If you copy and paste this example, you will need to remove the comments prefixed with `//`, which are not valid JSON.\\n{\\n  \"mcpServers\": {\\n    \"sentry\": {\\n      \"command\": \"npx\",\\n      // We can use the $SENTRY_HOST environment variable which is passed to\\n      // the server because of the `env` value below.\\n      \"args\": [\"@sentry/mcp-server@latest\", \"--host=$SENTRY_HOST\"],\\n      \"tools\": [\"get_issue_details\", \"get_issue_summary\"],\\n      \"env\": {\\n        // We can specify an environment variable value as a string...\\n        \"SENTRY_HOST\": \"https://contoso.sentry.io\",\\n        // or refer to a GitHub Actions secret with a name starting with\\n        // `COPILOT_MCP_`\\n        \"SENTRY_AUTH_TOKEN\": \"COPILOT_MCP_SENTRY_AUTH_TOKEN\"\\n      }\\n    }\\n  }\\n}\\n// If you copy and paste this example, you will need to remove the comments prefixed with `//`, which are not valid JSON.\\n{\\n  \"mcpServers\": {\\n    \"sentry\": {\\n      \"command\": \"npx\",\\n      // We can use the $SENTRY_HOST environment variable which is passed to\\n      // the server because of the `env` value below.\\n      \"args\": [\"@sentry/mcp-server@latest\", \"--host=$SENTRY_HOST\"],\\n      \"tools\": [\"get_issue_details\", \"get_issue_summary\"],\\n      \"env\": {\\n        // We can specify an environment variable value as a string...\\n        \"SENTRY_HOST\": \"https://contoso.sentry.io\",\\n        // or refer to a GitHub Actions secret with a name starting with\\n        // `COPILOT_MCP_`\\n        \"SENTRY_AUTH_TOKEN\": \"COPILOT_MCP_SENTRY_AUTH_TOKEN\"\\n      }\\n    }\\n  }\\n}\\n\\n示例：Notion\\nNotion MCP 服务器向 Copilot 经身份验证的访问权限，从而能够访问来自 Notion 的笔记和其他内容。\\nJavaScript// If you copy and paste this example, you will need to remove the comments prefixed with `//`, which are not valid JSON.\\n{\\n  \"mcpServers\": {\\n    \"notionApi\": {\\n      \"command\": \"docker\",\\n      \"args\": [\\n        \"run\",\\n        \"--rm\",\\n        \"-i\",\\n        \"-e\",\\n        // We can use the $NOTION_API_KEY environment variable which is passed to\\n        // the server because of the `env` value below.\\n        \"OPENAPI_MCP_HEADERS={\\\\\"Authorization\\\\\": \\\\\"Bearer $NOTION_API_KEY\\\\\", \\\\\"Notion-Version\\\\\": \\\\\"2022-06-28\\\\\"}\",\\n       \"mcp/notion\"\\n      ],\\n      \"env\": {\\n        // The value of the `COPILOT_MCP_NOTION_API_KEY` secret will be passed to the\\n        // server command as an environment variable called `NOTION_API_KEY`\\n        \"NOTION_API_KEY\": \"COPILOT_MCP_NOTION_API_KEY\"\\n      },\\n      \"tools\": [\"*\"]\\n    }\\n  }\\n}\\n// If you copy and paste this example, you will need to remove the comments prefixed with `//`, which are not valid JSON.\\n{\\n  \"mcpServers\": {\\n    \"notionApi\": {\\n      \"command\": \"docker\",\\n      \"args\": [\\n        \"run\",\\n        \"--rm\",\\n        \"-i\",\\n        \"-e\",\\n        // We can use the $NOTION_API_KEY environment variable which is passed to\\n        // the server because of the `env` value below.\\n        \"OPENAPI_MCP_HEADERS={\\\\\"Authorization\\\\\": \\\\\"Bearer $NOTION_API_KEY\\\\\", \\\\\"Notion-Version\\\\\": \\\\\"2022-06-28\\\\\"}\",\\n       \"mcp/notion\"\\n      ],\\n      \"env\": {\\n        // The value of the `COPILOT_MCP_NOTION_API_KEY` secret will be passed to the\\n        // server command as an environment variable called `NOTION_API_KEY`\\n        \"NOTION_API_KEY\": \"COPILOT_MCP_NOTION_API_KEY\"\\n      },\\n      \"tools\": [\"*\"]\\n    }\\n  }\\n}\\n\\n示例: Azure\\nAzure MCP 服务器在 Copilot 与重要的 Azure 服务（例如 Azure Cosmos DB 和 Azure 存储平台）之间建立无缝连接。\\n若要将 Azure MCP 与 Copilot 编码智能体 配合使用，需要更新存储库的 copilot-setup-steps.yml 文件，使其包含 Azure 登录工作流步骤。\\n\\n\\n在 Microsoft Entra 应用程序中配置 OIDC，以便信任 GitHub。 请参阅将 Azure 登录操作与 OpenID Connect 结合使用。\\n\\n\\n如果还没有作工作流文件，在存储库中添加 .github/workflows/copilot-setup-steps.yml 操作工作流文件。\\n\\n\\n将 Azure 登录步骤添加到 copilot-setup-steps 工作流作业。\\nYAMLon:\\n  workflow_dispatch:\\npermissions:\\n  id-token: write\\n  contents: read\\njobs:\\n  copilot-setup-steps:\\n    runs-on: ubuntu-latest\\n    permissions:\\n      id-token: write\\n      contents: read\\n    environment: Copilot\\n    steps:\\n      - name: Azure login\\n        uses: azure/login@a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8a9b0\\n        with:\\n          client-id: $\\n          tenant-id: $\\n          subscription-id: $\\non:\\n  workflow_dispatch:\\npermissions:\\n  id-token: write\\n  contents: read\\njobs:\\n  copilot-setup-steps:\\n    runs-on: ubuntu-latest\\n    permissions:\\n      id-token: write\\n      contents: read\\n    environment: Copilot\\n    steps:\\n      - name: Azure login\\n        uses: azure/login@a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8a9b0\\n        with:\\n          client-id: $\\n          tenant-id: $\\n          subscription-id: $\\n\\n此配置可确保在 azure/login Copilot 编码智能体 运行时会执行该操作。\\n\\n\\n在存储库的 Copilot 环境中，为 AZURE_CLIENT_ID、AZURE_TENANT_ID 和 AZURE_SUBSCRIPTION_ID 添加机密。\\n\\n\\n通过将 azure 对象添加到 MCP 配置中来配置 Azure MCP 服务器。\\nJSON{\\n  \"mcpServers\": {\\n    \"Azure MCP Server\": {\\n      \"command\": \"npx\",\\n      \"args\": [\\n        \"-y\",\\n        \"@azure/mcp@latest\",\\n        \"server\",\\n        \"start\"\\n      ]\\n    }\\n  }\\n}\\n{\\n  \"mcpServers\": {\\n    \"Azure MCP Server\": {\\n      \"command\": \"npx\",\\n      \"args\": [\\n        \"-y\",\\n        \"@azure/mcp@latest\",\\n        \"server\",\\n        \"start\"\\n      ]\\n    }\\n  }\\n}\\n\\n\\n\\n从 Visual Studio Code\\n重用 MCP 配置\\n如果已在 VS Code 中配置 MCP 服务器，可以对 Copilot 编码智能体 利用相似配置。\\n根据 VS Code 的配置方式，有可能可在存储库的 .vscode/mcp.json 文件或计算机的专用 settings.json 文件中找到 MCP 设置。\\n若要调整 Copilot 编码智能体 的配置，需要：\\n\\n为每个 MCP 服务器添加一个 tools 键，指定哪些工具可供 Copilot 使用。\\n如果配置了 inputs，直接切换为使用 env。\\n如果配置了 envFile，直接切换为使用 env。\\n将 args 配置中对 inputs 的任何引用更新为引用 env 中的环境变量。\\n\\n有关 VS Code 中 MCP 的详细信息，请参阅 VS Code 文档。\\n将配置添加到存储库\\n存储库管理员可以按照以下步骤配置 MCP 服务器：\\n\\n\\n在 GitHub 上，导航到存储库的主页面。\\n\\n\\n在存储库名称下，单击 “设置”。 如果看不到“设置”选项卡，请选择“”下拉菜单，然后单击“设置”。\\n\\n\\n\\n在边栏的“Code & automation”部分，单击 Copilot，然后单击 Copilot 代理********。\\n\\n\\n**** 在“MCP configuration”部分中添加配置。\\n\\n\\n单击“ 保存”。\\n系统将验证配置以确保其语法正确。\\n\\n\\n如果 MCP 服务器需要密钥或机密，请将机密添加到 Copilot 环境。 仅当机密名称带有前缀 COPILOT_MCP_ 才可用于 MCP 配置。 请参阅为 Copilot 编码智能体 设置 Copilot 环境。\\n\\n\\n为 Copilot 编码智能体 设置 Copilot 环境\\n某些 MCP 服务器需要密钥或机密。 若要在 Copilot 编码智能体 中利用这些服务器，可以在 Copilot 的环境中添加机密。 这可确保正确识别机密并将其传递给已配置的适用的 MCP 服务器。\\n要为存储库配置 Copilot 环境，需要具有存储库管理员身份。\\n\\n\\n在 GitHub 上，导航到存储库的主页面。\\n\\n\\n在存储库名称下，单击 “设置”。 如果看不到“设置”选项卡，请选择“”下拉菜单，然后单击“设置”。\\n\\n\\n\\n在左侧边栏中，单击“环境”。\\n\\n\\n单击“新建环境”。\\n\\n\\n**** 调用新环境 copilot，然后单击“Configure environment”。\\n\\n\\n**** 在“Environment secrets”下，单击“Add environment secret”。\\n\\n\\n**** 将机密命名为以 COPILOT_MCP_ 开头的名称，添加机密值，然后单击“Add secret”。\\n\\n\\n验证 MCP 配置\\n设置 MCP 配置后，应对其进行测试以确保设置正确。\\n\\n在存储库中创建问题，然后将其分配给 Copilot。\\n等待几秒钟，Copilot 会对议题做出 👀 反应。\\n再等待几秒钟，Copilot 会创建拉取请求，该请求会显示在议题的时间线中。\\n单击时间线中创建的拉取请求，等待“Copilot started work”时间线事件出现。\\n**** 单击“View session”，打开 Copilot 编码智能体 日志。\\n******** 单击日志查看器右上角的省略号按钮 (...)，然后单击边栏中的 Copilot。\\n**** 单击“Start MCP Servers”步骤，展开日志。\\n如果 MCP 服务器成功启动，日志底部会列出其工具。\\n\\n如果 MCP 服务器需要任何 GitHub Actions 运行程序上未默认安装的依赖项，如 uv 和 pipx，或需要专门设置步骤的依赖项，则可能需要创建 copilot-setup-steps.yml 操作工作流文件来安装它们。 有关详细信息，请参阅“自定义 Copilot 编码代理的开发环境”。\\n自定义内置 GitHub MCP 服务器\\nGitHub MCP 服务器默认处于启用状态，为 Copilot 提供访问 GitHub 数据（如问题和拉取请求）的权限。\\nMCP 服务器默认通过特定作用范围的令牌连接到 GitHub，该令牌对当前存储库仅具有只读访问权限。\\n如果要允许 Copilot 访问当前存储库以外的数据，可以向其提供具有更广泛访问权限的 personal access token。\\n\\n\\n创建具有适当权限的 personal access token。 建议使用 fine-grained personal access token，可在其中将令牌访问权限限为特定存储库的只读权限。 有关 personal access tokens 的详细信息，请参阅 管理个人访问令牌。\\n\\n\\n在 GitHub 上，导航到存储库的主页面。\\n\\n\\n在存储库名称下，单击 “设置”。 如果看不到“设置”选项卡，请选择“”下拉菜单，然后单击“设置”。\\n\\n\\n\\n在边栏的“Code & automation”部分，单击 Copilot，然后单击 Copilot 代理********。\\n\\n\\n**** 在“MCP configuration”部分中添加配置。\\n\\n\\n单击“ 保存”。\\n\\n\\n在左侧边栏中，单击“环境”。\\n\\n\\n单击 copilot 环境。\\n\\n\\n**** 在“Environment secrets”下，单击“Add environment secret”。\\n\\n\\n**** 调用机密 COPILOT_MCP_GITHUB_PERSONAL_ACCESS_TOKEN，在“Value”字段中输入 personal access token，然后单击“Add secret”。\\n\\n\\n有关如何在其他环境中配置 GitHub MCP 服务器的信息，请参阅“使用 GitHub MCP 服务器”。\\n最佳做法\\n\\n\\n启用第三方 MCP 服务器可能会影响代理的性能和输出的质量。 详尽审查第三方 MCP 服务器，确保其满足组织的要求。\\n\\n\\n默认情况下，Copilot 编码智能体 无权编写 MCP 服务器工具。 但是，某些 MCP 服务器确实包含此类工具。 请务必查看要使用的 MCP 服务器中可用的工具。 仅使用必要的工具更新 MCP 配置中的 tools 字段。\\n\\n\\n在保存配置之前，请仔细查看配置的 MCP 服务器，以确保配置了正确的服务器。\\n\\n\\n其他阅读材料\\n\\n自定义 Copilot 编码代理的开发环境\\n使用模型上下文协议 (MCP) 扩展 Copilot 对话助手\\n帮助和支持是否找到了所需的内容？ 是 否隐私策略仍需帮助？询问 GitHub 社区联系支持人员Legal此内容中的一些内容可能是机器翻译的或 AI 翻译的内容。© 2025 GitHub, Inc.术语隐私状态定价专家服务博客\\n\\n================================================================================\\n\\n================================================================================\\nSource: The Model Context Protocol (MCP) — A Complete Tutorial | by Dr ...\\n--------------------------------------------------------------------------------\\nURL: https://medium.com/@nimritakoul01/the-model-context-protocol-mcp-a-complete-tutorial-a3abe8a7f4ef\\n===\\nMost relevant content from source: Mar 27, 2025 ... Programming Language Syntax: The model understands language-specific patterns. External Documentation: Some models use APIs or docs for more\\xa0...\\n===\\nFull source content limited to 4000 tokens: The Model Context Protocol (MCP) — A Complete Tutorial | by Dr. Nimrita Koul | MediumSitemapOpen in appSign upSign inMedium LogoWriteSign upSign inThe Model Context Protocol (MCP) — A Complete TutorialDr. Nimrita KoulFollow25 min read·Mar 27, 2025--5ListenShareAnthropic released the Model Context Protocol(MCP) in Nov. 2024.It is developed by Mahesh Murag at Anthropic. Find the complete official documentation. At present, MCP is fully implemented as Python SDK and TypeScript SDK.Mahesh Murag delivered a great workshop on “Building Agents with Model Context Protocol” during AI Engineer Summit.Context is the keyThe basic capabilities of a Generative AI model depend on its pretraining details, the training data, and the model architecture. To make these pretrained models perform better and improve their relevance and coherence to your task, you must provide a good context to it.Here context refers to the information the model uses to generate relevant and coherent responses. Context determines how the model understands and continues a conversation, completes a text, or generates an image.Context can be provided in different ways, depending on the type of model and task:Text-Based Models (e.g., GPT, DeepSeek, LLaMA) receive their context through:Prompt Context: The input text or query that guides the model’s response.Token Window: The number of tokens the model can “remember” at a time (e.g., GPT-4-Turbo can handle ~128K tokens).Conversation History: In chatbots, previous exchanges help maintain context in multi-turn dialogues.Retrieval-Augmented Generation (RAG): Context from external documents retrieved dynamically to improve responses.2. Image and Multimodal Models (e.g., DALL·E, Gemini) receive their context through:Text Descriptions: The prompt guides image generation.Visual Context: If an image is provided, the model analyzes its content before generating new elements.Cross-Modal Context: When combining text and images, models interpret both to generate meaningful outputs.3. Code Generation Models (e.g., Codex, DeepSeek-Coder) receive their context through:Previous Code Blocks: Context includes existing code, function names, and comments.Programming Language Syntax: The model understands language-specific patterns.External Documentation: Some models use APIs or docs for more accurate suggestions.4. Speech and Audio Models (e.g., Whisper, AudioPaLM) receive their context through:Audio Segments: Prior speech or music informs the next generated part.Linguistic and Acoustic Features: Tone, speed, and intonation influence transcription and generation.In short, context is the key factor that enables generative AI to produce relevant and coherent outputs. The better the context management, the better the AI’s performance.Over time the AI models can auto fetch data to act as context. This is especially true of AI Agents which are the systems that use generative AI models at their core. This means the AI Agents have to search for data sources, request the sources for specific data and so on.https://www.anthropic.com/engineering/building-effective-agentsEach data source (server) is implemented is its own way (for example, as open source packages in another codebase — rather than emitting messages that can be consumed by anyone. Or these can be implemented as JSON RPC for messages) so there is no standard way for an AI model (client) to search for and request for data. (Fragmentation.)Before MCP, building AI systems often involved:Custom implementations for each AI application to hook into its required context, leading to a lot of duplicated effort.Inconsistent prompt logic and different methods for accessing and federating tools and data across different teams and companies.The “N times M problem” where a large number of client applications needed to interact with a large number of servers and tools, resulting in a complex web of integrations, each requiring specific development work.Model Context Protocol (MCP) solves this problem of fragmented data access.The MCP provides a open standard for connecting AI systems with data sources and tools (repositories, business tools, development environments), replacing fragmented integrations with a single protocol. Thus MCP provides fungibility between AI clients and servers.Thus MCP provides a standardized way for applications to:Share contextual information with language modelsExpose tools and capabilities to AI systemsBuild composable integrations and workflowsThe protocol uses JSON-RPC 2.0 messages to establish communication between:Hosts: LLM applications that initiate connectionsClients: Connectors within the host applicationServers: Services that provide context and capabilitiesThere are a number of popular AI tools that support MCP, including:CursorWindsurf (Codium)Cline (VS Code extension)Claude desktopClaude codeMCP takes some inspiration from the Language Server Protocol, which standardizes how to add support for programming languages across a whole ecosystem of development tools. In a similar way, MCP standardizes how to integrate additional context and tools into the ecosystem of AI applications.Architecture of MCPMCP follows a client-host-server architecture where each host can run multiple client instances.This architecture enables users to integrate AI capabilities across applications while maintaining clear security boundaries and isolating concerns.Built on JSON-RPC, MCP provides a stateful session protocol focused on context exchange and sampling coordination between clients and servers.https://spec.modelcontextprotocol.io/specification/2024-11-05/architecture/HostThe host process acts as the container and coordinator:Creates and manages multiple client instancesControls client connection permissions and lifecycleEnforces security policies and consent requirementsHandles user authorization decisionsCoordinates AI/LLM integration and samplingManages context aggregation across clientsClientsEach client is created by the host and maintains an isolated server connection:Establishes one stateful session per serverHandles protocol negotiation and capability exchangeRoutes protocol messages bidirectionallyManages subscriptions and notificationsMaintains security boundaries between serversA host application creates and manages multiple clients, with each client having a 1:1 relationship with a particular server.MCP Clients are the AI applications or agents that want to access external systems, tools, or data sources. Examples include Anthropic’s first-party applications, Curser, Windsurf, and agents like Goose. The key characteristic of an MCP client is its MCP compatibility, meaning it is built to communicate using the standardised interfaces defined by the protocol: prompts, tools, and resources.Once an MCP client is compatible, it can connect to any MCP server with minimal or no additional work. The client is responsible for invoking tools, querying for resources, and interpolating prompts.In the context of tools, the language model within the client application decides when it is best to invoke the tools exposed by the server. For resources, the client application has control over how the data exposed by the server is used. Prompts are considered user-controlled tools invoked by the user through the client application.ServersServers provide specialized context and capabilities:Expose resources, tools and prompts via MCP primitivesOperate independently with focused responsibilitiesRequest sampling through client interfacesMust respect security constraintsCan be local processes or remote servicesMCP Servers act as wrappers or intermediaries that provide a standardised way to access various external systems, tools, and data sources. An MCP server can provide access to databases, CRMs like Salesforce, local file systems, and version control systems like Git. The role of the server builder is to expose tools, resources, and prompts in a way that is consumable by any compatible client. Once an MCP server is built, it can be adopted by any MCP client, solving the “N times M problem” by reducing the need for individualised integrations. For tools, the server defines the available functions and their descriptions, allowing the client’s model to decide when to use them. For resources, the server defines and potentially creates or retrieves data that it exposes to the client application. For prompts, the server provides predefined templates for common interactions that the client application can trigger on behalf of the user.The MCP protocol acts as the communication layer between these two components, standardising how requests and responses are structured and exchanged. This separation offers several benefits, as it allows:Seamless Integration: Clients can connect to a wide range of servers without needing to know the specifics of each underlying system.Reusability: Server developers can build integrations once and have them accessible to many different client applications.Separation of Concerns: Different teams can focus on building client applications or server integrations independently. For example, an infrastructure team can manage an mCP server for a vector database, which can then be easily used by various AI application development teams.In essence, the relationship between MCP clients and servers is one of standardised interaction, where clients leverage the capabilities exposed by servers through the common language of the MCP protocol, leading to a more efficient and scalable ecosystem for building AI applications and agents.MCP FeaturesMCP Server FeaturesMCP Servers provide the fundamental building blocks (Prompts, Resources, Tools) for adding context to language models via MCP. These primitives enable rich interactions between clients, servers, and language models:Prompts: Pre-defined templates or instructions that guide language model interactionsResources: Structured data or content that provides additional context to the modelTools: Executable functions that allow models to perform actions or retrieve informationhttps://spec.modelcontextprotocol.io/specification/2024-11-05/server/The Model Context Protocol (MCP) provides a standardized way for servers to expose prompts, resources, and tools to clients.Prompts (Protocol Revision: 2024–11–05)Prompts allow servers to provide structured messages and instructions for interacting with language models. Clients can discover available prompts, retrieve their contents, and provide arguments to customize them.Prompts are designed to be user-controlled, meaning they are exposed from servers to clients with the intention of the user being able to explicitly select them for use.Typically, prompts would be triggered through user-initiated commands in the user interface, which allows users to naturally discover and invoke available prompts. For example, as slash commands.Servers that support prompts MUST declare the prompts capability during initialization:https://spec.modelcontextprotocol.io/specification/2024-11-05/server/prompts/Resources : Protocol Revision: 2024–11–05Resources allow servers to share data that provides context to language models, such as files, database schemas, or application-specific information. Each resource is uniquely identified by a URI.Resources in MCP are designed to be application-driven, with host applications determining how to incorporate context based on their needs.For example, applications could:Expose resources through UI elements for explicit selection, in a tree or list viewAllow the user to search through and filter available resourcesImplement automatic context inclusion, based on heuristics or the AI model’s selectionServers that support resources MUST declare the resources capability:https://spec.modelcontextprotocol.io/specification/2024-11-05/server/resources/The capability supports two optional features:subscribe: whether the client can subscribe to be notified of changes to individual resources.listChanged: whether the server will emit notifications when the list of available resources changes.Tools — Protocol Revision: 2024–11–05MCP allows servers to expose tools that can be invoked by language models. Tools enable models to interact with external systems, such as querying databases, calling APIs, or performing computations. Each tool is uniquely identified by a name and includes metadata describing its schema.Tools in MCP are designed to be model-controlled, meaning that the language model can discover and invoke tools automatically based on its contextual understanding and the user’s prompts. However, implementations are free to expose tools through any interface pattern that suits their needs.Servers that support tools MUST declare the tools capability:https://spec.modelcontextprotocol.io/specification/2024-11-05/server/tools/listChanged indicates whether the server will emit notifications when the list of available tools changes.MCP Client FeaturesClients can implement additional features to enrich connected MCP servers: Roots and Sampling.RootsRoots define the boundaries of where servers can operate within the filesystem, allowing them to understand which directories and files they have access to. MCP provides a standardized way for clients to expose filesystem “roots” to servers. Servers can request the list of roots from supporting clients and receive notifications when that list changes.A root definition includes:uri: Unique identifier for the root. This MUST be a file:// URI in the current specification.name: Optional human-readable name for display purposes.Example roots for different use cases:https://spec.modelcontextprotocol.io/specification/2024-11-05/client/roots/Sampling (Protocol Revision: 2024–11–05)MCP provides a standardized way for servers to request LLM sampling (“completions” or “generations”) from language models via clients. This flow allows clients to maintain control over model access, selection, and permissions while enabling servers to leverage AI capabilities — with no server API keys necessary. Servers can request text or image-based interactions and optionally include context from MCP servers in their prompts.Sampling in MCP allows servers to implement agentic behaviors, by enabling LLM calls to occur nested inside other MCP server features.Implementations are free to expose sampling through any interface pattern that suits their needs — the protocol itself does not mandate any specific user interaction model.ComposabilityComposability in MCP highlights that the distinction between a client and a server is logical rather than physical. This means that any application, API, or agent can function as both an MCP client and an MCP server simultaneously.This dual role allows for the creation of layered and chained systems. A user might interact with a primary agent application (a client), which then communicates with a specialised sub-agent (acting as a server). This sub-agent, in turn, can act as a client and invoke other MCP servers (such as a file system server or a web search server) to fulfil its task.Relevance to Agents: Composability is crucial for building advanced, modular agent architectures. It enables the creation of hierarchical systems of agents, where different agents can specialise in specific tasks and delegate sub-tasks to other agents. For instance, an orchestrator agent can receive a high-level goal and then break it down into smaller tasks, delegating these tasks to research agents, coding agents, or fact-checking agents, each operating as an MCP server but also potentially acting as a client to access necessary tools and data. This allows for building complex workflows and intelligent behaviours by combining the capabilities of multiple specialised agents. It also allows for reusing and connecting to agents built by others, even if they were not initially part of the main agent’s design.In combination, sampling and composability are powerful enablers for advanced AI agents. They allow for:Distribution of in... [truncated]\\n\\n================================================================================\\n\\n================================================================================\\nSource: microsoft/mcp-for-beginners: This open-source curriculum ... - GitHub\\n--------------------------------------------------------------------------------\\nURL: https://github.com/microsoft/mcp-for-beginners\\n===\\nMost relevant content from source: This open-source curriculum is designed to teach the concepts and fundamentals of the Model Context Protocol (MCP), with practical examples in .\\n===\\nFull source content limited to 4000 tokens: \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub - microsoft/mcp-for-beginners: This open-source curriculum is designed to teach the concepts and fundamentals of the Model Context Protocol (MCP), with practical examples in .NET, Java, TypeScript, JavaScript and Python.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNavigation Menu\\n\\nToggle navigation\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Sign in\\n          \\n\\n\\n \\n\\n\\nAppearance settings\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Product\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            GitHub Copilot\\n          \\n        Write better code with AI\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n            GitHub Models\\n              \\n                New\\n              \\n\\n        Manage and compare prompts\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n            GitHub Advanced Security\\n          \\n        Find and fix vulnerabilities\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n            Actions\\n          \\n        Automate any workflow\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n            Codespaces\\n          \\n        Instant dev environments\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Issues\\n          \\n        Plan and track work\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n            Code Review\\n          \\n        Manage code changes\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n            Discussions\\n          \\n        Collaborate outside of code\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n            Code Search\\n          \\n        Find more, search less\\n      \\n\\n\\n\\n\\n\\n\\nExplore\\n\\n\\n\\n      Why GitHub\\n\\n    \\n\\n\\n\\n      All features\\n\\n    \\n\\n\\n\\n      Documentation\\n\\n    \\n\\n\\n\\n\\n\\n      GitHub Skills\\n\\n    \\n\\n\\n\\n\\n\\n      Blog\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Solutions\\n        \\n\\n\\n\\n\\n\\n\\nBy company size\\n\\n\\n\\n      Enterprises\\n\\n    \\n\\n\\n\\n      Small and medium teams\\n\\n    \\n\\n\\n\\n      Startups\\n\\n    \\n\\n\\n\\n      Nonprofits\\n\\n    \\n\\n\\n\\n\\nBy use case\\n\\n\\n\\n      DevSecOps\\n\\n    \\n\\n\\n\\n      DevOps\\n\\n    \\n\\n\\n\\n      CI/CD\\n\\n    \\n\\n\\n\\n      View all use cases\\n\\n    \\n\\n\\n\\n\\n\\n\\nBy industry\\n\\n\\n\\n      Healthcare\\n\\n    \\n\\n\\n\\n      Financial services\\n\\n    \\n\\n\\n\\n      Manufacturing\\n\\n    \\n\\n\\n\\n      Government\\n\\n    \\n\\n\\n\\n      View all industries\\n\\n    \\n\\n\\n\\n\\n\\n\\n              View all solutions\\n              \\n\\n\\n \\n\\n\\n\\n\\n        Resources\\n        \\n\\n\\n\\n\\n\\n\\nTopics\\n\\n\\n\\n      AI\\n\\n    \\n\\n\\n\\n      DevOps\\n\\n    \\n\\n\\n\\n      Security\\n\\n    \\n\\n\\n\\n      Software Development\\n\\n    \\n\\n\\n\\n      View all\\n\\n    \\n\\n\\n\\n\\n\\n\\nExplore\\n\\n\\n\\n      Learning Pathways\\n\\n    \\n\\n\\n\\n\\n\\n      Events & Webinars\\n\\n    \\n\\n\\n\\n\\n\\n      Ebooks & Whitepapers\\n\\n    \\n\\n\\n\\n      Customer Stories\\n\\n    \\n\\n\\n\\n      Partners\\n\\n    \\n\\n\\n\\n\\n\\n      Executive Insights\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n        Open Source\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            GitHub Sponsors\\n          \\n        Fund open source developers\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            The ReadME Project\\n          \\n        GitHub community articles\\n      \\n\\n\\n\\n\\nRepositories\\n\\n\\n\\n      Topics\\n\\n    \\n\\n\\n\\n      Trending\\n\\n    \\n\\n\\n\\n      Collections\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n        Enterprise\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Enterprise platform\\n          \\n        AI-powered developer platform\\n      \\n\\n\\n\\n\\nAvailable add-ons\\n\\n\\n\\n\\n\\n\\n\\n\\n            GitHub Advanced Security\\n          \\n        Enterprise-grade security features\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n            Copilot for business\\n          \\n        Enterprise-grade AI features\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n            Premium Support\\n          \\n        Enterprise-grade 24/7 support\\n      \\n\\n\\n\\n\\n\\n\\n\\nPricing\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch or jump to...\\n\\n\\n\\n\\n\\n\\n\\nSearch code, repositories, users, issues, pull requests...\\n\\n \\n\\n\\n\\n\\n        Search\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nClear\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\nSearch syntax tips \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Provide feedback\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\nWe read every piece of feedback, and take your input very seriously.\\n\\n\\nInclude my email address so I can be contacted\\n\\n\\n     Cancel\\n\\n    Submit feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Saved searches\\n      \\nUse saved searches to filter your results more quickly\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nName\\n\\n\\n\\n\\n\\n\\nQuery\\n\\n\\n\\n            To see all available qualifiers, see our documentation.\\n          \\n \\n\\n\\n\\n\\n\\n     Cancel\\n\\n    Create saved search\\n\\n\\n\\n\\n\\n\\n\\n\\n                Sign in\\n              \\n\\n\\n                Sign up\\n              \\n\\n\\n \\n\\n\\nAppearance settings\\n\\n\\n\\nResetting focus\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYou signed in with another tab or window. Reload to refresh your session.\\nYou signed out in another tab or window. Reload to refresh your session.\\nYou switched accounts on another tab or window. Reload to refresh your session.\\n \\n\\n\\nDismiss alert\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        microsoft\\n \\n/\\n\\nmcp-for-beginners\\n\\nPublic\\n\\n\\n\\n\\n\\n \\n\\nNotifications\\n You must be signed in to change notification settings\\n\\n\\n \\n\\nFork\\n    955\\n\\n\\n\\n\\n \\n\\n\\n          Star\\n 3.9k\\n\\n\\n\\n\\n\\n\\n\\n\\n        This open-source curriculum is designed to teach the concepts and fundamentals of the Model Context Protocol (MCP), with practical examples in .NET, Java, TypeScript, JavaScript and Python.\\n      \\nLicense\\n\\n\\n\\n\\n\\n     MIT license\\n    \\n\\n\\n\\n\\n\\n\\n3.9k\\n          stars\\n \\n\\n\\n\\n955\\n          forks\\n \\n\\n\\n\\nBranches\\n \\n\\n\\n\\nTags\\n \\n\\n\\n\\nActivity\\n \\n\\n\\n\\n \\n\\n\\n          Star\\n\\n\\n\\n\\n \\n\\nNotifications\\n You must be signed in to change notification settings\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCode\\n\\n\\n\\n\\n\\n\\n\\nIssues\\n1\\n\\n\\n\\n\\n\\n\\nPull requests\\n0\\n\\n\\n\\n\\n\\n\\nActions\\n\\n\\n\\n\\n\\n\\n\\nProjects\\n0\\n\\n\\n\\n\\n\\n\\nSecurity\\n\\n\\n\\n\\n\\n        Uh oh!\\n\\n There was an error while loading. Please reload this page.\\n\\n \\n \\n\\n\\n\\n\\n\\n\\n\\nInsights\\n\\n\\n\\n \\n\\n \\n\\n\\nAdditional navigation options\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Code\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Issues\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Pull requests\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Actions\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Projects\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Security\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Insights\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nmicrosoft/mcp-for-beginners\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n   \\xa0mainBranchesTagsGo to fileCodeOpen more actions menuFolders and filesNameNameLast commit messageLast commit dateLatest commit\\xa0History536 Commits.github.github\\xa0\\xa000-Introduction00-Introduction\\xa0\\xa001-CoreConcepts01-CoreConcepts\\xa0\\xa002-Security02-Security\\xa0\\xa003-GettingStarted03-GettingStarted\\xa0\\xa004-PracticalImplementation04-PracticalImplementation\\xa0\\xa005-AdvancedTopics05-AdvancedTopics\\xa0\\xa006-CommunityContributions06-CommunityContributions\\xa0\\xa007-LessonsfromEarlyAdoption07-LessonsfromEarlyAdoption\\xa0\\xa008-BestPractices08-BestPractices\\xa0\\xa009-CaseStudy09-CaseStudy\\xa0\\xa010-StreamliningAIWorkflowsBuildingAnMCPServerWithAIToolkit10-StreamliningAIWorkflowsBuildingAnMCPServerWithAIToolkit\\xa0\\xa0imagesimages\\xa0\\xa0translated_imagestranslated_images\\xa0\\xa0translationstranslations\\xa0\\xa0.dockerignore.dockerignore\\xa0\\xa0.gitignore.gitignore\\xa0\\xa0CODE_OF_CONDUCT.mdCODE_OF_CONDUCT.md\\xa0\\xa0LICENSELICENSE\\xa0\\xa0README.mdREADME.md\\xa0\\xa0SECURITY.mdSECURITY.md\\xa0\\xa0SUPPORT.mdSUPPORT.md\\xa0\\xa0study_guide.mdstudy_guide.md\\xa0\\xa0View all filesRepository files navigationREADMECode of conductMIT licenseSecurity\\n\\n\\n\\n\\n\\n\\n\\n\\nFollow these steps to get started using these resources:\\n\\nFork the Repository: Click \\nClone the Repository:   git clone https://github.com/microsoft/mcp-for-beginners.git\\nJoin The Azure AI Foundry Discord and meet experts and fellow developers\\n\\n🌐 Multi-Language Support\\nSupported via GitHub Action (Automated & Always Up-to-Date)\\nFrench | Spanish | German | Russian | Arabic | Persian (Farsi) | Urdu | Chinese (Simplified) | Chinese (Traditional, Macau) | Chinese (Traditional, Hong Kong) | Chinese (Traditional, Taiwan) | Japanese | Korean | Hindi | Bengali | Marathi | Nepali | Punjabi (Gurmukhi) | Portuguese (Portugal) | Portuguese (Brazil) | Italian | Polish | Turkish | Greek | Thai | Swedish | Danish | Norwegian | Finnish | Dutch | Hebrew | Vietnamese | Indonesian | Malay | Tagalog (Filipino) | Swahili | Hungarian | Czech | Slovak | Romanian | Bulgarian | Serbian (Cyrillic) | Croatian | Slovenian | Ukrainian | Burmese (Myanmar\\n🚀 Model Context Protocol (MCP) Curriculum for Beginners\\nLearn MCP with Hands-on Code Examples in C#, Java, JavaScript, Python, and TypeScript\\n🧠 Overview of the Model Context Protocol Curriculum\\nThe Model Context Protocol (MCP) is a cutting-edge framework designed to standardize interactions between AI models and client applications. This open-source curriculum offers a structured learning path, complete with practical coding examples and real-world use cases, across popular programming languages including C#, Java, JavaScript, TypeScript, and Python.\\nWhether you\\'re an AI developer, system architect, or software engineer, this guide is your comprehensive resource for mastering MCP fundamentals and implementation strategies.\\n🔗 Official MCP Resources\\n\\n📘 MCP Documentation – Detailed tutorials and user guides\\n📜 MCP Specification – Protocol architecture and technical references\\n🧑\\u200d💻 MCP GitHub Repository – Open-source SDKs, tools, and code samples\\n\\n🧭 MCP Curriculum Overview\\n\\n00-03: Foundations\\n\\n00. Introduction to MCP\\nOverview of the Model Context Protocol and its significance in AI pipelines. Read more\\n01. Core Concepts Explained\\nIn-depth exploration of core MCP concepts. Read more\\n02. Security in MCP\\nSecurity threats and best practices. Read more\\n03. Getting Started with MCP\\nEnvironment setup, basic servers/clients, integration. Read more\\n\\n\\n\\n03.x: Hands-On Labs\\n\\n3.1. First server – Guide\\n3.2. First client – Guide\\n3.3. Client with LLM – Guide\\n3.4. Consuming a server with Visual Studio Code – Guide\\n3.5. Creating a server using SSE – Guide\\n3.6. HTTP Streaming – Guide\\n3.7. Use AI Toolkit – Guide\\n3.8. Testing your server – Guide\\n3.9. Deploy your server – Guide\\n\\n\\n\\n04-05: Practical & Advanced\\n\\n04. Practical Implementation\\nSDKs, debugging, testing, reusable prompt templates. Read more\\n05. Advanced Topics in MCP\\nMulti-modal AI, scaling, enterprise use. Read more\\n5.1. MCP Integration with Azure – Guide\\n5.2. Multi modality – Guide\\n5.3. MCP OAuth2 Demo – Guide\\n5.4. Root Contexts – Guide\\n5.5. Routing – Guide\\n5.6. Sampling – Guide\\n5.7. Scaling – Guide\\n5.8. Security – Guide\\n5.9. Web Search MCP – Guide\\n5.10. Realtime Streaming – Guide\\n5.11. Realtime Web Search – Guide\\n\\n\\n\\n06-10: Community, Best Practices & Labs\\n\\n06. Community Contributions – Guide\\n07. Insights from Early Adoption – Guide\\n08. Best Practices for MCP – Guide\\n09. MCP Case Studies – Guide\\n10. Streamlining AI Workflows: Building an MCP Server with AI Toolkit – Hands On Lab\\n\\n\\nSample Projects\\n🧮 MCP Calculator Sample Projects:\\n\\nExplore Code Implementations by Language\\n\\nC# MCP Server Example\\nJava MCP Calculator\\nJavaScript MCP Demo\\nPython MCP Server\\nTypeScript MCP Example\\n\\n\\n💡 MCP Advanced Calculator Projects:\\n\\nExplore Advanced Samples\\n\\nAdvanced C# Sample\\nJava Container App Example\\nJavaScript Advanced Sample\\nPython Complex Implementation\\nTypeScript Container Sample\\n\\n\\n🎯 Prerequisites for Learning MCP\\nTo get the most out of this curriculum, you should have:\\n\\nBasic knowledge of C#, Java, or Python\\nUnderstanding of client-server model and APIs\\n(Optional) Familiarity with machine learning concepts\\n\\n📚 Study Guide\\nA comprehensive Study Guide is available to help you navigate this repository effectively. The guide includes:\\n\\nA visual curriculum map showing all topics covered\\nDetailed breakdown of each repository section\\nGuidance on how to use sample projects\\nRecommended learning paths for different skill levels\\nAdditional resources to complement your learning journey\\n\\n🛠️ How to Use This Curriculum Effectively\\nEach lesson in this guide includes:\\n\\nClear explanations of MCP concepts\\nLive code examples in multiple languages\\nExercises to build real MCP applications\\nExtra resources for advanced learners\\n\\n📜 License Information\\nThis content is licensed under the MIT License. For terms and conditions, see the LICENSE.\\n🤝 Contribution Guidelines\\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\\nprovided by the bot. You will only need to do this once across all repos using our CLA.\\nThis project has adopted the Microsoft Open Source Code of Conduct.\\nFor more information see the Code of Conduct FAQ or\\ncontact opencode@microsoft.com with any additional questions or comments.\\n🎒 Other Courses\\nOur team produces other courses! Check out:\\n\\nAI Agents For Beginners\\nGenerative AI for Beginners using .NET\\nGenerative AI for Beginners using JavaScript\\nGenerative AI for Beginners\\nML for Beginners\\nData Science for Beginners\\nAI for Beginners\\nCybersecurity for Beginners\\nWeb Dev for Beginners\\nIoT for Beginners\\nXR Development for Beginners\\nMastering GitHub Copilot for AI Paired Programming\\nMastering GitHub Copilot for C#/.NET Developers\\nChoose Your Own Copilot Adventure\\n\\n™️ Trademark Notice\\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\\ntrademarks or logos is subject to and must follow\\nMicrosoft\\'s Trademark & Brand Guidelines.\\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\\nAny use of third-party trademarks or logos is subject to those third-parties\\' policies.\\n   \\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\n\\n        This open-source curriculum is designed to teach the concepts and fundamentals of the Model Context Protocol (MCP), with practical examples in .NET, Java, TypeScript, JavaScript and Python.\\n      \\nTopics\\n\\n\\n\\n  javascript\\n\\n\\n  python\\n\\n\\n  java\\n\\n\\n  csharp\\n\\n\\n  model\\n\\n\\n  mcp\\n\\n\\n  mcp-server\\n\\n\\n  modelcontextprotocol\\n\\n\\n  mcp-client\\n\\n\\n  mcp-security\\n\\n\\n\\nResources\\n\\n\\n\\n\\n\\n        Readme\\n \\nLicense\\n\\n\\n\\n\\n\\n     MIT license\\n    \\n\\nCode of conduct\\n\\n\\n\\n\\n\\n        Code of conduct\\n      \\n\\nSecurity policy\\n\\n\\n\\n\\n\\n        Security policy\\n      \\n\\n\\n\\n\\n\\n\\n        Uh oh!\\n\\n There was an error while loading. Please reload this page.\\n\\n \\n \\n\\n\\n\\n\\n\\nActivity \\n\\n\\n\\n\\nCustom properties \\nStars\\n\\n\\n\\n\\n3.9k\\n      stars \\nWatchers\\n\\n\\n\\n\\n43\\n      watching \\nForks\\n\\n\\n\\n\\n955\\n      forks \\n\\n\\n          Report repository\\n \\n\\n\\n\\n\\n\\n\\nReleases\\nNo releases published\\n\\n\\n\\n\\n\\nPackages\\n      0\\n\\n        No packages published \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Uh oh!\\n\\n There was an error while loading. Please reload this page.\\n\\n \\n \\n\\n\\n\\n\\n\\n\\nContributors\\n      27\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Uh oh!\\n\\n There was an error while loading. Please reload this page.\\n\\n \\n \\n\\n\\n+ 13 contributors\\n\\n\\n\\n\\nLanguages\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPython\\n37.3%\\n\\n\\n\\n\\n\\n\\n\\nJava\\n33.9%\\n\\n\\n\\n\\n\\n\\n\\nC#\\n9.6%\\n\\n\\n\\n\\n\\n\\n\\nTypeScript\\n9.3%\\n\\n\\n\\n\\n\\n\\n\\nHTML\\n5.5%\\n\\n\\n\\n\\n\\n\\n\\nJavaScript\\n3.5%\\n\\n\\n\\n\\n\\n\\n\\nDockerfile\\n0.9%\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFooter\\n\\n\\n\\n\\n\\n\\n\\n\\n        © 2025 GitHub,\\xa0Inc.\\n      \\n\\n\\nFooter navigation\\n\\n\\nTerms\\n\\n\\nPrivacy\\n\\n\\nSecurity\\n\\n\\nStatus\\n\\n\\nDocs\\n\\n\\nContact\\n\\n\\n\\n\\n       Manage cookies\\n    \\n\\n\\n\\n\\n\\n      Do not share my personal information\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    You can’t perform that action at this time.\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n================================================================================\\n\\n================================================================================\\nSource: Model Context Protocol(MCP) with Google Gemini 2.5 Pro - Deep ...\\n--------------------------------------------------------------------------------\\nURL: https://medium.com/google-cloud/model-context-protocol-mcp-with-google-gemini-llm-a-deep-dive-full-code-ea16e3fac9a3\\n===\\nMost relevant content from source: Apr 4, 2025 ... A step-by-step guide with code, architecture, and real-world use case. As Large Language Models (LLMs) like GPT-4, Claude, Gemini and Llama3\\xa0...\\n===\\nFull source content limited to 4000 tokens: Model Context Protocol(MCP) with Google Gemini 2.5 Pro - Deep Dive , Google Cloud Gen AI  | Google Cloud - CommunitySitemapOpen in appSign upSign inMedium LogoWriteSign upSign inGoogle Cloud - Community·Follow publicationA collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don\\'t necessarily reflect those of Google.Follow publicationModel Context Protocol(MCP) with Google Gemini 2.5 Pro — A Deep Dive (Full Code)Arjun PrabhulalFollow10 min read·Apr 4, 2025--19ListenShareA step-by-step guide with code, architecture, and real-world use caseAs Large Language Models (LLMs) like GPT-4, Claude, Gemini and Llama3 evolve , we need standardized ways to connect them to tools, APIs, and systems .However, these models operate in isolation based on pre-trained data and don’t have built-in access to real-time data, databases , external APIs, or local files.In my previous article on Function Calling, I explained how it allows models to translate natural language into real-world actions and data access.While Function Calling provides the core capability, the Model Context Protocol (MCP) takes it a step further — making the entire process structured, flexible, and interoperable across a wide range of tools and systems.In this article , we’ll discuss more on Model Context Protocol (MCP) concept before diving into implementation :1. What is Model Context Protocol (MCP) ?The Model Context Protocol (MCP) is a standardized, open protocol developed by Anthropic that enables AI models to seamlessly interact with external data sources and tools, acting as a universal connector for AI integrations.Think of MCP as a “USB-C for AI integrations,” providing a universal way for AI models to connect to different devices and data sourcesHow MCP works ?MCP follows a client-server architecture, where:Clients (like AI applications or LLMs) connect toServers (MCP tool providers) expose tools, APIs, or data sources to clients.This enables dynamic and structured interactions between LLM models and the external API’s.Benefits of MCP:Standardized Integration: Connect LLMs to any external system with less custom work.Flexibility: LLMs can use multiple tools and services — on demand.Security: Supports secure API interaction without hardcoding credentials.Simplified Development: Build and expose custom MCP servers easily.Easier Maintenance: No more repetitive integration logic.MCP transforms the API into a model-friendly tool, complete with auto-discovery, a predictable schema, and structured interaction.Examples of MCP Servers:File System: Accessing local files and directories.Web Search: Run real-time web searches.Databases: Query SQL or NoSQL databasesCRMs: Connecting to CRM systems like Salesforce.Version Control: Accessing version control systems like GitWhen to use Model Context Protocol (MCP) ?MCP to be used when:We’re building agentic systemsWe want tools to be modular, reusable, discoverableWe want use multiple external sources -We want scaling to multiple tools or toolchainsArchitectureBelow project integrates multiple components to enable natural language flight search using Gemini + MCPImage by AuthorComponent Interactions1. User to ClientUser provides natural language query (e.g., “Find flights from Atlanta to Las Vegas tomorrow”)Client script (`client.py`) processes the input2. Client to MCP ServerClient starts the MCP server process (`mcp-flight-search`)Establishes stdio communication channelRetrieves available tools and their descriptions3. Client to Gemini APISends the user’s queryProvides tool descriptions for function callingReceives structured function call with extracted parameters4. Client to MCP ToolTakes function call parameters from GeminiCalls appropriate MCP tool with parametersHandles response processing5. MCP Server to SerpAPIMCP server makes requests to SerpAPIQueries Google Flights dataProcesses and formats flight informationImplementationLet us dive into building this pipeline with Gemini AI by breaking down into key implementation stepsPre-RequisitesPython 3.8+ installed2. Google Gemini Generative AI access via API key3. A valid SerpAPI key (used to fetch live flight data)Step 1 : Setup virtual environmentInstall the dependancies#Setup virtual envpython -n venv venv #Activate venvsource venv/bin/activate#Install dependanciespip install google-genai mcpgoogle-genai: The official Python library for interacting with Google\\'s Generative AI models (like Gemini).mcp: A Python SDK for interacting with an MCP (Model Context Protocol) server. This SDK likely provides functionalities to communicate with external tools or services.Set Environment variablesexport GEMINI_API_KEY=\"your-google-api-key\"export SERP_API_KEY=\"your-serpapi-key\"Step 2: Install the MCP Server — mcp-flight-searchTo enable Gemini to interact with real-world APIs, we’ll use an MCP-compliant serverFor this article, we’ll use mcp-flight-search — a lightweight MCP Server built using FastMCP which exposes a tool that searches real-time flight data using the SerpAPI.Install MCP server package where I published to PyPi https://pypi.org/project/mcp-flight-search/# Install from PyPIpip install mcp-flight-searchlet us verify if MCP server package is installed successfullyStep 3 : Understanding MCP Tool PackagesImport library which initializes both Gemini and MCP SDKs and prepares for async execution.from google import genaiAbove imports the genai module from the google-generativeai library. It provides access to Google’s powerful LLMs, such as Gemini 1.5 and 2.0,2.5 model and includes client methods to interact with models using natural language.from google.genai import typesAbove module gives access to the type definitions and configuration structures used by the Gemini API. For example:• Tool: Defines tools (functions) that the model can call.GenerateContentConfig: Allows us to configure how the model responds (e.g., temperature, tool support, etc.).from mcp import ClientSession, StdioServerParametersAbove classes come from the mcp-sdk-python library and are essential for interacting with MCP servers:• ClientSession: Manages the communication session between our client/app and the MCP server.StdioServerParameters: stdio allows the server to be language-neutral and easily embedded in different environments.from mcp.client.stdio import stdio_clientThis imports the stdio_client, an asynchronous context manager used to establish a connection with an MCP server over standard I/O. It ensures that the server is correctly launched, and the client is ready to send/receive structured requests.from google import genaifrom google.genai import typesfrom mcp import ClientSession, StdioServerParametersfrom mcp.client.stdio import stdio_clientAbove 4 key imports together form the backbone of how we bridge Gemini’s LLM interaction with real-world APIs exposed via MCP tools.Step 4: Initialize Gemini Clientclient = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))genai.Client() is the primary interface used to interact with Google’s generative models (e.g., Gemini 2.5 Pro, Gemini 2 Flash)Once GenAI Client is initialized, this client object can:Send prompts to Gemini modelsPass tool definitions (function calling)Receive structured responses and function call objectsStep 5 : Configure MCP Tool ServerBelow block sets up the parameters required to launch and communicate with the MCP server that exposes tools (in our case, a flight search function).server_params = StdioServerParameters(    command=\"mcp-flight-search\",    args=[\"--connection_type\", \"stdio\"],    env={\"SERP_API_KEY\": os.getenv(\"SERP_API_KEY\")},)mcp-flight-search — This is the CLI entry point to run local MCP server,or could be a Python module in our case that implements the MCP protocol.stdio — This tells the server to use standard input/output (stdio) as its communication channel. Stdio is simple, language-agnostic, and great for running tool servers locally or in subprocesses.SERP_API_KEY — This passes an environment variable (SERP_API_KEY) to the subprocess running the tool. In our case, the tool needs it to authenticate with SerpAPI, which fetches real-time flight dataOnce server_params is defined, we can use it to spin up the server using the stdio_client async context manager.Python 3.11.11 (main, Dec  3 2024, 17:20:40) [Clang 16.0.0 (clang-1600.0.26.4)] on darwinType \"help\", \"copyright\", \"credits\" or \"license\" for more information.>>> import os>>> from google import genai>>> from google.genai import types>>> from mcp import ClientSession, StdioServerParameters>>> from mcp.client.stdio import stdio_client>>>>>> client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))>>>>>> server_params = StdioServerParameters(...     command=\"mcp-flight-search\",...     args=[\"--connection_type\", \"stdio\"],...     env={\"SERP_API_KEY\": os.getenv(\"SERP_API_KEY\")},... )>>> server_paramsStdioServerParameters(command=\\'mcp-flight-search\\', args=[\\'--connection_type\\', \\'stdio\\'], env={\\'SERP_API_KEY\\':\\'XXXXXXXXX\\'}, cwd=None, encoding=\\'utf-8\\', encoding_error_handler=\\'strict\\')>>>The Gemini client handles language understanding, prompt generation, and function calling.The MCP tool server (flight search) listens for tool calls and executes them in real time via SerpAPI.Step 6 : Connecting to MCP Server and listing toolsBelow block of code does three important stepsStarts connection with the MCP server ,Initializes a session for structured tool communication andDynamically discovers and formats available tools for Gemini.async def run():    # Remove debug prints    async with stdio_client(server_params) as (read, write):        async with ClientSession(read, write) as session:            prompt = f\"Find Flights from Atlanta to Las Vegas 2025-05-05\"            await session.initialize()            # Remove debug prints            mcp_tools = await session.list_tools()            # Remove debug prints            tools = [                types.Tool(                    function_declarations=[                        {                            \"name\": tool.name,                            \"description\": tool.description,                            \"parameters\": {                                k: v                                for k, v in tool.inputSchema.items()                                if k not in [\"additionalProperties\", \"$schema\"]                            },                        }                    ]                )                for tool in mcp_tools.tools            ]            # Remove debug prints            response = client.models.generate_content(                model=\"gemini-2.5-pro-exp-03-25\",                contents=prompt,                config=types.GenerateContentConfig(                    temperature=0,                    tools=tools,                ),            )Let us breakdown line-by-line to understand how the MCP Client-Server communication is happening under the hood along with Gemini LLMstdio_client is an asynchronous context manager that handles:Launching the MCP server as a subprocessManaging the input/output streams for message exchangeread and write objects are asynchronous streamsread: reads responses or tool registration from the serverwrite: sends requests or tool invocations to the server prompt = f\"Find Flights from Atlanta to Las Vegas 2025-05-05\"Above prompt is natural language query we’ll send to the Gemini model. which Gemini will later turn into a structured tool call.await session.initialize()session.initialize() is the one which triggers the initial MCP handshake between our client and the server.server registers its available tools (in our case: a flight search tool).Server registers its available tools (in our case: a flight search tool).Session is now ready to list, call, and execute tools. mcp_tools = await session.list_tools()Above requests the list of all tools (functions) exposed by the server.Each tool in mcp_tools.tools contains:A nameA descriptionAn input schema (i.e., what parameters it accepts, in JSON Schema format)mcp_tools.tools makes the MCP server self-describing, so that LLM can automatically understand how to call each tool.tools = [    types.Tool(        function_declarations=[            {                \"name\": tool.name,                \"description\": tool.description,                \"parameters\": {                    k: v                    for k, v in tool.inputSchema.items()                    if k not in [\"additionalProperties\", \"$schema\"]                },            }        ]    )    for tool in mcp_tools.tools]Above step converts the MCP tool definitions into Gemini’s function_declarations format.Now that our MCP server is running and session is initialized to discover tools from MCP Server for Gemini to useStep 6 : Gemini — Interprets Prompt and suggest a Function Callresponse = client.models.generate_content(    model=\"gemini-2.5-pro-exp-03-25\",    contents=prompt,    config=types.GenerateContentConfig(        temperature=0,        tools=tools,    ),)Finally the user’s prompt is sent to the Gemini model, along with a list of available tools discovered from the MCP server.If Gemini recognizes the prompt as matching a function’s schema, it returns a function_call object that includes the tool name and the auto-filled parameters.result = await session.call_tool(    function_call.name, arguments=dict(function_call.args))Step 7 : Gemini LLM final responseIf Gemini determines that the prompt aligns with a function (based on name, description, or parameters), it returns a structured function_call object like:{  \"function_call\": {    \"name\": \"search_flights\",    \"args\": {      \"source\": \"ATL\",      \"destination\": \"LAS\",      \"date\": \"2025-05-05\"    }  }}Gemini LLM transitions from a passive text model to an active decision-maker that:Interprets natural inputSelects an appropriate toolFills in the function’s arguments automaticallyWe didn’t write any parsing logic.Gemini LLM model filled in all the fields by interpreting the user’s natural language.function call is structured and ready for execution.Final Demo: Gemini 2.5 Pro with MCPBelow debug logs show exactly how Gemini , Model Context Protocol (MCP) work together to interpret user intent, match a tool, and return real-time data.Best Practices for Using Model Context Protocol (MCP) with Gemini LLMTool DesignClear Tool Names: Use short, meaningful names (e.g., search_flights, get_weather).Describe Each Tool Well: Provide simple, clear descriptions — the model uses this to decide when and how to call the tool.Use Strong Typing: Define input parameters explicitly (e.g., string, enum, number) to help the model fill them accurately.2. Model InteractionFewer Tools means Better Accuracy: Avoid overloading the model — stick to relevant tools only.Dynamic Tool Loading: Load tools based on the user’s query or conversation context.Prompt the Model Clearly: Set the model’s role and explain how and when to use the tools.• Prompt the Model Clearly: Set the model’s role and explain how and when to use the tools.3. Server SetupUse stdio for Simplicity: Start MCP servers using — connection_type stdio for easy local development.Pass Environment Variables Safely: Use env to send keys like SERP_API_KEY securely to tool servers.4. Request HandlingInitialize Session First: Always run session.initialize() before listing or calling tools.List Tools Dynamically: Use session.list_tools() to keep client flexible and tool-agnostic.5. Error Handling & SecurityReturn Helpful Errors: Make sure tool server responds with meaningful messages when something fails.Secure APIs: Never expose secrets like API keys in logs or error messages.GitHub Repository:You can access all the code used in this tutorial in my GitHub:GitHub - arjunprabhulal/mcp-gemini-search: natural language flight search system using Google\\'s…natural language flight search system using Google\\'s Gemini LLM with Model Context Protocol (MCP) . Convert…github.comLimitationsAs of this ... [truncated]\\n\\n================================================================================', name='google_search_tool', id='b44f7a2e-5943-4a44-86d8-19d0ec290ee7', tool_call_id='e9c2a61f-0e2b-400b-a1b1-8c8e50caae42'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'Sections', 'arguments': '{\"sections\": [\"\\\\u6a21\\\\u578b\\\\u4e0a\\\\u4e0b\\\\u6587\\\\u534f\\\\u8bae\\\\uff08MCP\\\\uff09\\\\u7684\\\\u57fa\\\\u672c\\\\u6982\\\\u5ff5\", \"MCP\\\\u7684\\\\u4e3b\\\\u8981\\\\u7528\\\\u9014\", \"MCP\\\\u7684\\\\u67b6\\\\u6784\\\\u548c\\\\u5de5\\\\u4f5c\\\\u539f\\\\u7406\", \"MCP\\\\u7684\\\\u4f18\\\\u52bf\\\\u548c\\\\u5e94\\\\u7528\\\\u573a\\\\u666f\", \"MCP\\\\u7684\\\\u672a\\\\u6765\\\\u53d1\\\\u5c55\"]}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite-preview-06-17', 'safety_ratings': []}, id='run--5849885a-8378-49f6-8836-cbf6c0b6026e-0', tool_calls=[{'name': 'Sections', 'args': {'sections': ['模型上下文协议（MCP）的基本概念', 'MCP的主要用途', 'MCP的架构和工作原理', 'MCP的优势和应用场景', 'MCP的未来发展']}, 'id': 'c8334d12-b4ff-49b0-975f-d01843e5efe2', 'type': 'tool_call'}], usage_metadata={'input_tokens': 29770, 'output_tokens': 47, 'total_tokens': 29817, 'input_token_details': {'cache_read': 0}}), ToolMessage(content=\"sections=['模型上下文协议（MCP）的基本概念', 'MCP的主要用途', 'MCP的架构和工作原理', 'MCP的优势和应用场景', 'MCP的未来发展']\", name='Sections', id='e2cad855-d7b1-46c4-8bd1-7b087dd8151c', tool_call_id='c8334d12-b4ff-49b0-975f-d01843e5efe2'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'Introduction', 'arguments': '{\"name\": \"\\\\u6a21\\\\u578b\\\\u4e0a\\\\u4e0b\\\\u6587\\\\u534f\\\\u8bae\\\\uff08MCP\\\\uff09\\\\u6982\\\\u8ff0\", \"content\": \"\\\\u6a21\\\\u578b\\\\u4e0a\\\\u4e0b\\\\u6587\\\\u534f\\\\u8bae\\\\uff08MCP\\\\uff09\\\\u4f5c\\\\u4e3a\\\\u4e00\\\\u79cd\\\\u5f00\\\\u653e\\\\u6807\\\\u51c6\\\\u548c\\\\u5f00\\\\u6e90\\\\u6846\\\\u67b6\\\\uff0c\\\\u65e8\\\\u5728\\\\u9769\\\\u65b0\\\\u5927\\\\u578b\\\\u8bed\\\\u8a00\\\\u6a21\\\\u578b\\\\uff08LLM\\\\uff09\\\\u4e0e\\\\u5916\\\\u90e8\\\\u5de5\\\\u5177\\\\u3001\\\\u7cfb\\\\u7edf\\\\u53ca\\\\u6570\\\\u636e\\\\u6e90\\\\u7684\\\\u4ea4\\\\u4e92\\\\u65b9\\\\u5f0f\\\\u3002\\\\u5b83\\\\u901a\\\\u8fc7\\\\u63d0\\\\u4f9b\\\\u4e00\\\\u4e2a\\\\u7edf\\\\u4e00\\\\u7684\\\\u63a5\\\\u53e3\\\\uff0c\\\\u89e3\\\\u51b3\\\\u4e86AI\\\\u7cfb\\\\u7edf\\\\u5728\\\\u4fe1\\\\u606f\\\\u5b64\\\\u5c9b\\\\u548c\\\\u788e\\\\u7247\\\\u5316\\\\u96c6\\\\u6210\\\\u65b9\\\\u9762\\\\u9762\\\\u4e34\\\\u7684\\\\u6311\\\\u6218\\\\uff0c\\\\u4f7f\\\\u5f97AI\\\\u6a21\\\\u578b\\\\u80fd\\\\u591f\\\\u66f4\\\\u6709\\\\u6548\\\\u5730\\\\u8bbf\\\\u95ee\\\\u6587\\\\u4ef6\\\\u3001\\\\u6267\\\\u884c\\\\u529f\\\\u80fd\\\\u5e76\\\\u5904\\\\u7406\\\\u4e0a\\\\u4e0b\\\\u6587\\\\u4fe1\\\\u606f\\\\u3002\\\\u672c\\\\u62a5\\\\u544a\\\\u5c06\\\\u6df1\\\\u5165\\\\u63a2\\\\u8ba8MCP\\\\u7684\\\\u57fa\\\\u672c\\\\u6982\\\\u5ff5\\\\u3001\\\\u4e3b\\\\u8981\\\\u7528\\\\u9014\\\\u3001\\\\u67b6\\\\u6784\\\\u4e0e\\\\u5de5\\\\u4f5c\\\\u539f\\\\u7406\\\\u3001\\\\u4f18\\\\u52bf\\\\u4e0e\\\\u5e94\\\\u7528\\\\u573a\\\\u666f\\\\uff0c\\\\u5e76\\\\u5c55\\\\u671b\\\\u5176\\\\u672a\\\\u6765\\\\u53d1\\\\u5c55\\\\u8d8b\\\\u52bf\\\\u3002\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite-preview-06-17', 'safety_ratings': []}, id='run--fd11d77d-3678-4295-994e-eecb37cc6f3e-0', tool_calls=[{'name': 'Introduction', 'args': {'name': '模型上下文协议（MCP）概述', 'content': '模型上下文协议（MCP）作为一种开放标准和开源框架，旨在革新大型语言模型（LLM）与外部工具、系统及数据源的交互方式。它通过提供一个统一的接口，解决了AI系统在信息孤岛和碎片化集成方面面临的挑战，使得AI模型能够更有效地访问文件、执行功能并处理上下文信息。本报告将深入探讨MCP的基本概念、主要用途、架构与工作原理、优势与应用场景，并展望其未来发展趋势。'}, 'id': '754caf44-d57c-417f-bf95-e70e08625949', 'type': 'tool_call'}], usage_metadata={'input_tokens': 32575, 'output_tokens': 127, 'total_tokens': 32702, 'input_token_details': {'cache_read': 29495}}), ToolMessage(content=\"name='模型上下文协议（MCP）概述' content='模型上下文协议（MCP）作为一种开放标准和开源框架，旨在革新大型语言模型（LLM）与外部工具、系统及数据源的交互方式。它通过提供一个统一的接口，解决了AI系统在信息孤岛和碎片化集成方面面临的挑战，使得AI模型能够更有效地访问文件、执行功能并处理上下文信息。本报告将深入探讨MCP的基本概念、主要用途、架构与工作原理、优势与应用场景，并展望其未来发展趋势。'\", name='Introduction', id='4d6bbc9a-eaca-4ab8-a192-b83c1f849e3c', tool_call_id='754caf44-d57c-417f-bf95-e70e08625949'), HumanMessage(content='Introduction written. Now write a conclusion section.', additional_kwargs={}, response_metadata={}, id='056c232e-9158-4850-b1c1-57d57a407e29'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'Conclusion', 'arguments': '{\"name\": \"\\\\u6a21\\\\u578b\\\\u4e0a\\\\u4e0b\\\\u6587\\\\u534f\\\\u8bae\\\\uff08MCP\\\\uff09\\\\u603b\\\\u7ed3\", \"content\": \"\\\\u6a21\\\\u578b\\\\u4e0a\\\\u4e0b\\\\u6587\\\\u534f\\\\u8bae\\\\uff08MCP\\\\uff09\\\\u901a\\\\u8fc7\\\\u63d0\\\\u4f9b\\\\u4e00\\\\u4e2a\\\\u6807\\\\u51c6\\\\u5316\\\\u7684\\\\u63a5\\\\u53e3\\\\uff0c\\\\u6781\\\\u5927\\\\u5730\\\\u589e\\\\u5f3a\\\\u4e86\\\\u5927\\\\u578b\\\\u8bed\\\\u8a00\\\\u6a21\\\\u578b\\\\uff08LLM\\\\uff09\\\\u8bbf\\\\u95ee\\\\u5916\\\\u90e8\\\\u6570\\\\u636e\\\\u548c\\\\u5de5\\\\u5177\\\\u7684\\\\u80fd\\\\u529b\\\\u3002\\\\u5176\\\\u5ba2\\\\u6237\\\\u7aef-\\\\u670d\\\\u52a1\\\\u5668\\\\u67b6\\\\u6784\\\\u3001\\\\u7edf\\\\u4e00\\\\u7684\\\\u901a\\\\u4fe1\\\\u534f\\\\u8bae\\\\u4ee5\\\\u53ca\\\\u5bf9\\\\u5de5\\\\u5177\\\\u3001\\\\u8d44\\\\u6e90\\\\u548c\\\\u63d0\\\\u793a\\\\u7684\\\\u652f\\\\u6301\\\\uff0c\\\\u4f7f\\\\u5f97AI\\\\u7cfb\\\\u7edf\\\\u80fd\\\\u591f\\\\u66f4\\\\u667a\\\\u80fd\\\\u3001\\\\u66f4\\\\u9ad8\\\\u6548\\\\u5730\\\\u8fd0\\\\u884c\\\\u3002MCP\\\\u7684\\\\u51fa\\\\u73b0\\\\u4e0d\\\\u4ec5\\\\u7b80\\\\u5316\\\\u4e86AI\\\\u5e94\\\\u7528\\\\u7684\\\\u5f00\\\\u53d1\\\\u548c\\\\u96c6\\\\u6210\\\\uff0c\\\\u8fd8\\\\u4e3a\\\\u6784\\\\u5efa\\\\u66f4\\\\u5f3a\\\\u5927\\\\u3001\\\\u66f4\\\\u5177\\\\u9002\\\\u5e94\\\\u6027\\\\u7684AI\\\\u4ee3\\\\u7406\\\\u5960\\\\u5b9a\\\\u4e86\\\\u57fa\\\\u7840\\\\u3002\\\\u968f\\\\u7740MCP\\\\u751f\\\\u6001\\\\u7cfb\\\\u7edf\\\\u7684\\\\u4e0d\\\\u65ad\\\\u53d1\\\\u5c55\\\\uff0c\\\\u5b83\\\\u6709\\\\u671b\\\\u5728\\\\u81ea\\\\u52a8\\\\u5316\\\\u3001\\\\u6570\\\\u636e\\\\u5206\\\\u6790\\\\u3001\\\\u8f6f\\\\u4ef6\\\\u5f00\\\\u53d1\\\\u7b49\\\\u591a\\\\u4e2a\\\\u9886\\\\u57df\\\\u53d1\\\\u6325\\\\u8d8a\\\\u6765\\\\u8d8a\\\\u91cd\\\\u8981\\\\u7684\\\\u4f5c\\\\u7528\\\\uff0c\\\\u63a8\\\\u52a8AI\\\\u6280\\\\u672f\\\\u7684\\\\u5e7f\\\\u6cdb\\\\u5e94\\\\u7528\\\\u548c\\\\u521b\\\\u65b0\\\\u3002\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite-preview-06-17', 'safety_ratings': []}, id='run--1ec29599-faab-463c-8f50-28d0770408e5-0', tool_calls=[{'name': 'Conclusion', 'args': {'name': '模型上下文协议（MCP）总结', 'content': '模型上下文协议（MCP）通过提供一个标准化的接口，极大地增强了大型语言模型（LLM）访问外部数据和工具的能力。其客户端-服务器架构、统一的通信协议以及对工具、资源和提示的支持，使得AI系统能够更智能、更高效地运行。MCP的出现不仅简化了AI应用的开发和集成，还为构建更强大、更具适应性的AI代理奠定了基础。随着MCP生态系统的不断发展，它有望在自动化、数据分析、软件开发等多个领域发挥越来越重要的作用，推动AI技术的广泛应用和创新。'}, 'id': 'd3f6dafe-a735-40ec-8af2-f7db4c77a506', 'type': 'tool_call'}], usage_metadata={'input_tokens': 30151, 'output_tokens': 145, 'total_tokens': 30296, 'input_token_details': {'cache_read': 29459}}), ToolMessage(content=\"name='模型上下文协议（MCP）总结' content='模型上下文协议（MCP）通过提供一个标准化的接口，极大地增强了大型语言模型（LLM）访问外部数据和工具的能力。其客户端-服务器架构、统一的通信协议以及对工具、资源和提示的支持，使得AI系统能够更智能、更高效地运行。MCP的出现不仅简化了AI应用的开发和集成，还为构建更强大、更具适应性的AI代理奠定了基础。随着MCP生态系统的不断发展，它有望在自动化、数据分析、软件开发等多个领域发挥越来越重要的作用，推动AI技术的广泛应用和创新。'\", name='Conclusion', id='ad549299-f81d-4915-a930-8203984c1aa9', tool_call_id='d3f6dafe-a735-40ec-8af2-f7db4c77a506'), HumanMessage(content='Report is now complete with introduction, body sections, and conclusion.', additional_kwargs={}, response_metadata={}, id='baf6fe35-19ed-49f4-ac5d-7c9f7b854144'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'FinishReport', 'arguments': '{}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite-preview-06-17', 'safety_ratings': []}, id='run--e1bc34ca-5264-4d0b-a93f-3013353870de-0', tool_calls=[{'name': 'FinishReport', 'args': {}, 'id': '68a61c72-0dbe-4fcd-bae3-ff39a85f382a', 'type': 'tool_call'}], usage_metadata={'input_tokens': 30460, 'output_tokens': 9, 'total_tokens': 30469, 'input_token_details': {'cache_read': 29441}})], 'final_report': '# 模型上下文协议（MCP）概述\\n\\n模型上下文协议（MCP）作为一种开放标准和开源框架，旨在革新大型语言模型（LLM）与外部工具、系统及数据源的交互方式。它通过提供一个统一的接口，解决了AI系统在信息孤岛和碎片化集成方面面临的挑战，使得AI模型能够更有效地访问文件、执行功能并处理上下文信息。本报告将深入探讨MCP的基本概念、主要用途、架构与工作原理、优势与应用场景，并展望其未来发展趋势。\\n\\n## 模型上下文协议（MCP）的基本概念\\n\\n模型上下文协议（MCP）是由Anthropic于2024年11月推出的一种开放标准和开源框架，旨在标准化AI模型（如大型语言模型LLM）与外部工具、系统和数据源之间的数据集成和共享方式。它的核心目标是解决AI系统面临的信息孤岛和碎片化集成问题，提供一个通用的接口来读取文件、执行功能和处理上下文提示。\\n\\n**核心概念：**\\n*   **标准化集成：** MCP提供了一个统一的协议，取代了为每个数据源定制连接器的复杂性，简化了AI应用与数据的连接。\\n*   **客户端-服务器架构：** MCP遵循客户端-服务器模型。MCP服务器暴露数据源和能力（如文件系统、数据库），而MCP客户端（AI应用或代理）连接到这些服务器以获取所需上下文。\\n*   **主要功能：** MCP定义了三种基本功能：\\n    *   **提示（Prompts）：** 允许服务器提供结构化消息和指令，指导语言模型交互。\\n    *   **资源（Resources）：** 使服务器能够共享数据，如文件或数据库模式，为模型提供额外上下文。\\n    *   **工具（Tools）：** 允许服务器暴露可由语言模型调用的函数，使模型能够与外部系统交互。\\n\\nMCP的广泛采用，包括OpenAI和Google DeepMind等主要AI提供商，凸显了其作为AI系统连接和互操作性通用开放标准的潜力。\\n\\n### Sources\\n1.  https://www.anthropic.com/news/model-context-protocol\\n2.  https://modelcontextprotocol.io/introduction\\n3.  https://en.wikipedia.org/wiki/Model_Context_Protocol\\n4.  https://medium.com/@nimritakoul01/the-model-context-protocol-mcp-a-complete-tutorial-a3abe8a7f4ef\\n\\n## MCP的主要用途\\n\\n模型上下文协议 (MCP) 是一个开放标准，旨在彻底改变大型语言模型 (LLM) 与外部世界互动的方式。其主要用途和应用包括：\\n\\n*   **连接LLM与外部数据和工具**：MCP 充当 AI 应用程序的“USB-C 接口”，使 LLM 能够无缝访问各种数据源（如代码库、业务工具）和执行操作（如网络搜索、文件管理）。\\n*   **增强AI助手和Agentic AI应用**：通过提供上下文信息，MCP 帮助 AI 助手和代理执行任务、调用外部 API 并提供更准确和相关的响应，超越其训练数据。这简化了复杂 AI 工作流和多代理系统的构建和管理。\\n*   **标准化集成**：MCP 提供了一种统一的方法来连接 AI 系统与数据源，取代了碎片化的定制集成，从而实现更简单、更可靠的 AI 应用架构。\\n*   **AI辅助编程**：在开发环境中，MCP 助力 AI 代理检索相关信息，理解编码任务，并生成更精确的代码。\\n\\n### Sources\\n1.  https://github.com/liaokongVFX/MCP-Chinese-Getting-Started-Guide\\n2.  https://www.threads.com/@mr.__.l/post/DHIHf5PBtyb/%E4%B8%BB%E8%A6%81%E7%94%A8%E9%80%94%E8%88%87%E6%87%89%E7%94%A8%E5%A0%B4%E6%99%AFmcp-%E7%9A%84%E7%94%A8%E9%80%94%E5%9C%A8%E6%96%BC%E7%82%BA-ai-%E9%96%8B%E7%99%BC%E8%80%85%E8%88%87%E7%B3%BB%E7%B5%B1%E6%95%B4%E5%90%88%E6%8F%90%E4%BE%9B%E4%BE%BF%E5%88%A9%E8%97%89%E7%94%B1-mcp%E9%96%8B%E7%99%BC%E8%80%85%E5%8F%AF%E4%BB%A5%E8%AE%93%E4%BB%BB%E6%84%8F%E7%AC%A6%E5%90%88%E8%A6%8F%E7%AF%84%E7%9A%84-llm-%E7%8D%B7%E5%BE%97%E5%8B%95%E6%89%8B%E8%83%BD%E5%8A%9B%E5%9C%A8%E5%90%84%E7%A8%AE%E7%92%B0%E5%A2%83%E4%B8%AD%E5%9F%B7%E8%A1%8C%E6%93%8D%E4%BD%9C%E6%88%96%E8%AA%BF%E7%94%A8%E8%B3%87%E6%96%99\\n3.  https://aws.amazon.com/cn/blogs/china/practical-guide-to-building-agentic-ai-applications-for-aws-china-region/\\n4.  https://medium.com/@amosgyamfi/the-top-7-mcp-supported-ai-frameworks-a8e5030c87ab\\n5.  https://www.anthropic.com/news/model-context-protocol\\n6.  https://docs.anthropic.com/en/docs/agents-and-tools/mcp\\n\\n## MCP的架构和工作原理\\n\\n模型上下文协议（MCP）是一个开放协议，旨在标准化应用程序如何向大型语言模型（LLM）提供上下文信息。它类似于AI世界的USB-C端口，允许AI模型与各种数据源和工具进行标准化连接。\\n\\n### 架构\\nMCP遵循客户端-服务器架构，主要角色包括：\\n*   **MCP主机：** 希望通过MCP访问数据的应用程序（如AI工具、IDE）。\\n*   **MCP客户端：** 嵌入在LLM中，与MCP服务器保持连接，负责转发请求和返回结果。\\n*   **MCP服务器：** 轻量级程序，通过标准化协议暴露特定功能（工具和数据访问），可访问本地数据源或远程服务。\\n\\n### 工作原理\\n当LLM需要外部信息或执行特定操作时，它会通过MCP客户端调用MCP服务器上定义的“工具”。这些工具是可执行函数，能够从本地文件、数据库或远程API中检索数据或执行操作。服务器处理请求并将结果返回给LLM，LLM再结合这些结果生成响应。\\n\\n## MCP的优势和应用场景\\n\\nMCP（模型上下文协议）旨在作为AI模型的“万能转接头”，使其能够轻松获取数据或调用工具。其主要优势包括：\\n\\n*   **生态与效率：** 提供大量现有插件，避免重复开发，并支持开放标准，利于服务商开发API。\\n*   **精细控制：** 对AI思维过程提供更精细的控制，实现更精准的操作。\\n*   **易用性与安全性：** 提供开箱即用的服务器和可视化配置，无需编码，同时本地服务器进程控制数据访问，保障安全性。\\n*   **扩展性：** 能够无缝接入企业知识库，如设计讨论和内部文档。\\n\\nMCP的应用场景广泛，包括：\\n\\n*   **行程规划助手：** AI助手可无缝检查日历、预订航班、发送确认邮件。\\n*   **高级IDE：** 智能代码编辑器通过MCP连接文件系统、版本控制等，提供更智能的代码建议。\\n*   **复杂数据分析：** AI分析平台可自动发现并与多个数据库和可视化工具交互。\\n*   **AI外呼服务：** 降低语音克隆服务门槛，实现自然语言构建AI外呼。\\n*   **代码开发：** 允许LLM完全访问IDE，实现从需求分析到功能上线的全流程自动化。\\n\\n### Sources\\n1.  https://zhuanlan.zhihu.com/p/29001189476\\n2.  https://www.reddit.com/r/ClaudeAI/comments/1j6zi68/sequential_thinking_mcp_vs_claude_37_extended/?tl=zh-hans\\n3.  https://zhuanlan.zhihu.com/p/27327515233\\n4.  https://x.com/op7418/status/1892056670160949578\\n5.  https://zhuanlan.zhihu.com/p/1890326398637417398\\n6.  https://x.com/aigclink/status/1909514004684587267\\n7.  https://www.reddit.com/r/mcp/comments/1j6y8ct/what_are_some_of_the_best_current_use_cases_for/?tl=zh-hans\\n8.  https://www.cnblogs.com/ytkah/p/18763320/mcp\\n9.  https://github.com/yzfly/Awesome-MCP-ZH\\n\\n## MCP的未来发展\\n\\n模型上下文协议 (MCP) 作为AI代理技术的重要突破，正彻底改变AI与外部世界的互动方式。其未来发展趋势主要体现在以下几个方面：\\n\\n*   **广泛的行业应用：** 将出现针对医疗、金融、教育等特定行业的MCP服务器，实现AI与垂直领域数据的深度融合。\\n*   **服务化与生态扩展：** 预计将涌现MCP即服务（MCP-as-a-Service）的商业模式，并形成多公司联盟共同推动协议的进一步发展。AIbase等平台正在构建MCP资源仓库，促进生态系统建设。\\n*   **云平台深度集成：** 主要AI平台和云计算提供商（如AWS、Cloudflare）将更广泛地集成MCP，支持远程MCP服务器部署，使AI代理能安全、高效地访问和操作外部服务。\\n*   **增强型AI代理能力：** MCP将促使AI从被动响应者转变为主动任务执行者，实现跨工具协作、自主任务执行和弹性模块集成，例如自动化报告、智能家居控制和个人AI秘书等。\\n*   **标准化与安全性：** 作为开放标准，MCP将持续推动AI与外部资源通信的标准化，同时通过内置的访问控制和授权机制，确保数据安全和隐私。\\n\\n### Sources\\n1. https://ikala.ai/zh-tw/blog/ikala-ai-insight/what-is-model-context-protocol-mcp/\\n2. https://www.ibest.com.tw/news-detail/what-is-mcp/\\n3. https://www.cloudflare.com/zh-cn/press-releases/2025/cloudflare-accelerates-ai-agent-development-remote-mcp/\\n4. https://aws.amazon.com/cn/blogs/china/practical-guide-to-building-agentic-ai-applications-for-aws-china-region/\\n5. https://news.qq.com/rain/a/20250523A05RLD00\\n\\n## 模型上下文协议（MCP）总结\\n\\n模型上下文协议（MCP）通过提供一个标准化的接口，极大地增强了大型语言模型（LLM）访问外部数据和工具的能力。其客户端-服务器架构、统一的通信协议以及对工具、资源和提示的支持，使得AI系统能够更智能、更高效地运行。MCP的出现不仅简化了AI应用的开发和集成，还为构建更强大、更具适应性的AI代理奠定了基础。随着MCP生态系统的不断发展，它有望在自动化、数据分析、软件开发等多个领域发挥越来越重要的作用，推动AI技术的广泛应用和创新。', 'source_str': ''}\n"
     ]
    }
   ],
   "source": [
    "from open_deep_research.intelligent_research import ResearchMode\n",
    "\n",
    "\n",
    "# Configure and run the multi-agent system\n",
    "# This sets up the model configuration and executes the research workflow\n",
    "\n",
    "# 🔧 修复后的配置 - 解决 [Errno 11001] getaddrinfo failed 错误\n",
    "# 方案1: 离线模式（推荐）- 避免所有网络请求\n",
    "config_offline = {\n",
    "    \"thread_id\": str(uuid.uuid4()),\n",
    "    \"search_api\": \"none\",  # 禁用搜索工具，专注于基于知识的回答\n",
    "    \"supervisor_model\": \"google_genai:gemini-2.5-flash-lite-preview-06-17\",\n",
    "    \"researcher_model\": \"google_genai:gemini-2.5-flash\",\n",
    "    \"research_mode\": ResearchMode.REFLECTIVE.value, # 🧠 Enable intelligent reflection\n",
    "}\n",
    "\n",
    "# 方案2: 备选配置 - 使用DuckDuckGo搜索（无需API key）\n",
    "config_duckduckgo = {\n",
    "    \"thread_id\": str(uuid.uuid4()),\n",
    "    \"search_api\": \"duckduckgo\",  # 使用DuckDuckGo替代Tavily\n",
    "    \"supervisor_model\": \"google_genai:gemini-2.5-flash-lite-preview-06-17\",\n",
    "    \"researcher_model\": \"google_genai:gemini-2.5-flash\"\n",
    "}\n",
    "\n",
    "# 方案3: 原始配置（如果网络修复后）\n",
    "config_original = {\n",
    "    \"thread_id\": str(uuid.uuid4()),\n",
    "    \"search_api\": \"googlesearch\",\n",
    "    \"supervisor_model\": \"google_genai:gemini-2.5-flash-lite-preview-06-17\",\n",
    "    \"researcher_model\": \"google_genai:gemini-2.5-flash\",\n",
    "    \"research_mode\": ResearchMode.REFLECTIVE.value, # 🧠 Enable intelligent reflection\n",
    "    \"max_research_iterations\": 3\n",
    "}\n",
    "\n",
    "# 默认使用离线模式来避免网络错误\n",
    "config = config_original\n",
    "print(f\"🔧 使用配置: {config['search_api']} 模式\")\n",
    "\n",
    "# Set up thread configuration with the specified parameters\n",
    "thread_config = {\"configurable\": config, \"recursion_limit\": 50}  # 增加递归限制\n",
    "\n",
    "# Define the research topic as a user message\n",
    "# 针对离线模式优化的查询 - 使用更简单直接的查询避免递归问题\n",
    "msg = [{\"role\": \"user\", \"content\": \"请简要介绍模型上下文协议（MCP）的基本概念和主要用途。\"}]\n",
    "# msg = [{\"role\": \"user\", \"content\": \"请详细介绍Anthropic支持的MCP协议：1）MCP的架构设计和开发者指南，2）有趣的MCP服务器实现，3）与Google Agent2Agent协议的对比分析。请直接生成完整报告，无需询问后续问题。\"}]\n",
    "\n",
    "\n",
    "# Run the multi-agent workflow with enhanced error handling\n",
    "import asyncio\n",
    "\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    print(\"✅ 异步环境已配置\")\n",
    "except ImportError:\n",
    "    print(\"⚠️ 如果遇到异步问题，请安装 nest-asyncio: pip install nest-asyncio\")\n",
    "\n",
    "print(f\"🚀 开始运行多智能体工作流（{config['search_api']} 模式）...\")\n",
    "\n",
    "try:\n",
    "    response = await agent.ainvoke({\"messages\": msg}, config=thread_config)\n",
    "    print(\"✅ 工作流执行成功！网络连接问题已修复\")\n",
    "    print(\"response: \",response)\n",
    "    \n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    print(f\"❌ 错误: {error_msg}\")\n",
    "    \n",
    "    import traceback\n",
    "    # 打印完整的traceback\n",
    "    print(\"\\n🔴 完整错误追踪:\")\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # 如果是 KeyError，打印更多调试信息\n",
    "    if isinstance(e, KeyError):\n",
    "        print(f\"\\n🔑 KeyError 详细信息:\")\n",
    "        print(f\"缺少的键: {e.args}\")\n",
    "    \n",
    "    # 网络相关错误处理\n",
    "    if \"[Errno 11001] getaddrinfo failed\" in error_msg or \"DNS\" in error_msg:\n",
    "        print(\"🔍 检测到网络/DNS错误，自动切换到离线模式...\")\n",
    "        config = config_offline\n",
    "        thread_config = {\"configurable\": config, \"recursion_limit\": 50}\n",
    "        \n",
    "        try:\n",
    "            response = await agent.ainvoke({\"messages\": msg}, config=thread_config)\n",
    "            print(\"✅ 离线模式执行成功！\")\n",
    "        except Exception as offline_error:\n",
    "            print(f\"❌ 离线模式也失败: {offline_error}\")\n",
    "            print(\"💡 尝试使用DuckDuckGo搜索...\")\n",
    "            \n",
    "            config = config_duckduckgo\n",
    "            thread_config = {\"configurable\": config, \"recursion_limit\": 50}\n",
    "            try:\n",
    "                response = await agent.ainvoke({\"messages\": msg}, config=thread_config)\n",
    "                print(\"✅ DuckDuckGo搜索模式执行成功！\")\n",
    "            except Exception as final_error:\n",
    "                print(f\"❌ 所有方案都失败: {final_error}\")\n",
    "                raise final_error\n",
    "    \n",
    "    # 递归限制错误处理\n",
    "    elif \"Recursion limit\" in error_msg:\n",
    "        print(\"🔍 检测到递归限制错误，尝试使用更简单的查询...\")\n",
    "        simple_msg = [{\"role\": \"user\", \"content\": \"请用几句话简单介绍MCP协议的概念。\"}]\n",
    "        \n",
    "        # 使用更高的递归限制和更简单的查询\n",
    "        config = config_offline  \n",
    "        thread_config = {\"configurable\": config, \"recursion_limit\": 100}\n",
    "        \n",
    "        try:\n",
    "            response = await agent.ainvoke({\"messages\": simple_msg}, config=thread_config)\n",
    "            print(\"✅ 简化查询执行成功！\")\n",
    "        except Exception as simple_error:\n",
    "            print(f\"❌ 简化查询也失败: {simple_error}\")\n",
    "            print(\"💡 建议: 多智能体系统可能过于复杂，考虑使用单模型方法\")\n",
    "            raise simple_error\n",
    "    \n",
    "    # Gemini模型特定错误处理\n",
    "    elif \"parallel_tool_calls\" in error_msg:\n",
    "        print(\"💡 parallel_tool_calls问题应该已修复\")\n",
    "        \n",
    "    elif \"function call turn\" in error_msg:\n",
    "        print(\"💡 消息序列问题应该已修复\")\n",
    "        print(\"🔍 如果仍然出现此错误，请联系开发者\")\n",
    "        \n",
    "    else:\n",
    "        print(\"💡 遇到其他问题...\")\n",
    "        print(\"🔄 建议检查:\")\n",
    "        print(\"  1. GOOGLE_API_KEY 环境变量是否正确设置\")\n",
    "        print(\"  2. 网络连接是否正常\")\n",
    "        print(\"  3. 模型配置是否正确\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  FinishReport (68a61c72-0dbe-4fcd-bae3-ff39a85f382a)\n",
      " Call ID: 68a61c72-0dbe-4fcd-bae3-ff39a85f382a\n",
      "  Args:\n"
     ]
    }
   ],
   "source": [
    "messages = agent.get_state(thread_config).values['messages']\n",
    "messages[-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 开始生成详细报告...\n",
      "[Supervisor] 消息序列修复: 合并了 3 个连续的 tool 消息, 合并了 2 个连续的 tool 消息\n",
      "❌ 错误: Invalid argument provided to Gemini: 400 * GenerateContentRequest.contents[8].parts: contents.parts must not be empty.\n",
      "\n"
     ]
    },
    {
     "ename": "ChatGoogleGenerativeAIError",
     "evalue": "Invalid argument provided to Gemini: 400 * GenerateContentRequest.contents[8].parts: contents.parts must not be empty.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgument\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:234\u001b[39m, in \u001b[36m_achat_with_retry.<locals>._achat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m generation_method(**kwargs)\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    236\u001b[39m     \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\async_client.py:444\u001b[39m, in \u001b[36mGenerativeServiceAsyncClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    443\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m rpc(\n\u001b[32m    445\u001b[39m     request,\n\u001b[32m    446\u001b[39m     retry=retry,\n\u001b[32m    447\u001b[39m     timeout=timeout,\n\u001b[32m    448\u001b[39m     metadata=metadata,\n\u001b[32m    449\u001b[39m )\n\u001b[32m    451\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary_async.py:231\u001b[39m, in \u001b[36mAsyncRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    228\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    229\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    230\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m retry_target(\n\u001b[32m    232\u001b[39m     functools.partial(func, *args, **kwargs),\n\u001b[32m    233\u001b[39m     predicate=\u001b[38;5;28mself\u001b[39m._predicate,\n\u001b[32m    234\u001b[39m     sleep_generator=sleep_generator,\n\u001b[32m    235\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m._timeout,\n\u001b[32m    236\u001b[39m     on_error=on_error,\n\u001b[32m    237\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary_async.py:163\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary_async.py:158\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m target()\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers_async.py:88\u001b[39m, in \u001b[36m_WrappedUnaryResponseMixin.__await__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(rpc_error) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrpc_error\u001b[39;00m\n",
      "\u001b[31mInvalidArgument\u001b[39m: 400 * GenerateContentRequest.contents[8].parts: contents.parts must not be empty.\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m         m.pretty_print()\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🚀 开始生成详细报告...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m agent.ainvoke({\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: msg}, config=thread_config)\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ 报告生成成功\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# 显示所有消息\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2788\u001b[39m, in \u001b[36mPregel.ainvoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2785\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]] = []\n\u001b[32m   2786\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2788\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(\n\u001b[32m   2789\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2790\u001b[39m     config,\n\u001b[32m   2791\u001b[39m     stream_mode=stream_mode,\n\u001b[32m   2792\u001b[39m     output_keys=output_keys,\n\u001b[32m   2793\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   2794\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   2795\u001b[39m     checkpoint_during=checkpoint_during,\n\u001b[32m   2796\u001b[39m     debug=debug,\n\u001b[32m   2797\u001b[39m     **kwargs,\n\u001b[32m   2798\u001b[39m ):\n\u001b[32m   2799\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2800\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2801\u001b[39m             \u001b[38;5;28misinstance\u001b[39m(chunk, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m   2802\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m (ints := chunk.get(INTERRUPT)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2803\u001b[39m         ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2655\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2653\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2654\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2655\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2656\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2657\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2658\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2659\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2660\u001b[39m ):\n\u001b[32m   2661\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2662\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[32m   2663\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langgraph\\pregel\\runner.py:294\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    292\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    295\u001b[39m         t,\n\u001b[32m    296\u001b[39m         retry_policy,\n\u001b[32m    297\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    298\u001b[39m         configurable={\n\u001b[32m    299\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    300\u001b[39m                 _acall,\n\u001b[32m    301\u001b[39m                 weakref.ref(t),\n\u001b[32m    302\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    303\u001b[39m                 retry=retry_policy,\n\u001b[32m    304\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    305\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    306\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    307\u001b[39m                 loop=loop,\n\u001b[32m    308\u001b[39m             ),\n\u001b[32m    309\u001b[39m         },\n\u001b[32m    310\u001b[39m     )\n\u001b[32m    311\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langgraph\\pregel\\retry.py:136\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policies, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    134\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    138\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py:672\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    670\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    671\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m672\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    673\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    674\u001b[39m         )\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    676\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\python312\\Lib\\asyncio\\futures.py:287\u001b[39m, in \u001b[36mFuture.__await__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    286\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncio_future_blocking = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mawait wasn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt used with future\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\python312\\Lib\\asyncio\\tasks.py:385\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    388\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\python312\\Lib\\asyncio\\futures.py:203\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\python312\\Lib\\asyncio\\tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py:440\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    438\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    442\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\src\\open_deep_research\\multi_agent.py:358\u001b[39m, in \u001b[36msupervisor\u001b[39m\u001b[34m(state, config)\u001b[39m\n\u001b[32m    348\u001b[39m llm_messages = [\n\u001b[32m    349\u001b[39m     {\n\u001b[32m    350\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    351\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: system_prompt\n\u001b[32m    352\u001b[39m     }\n\u001b[32m    353\u001b[39m ] + messages\n\u001b[32m    355\u001b[39m \u001b[38;5;66;03m# Invoke\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    357\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m--> \u001b[39m\u001b[32m358\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m llm_with_tools.ainvoke(llm_messages)\n\u001b[32m    359\u001b[39m     ]\n\u001b[32m    360\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5444\u001b[39m, in \u001b[36mRunnableBindingBase.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5437\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5438\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m   5439\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5442\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5443\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5444\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.ainvoke(\n\u001b[32m   5445\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5446\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5447\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5448\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:394\u001b[39m, in \u001b[36mBaseChatModel.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> BaseMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m    395\u001b[39m         [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    396\u001b[39m         stop=stop,\n\u001b[32m    397\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    398\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    399\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    400\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    401\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    402\u001b[39m         **kwargs,\n\u001b[32m    403\u001b[39m     )\n\u001b[32m    404\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m, llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:968\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    959\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m    961\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    965\u001b[39m     **kwargs: Any,\n\u001b[32m    966\u001b[39m ) -> LLMResult:\n\u001b[32m    967\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m968\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m    969\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m    970\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:926\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    913\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[32m    914\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    915\u001b[39m             *[\n\u001b[32m    916\u001b[39m                 run_manager.on_llm_end(\n\u001b[32m   (...)\u001b[39m\u001b[32m    924\u001b[39m             ]\n\u001b[32m    925\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[32m0\u001b[39m]\n\u001b[32m    927\u001b[39m flattened_outputs = [\n\u001b[32m    928\u001b[39m     LLMResult(generations=[res.generations], llm_output=res.llm_output)  \u001b[38;5;66;03m# type: ignore[list-item, union-attr]\u001b[39;00m\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[32m    930\u001b[39m ]\n\u001b[32m    931\u001b[39m llm_output = \u001b[38;5;28mself\u001b[39m._combine_llm_outputs([res.llm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\python312\\Lib\\asyncio\\tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1094\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1092\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._agenerate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1094\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m   1095\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1096\u001b[39m     )\n\u001b[32m   1097\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1098\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:1391\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   1376\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()._agenerate(\n\u001b[32m   1377\u001b[39m         messages, stop, run_manager, **updated_kwargs\n\u001b[32m   1378\u001b[39m     )\n\u001b[32m   1380\u001b[39m request = \u001b[38;5;28mself\u001b[39m._prepare_request(\n\u001b[32m   1381\u001b[39m     messages,\n\u001b[32m   1382\u001b[39m     stop=stop,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1389\u001b[39m     tool_choice=tool_choice,\n\u001b[32m   1390\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1391\u001b[39m response: GenerateContentResponse = \u001b[38;5;28;01mawait\u001b[39;00m _achat_with_retry(\n\u001b[32m   1392\u001b[39m     request=request,\n\u001b[32m   1393\u001b[39m     **kwargs,\n\u001b[32m   1394\u001b[39m     generation_method=\u001b[38;5;28mself\u001b[39m.async_client.generate_content,\n\u001b[32m   1395\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m.default_metadata,\n\u001b[32m   1396\u001b[39m )\n\u001b[32m   1397\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:243\u001b[39m, in \u001b[36m_achat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _achat_with_retry(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189\u001b[39m, in \u001b[36mAsyncRetrying.wraps.<locals>.async_wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    188\u001b[39m async_wrapped.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m copy(fn, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    109\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     do = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter(retry_state=retry_state)\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    113\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153\u001b[39m, in \u001b[36mAsyncRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    151\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m action(retry_state)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\tenacity\\_utils.py:99\u001b[39m, in \u001b[36mwrap_to_async_func.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args: typing.Any, **kwargs: typing.Any) -> typing.Any:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\python312\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\python312\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m fn(*args, **kwargs)\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    116\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:237\u001b[39m, in \u001b[36m_achat_with_retry.<locals>._achat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m generation_method(**kwargs)\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    236\u001b[39m     \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[32m    238\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    239\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m: Invalid argument provided to Gemini: 400 * GenerateContentRequest.contents[8].parts: contents.parts must not be empty.\n",
      "During task with name 'supervisor' and id '6a266765-08a6-43eb-dcf1-ad9d621b464f'"
     ]
    }
   ],
   "source": [
    "msg = [{\"role\": \"user\", \"content\": \"请详细介绍Anthropic支持的MCP协议：1）MCP的架构设计和开发者指南，2）有趣的MCP服务器实现，3）与Google Agent2Agent协议的对比分析。请直接生成完整报告，无需询问后续问题。\"}]\n",
    "\n",
    "# 运行改进的查询\n",
    "try:\n",
    "    print(\"🚀 开始生成详细报告...\")\n",
    "    response = await agent.ainvoke({\"messages\": msg}, config=thread_config)\n",
    "    print(\"✅ 报告生成成功\")\n",
    "    \n",
    "    # 显示所有消息\n",
    "    for m in agent.get_state(thread_config).values['messages']:\n",
    "        m.pretty_print()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ 错误: {e}\")\n",
    "    if \"parallel_tool_calls\" in str(e):\n",
    "        print(\"💡 检测到模型兼容性问题，代码已自动修复\")\n",
    "        response = await agent.ainvoke({\"messages\": msg}, config=thread_config)\n",
    "        for m in agent.get_state(thread_config).values['messages']:\n",
    "            m.pretty_print()\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# 模型上下文协议（MCP）概述\n",
       "\n",
       "模型上下文协议（MCP）作为一种开放标准和开源框架，旨在革新大型语言模型（LLM）与外部工具、系统及数据源的交互方式。它通过提供一个统一的接口，解决了AI系统在信息孤岛和碎片化集成方面面临的挑战，使得AI模型能够更有效地访问文件、执行功能并处理上下文信息。本报告将深入探讨MCP的基本概念、主要用途、架构与工作原理、优势与应用场景，并展望其未来发展趋势。\n",
       "\n",
       "## 模型上下文协议（MCP）的基本概念\n",
       "\n",
       "模型上下文协议（MCP）是由Anthropic于2024年11月推出的一种开放标准和开源框架，旨在标准化AI模型（如大型语言模型LLM）与外部工具、系统和数据源之间的数据集成和共享方式。它的核心目标是解决AI系统面临的信息孤岛和碎片化集成问题，提供一个通用的接口来读取文件、执行功能和处理上下文提示。\n",
       "\n",
       "**核心概念：**\n",
       "*   **标准化集成：** MCP提供了一个统一的协议，取代了为每个数据源定制连接器的复杂性，简化了AI应用与数据的连接。\n",
       "*   **客户端-服务器架构：** MCP遵循客户端-服务器模型。MCP服务器暴露数据源和能力（如文件系统、数据库），而MCP客户端（AI应用或代理）连接到这些服务器以获取所需上下文。\n",
       "*   **主要功能：** MCP定义了三种基本功能：\n",
       "    *   **提示（Prompts）：** 允许服务器提供结构化消息和指令，指导语言模型交互。\n",
       "    *   **资源（Resources）：** 使服务器能够共享数据，如文件或数据库模式，为模型提供额外上下文。\n",
       "    *   **工具（Tools）：** 允许服务器暴露可由语言模型调用的函数，使模型能够与外部系统交互。\n",
       "\n",
       "MCP的广泛采用，包括OpenAI和Google DeepMind等主要AI提供商，凸显了其作为AI系统连接和互操作性通用开放标准的潜力。\n",
       "\n",
       "### Sources\n",
       "1.  https://www.anthropic.com/news/model-context-protocol\n",
       "2.  https://modelcontextprotocol.io/introduction\n",
       "3.  https://en.wikipedia.org/wiki/Model_Context_Protocol\n",
       "4.  https://medium.com/@nimritakoul01/the-model-context-protocol-mcp-a-complete-tutorial-a3abe8a7f4ef\n",
       "\n",
       "## MCP的主要用途\n",
       "\n",
       "模型上下文协议 (MCP) 是一个开放标准，旨在彻底改变大型语言模型 (LLM) 与外部世界互动的方式。其主要用途和应用包括：\n",
       "\n",
       "*   **连接LLM与外部数据和工具**：MCP 充当 AI 应用程序的“USB-C 接口”，使 LLM 能够无缝访问各种数据源（如代码库、业务工具）和执行操作（如网络搜索、文件管理）。\n",
       "*   **增强AI助手和Agentic AI应用**：通过提供上下文信息，MCP 帮助 AI 助手和代理执行任务、调用外部 API 并提供更准确和相关的响应，超越其训练数据。这简化了复杂 AI 工作流和多代理系统的构建和管理。\n",
       "*   **标准化集成**：MCP 提供了一种统一的方法来连接 AI 系统与数据源，取代了碎片化的定制集成，从而实现更简单、更可靠的 AI 应用架构。\n",
       "*   **AI辅助编程**：在开发环境中，MCP 助力 AI 代理检索相关信息，理解编码任务，并生成更精确的代码。\n",
       "\n",
       "### Sources\n",
       "1.  https://github.com/liaokongVFX/MCP-Chinese-Getting-Started-Guide\n",
       "2.  https://www.threads.com/@mr.__.l/post/DHIHf5PBtyb/%E4%B8%BB%E8%A6%81%E7%94%A8%E9%80%94%E8%88%87%E6%87%89%E7%94%A8%E5%A0%B4%E6%99%AFmcp-%E7%9A%84%E7%94%A8%E9%80%94%E5%9C%A8%E6%96%BC%E7%82%BA-ai-%E9%96%8B%E7%99%BC%E8%80%85%E8%88%87%E7%B3%BB%E7%B5%B1%E6%95%B4%E5%90%88%E6%8F%90%E4%BE%9B%E4%BE%BF%E5%88%A9%E8%97%89%E7%94%B1-mcp%E9%96%8B%E7%99%BC%E8%80%85%E5%8F%AF%E4%BB%A5%E8%AE%93%E4%BB%BB%E6%84%8F%E7%AC%A6%E5%90%88%E8%A6%8F%E7%AF%84%E7%9A%84-llm-%E7%8D%B7%E5%BE%97%E5%8B%95%E6%89%8B%E8%83%BD%E5%8A%9B%E5%9C%A8%E5%90%84%E7%A8%AE%E7%92%B0%E5%A2%83%E4%B8%AD%E5%9F%B7%E8%A1%8C%E6%93%8D%E4%BD%9C%E6%88%96%E8%AA%BF%E7%94%A8%E8%B3%87%E6%96%99\n",
       "3.  https://aws.amazon.com/cn/blogs/china/practical-guide-to-building-agentic-ai-applications-for-aws-china-region/\n",
       "4.  https://medium.com/@amosgyamfi/the-top-7-mcp-supported-ai-frameworks-a8e5030c87ab\n",
       "5.  https://www.anthropic.com/news/model-context-protocol\n",
       "6.  https://docs.anthropic.com/en/docs/agents-and-tools/mcp\n",
       "\n",
       "## MCP的架构和工作原理\n",
       "\n",
       "模型上下文协议（MCP）是一个开放协议，旨在标准化应用程序如何向大型语言模型（LLM）提供上下文信息。它类似于AI世界的USB-C端口，允许AI模型与各种数据源和工具进行标准化连接。\n",
       "\n",
       "### 架构\n",
       "MCP遵循客户端-服务器架构，主要角色包括：\n",
       "*   **MCP主机：** 希望通过MCP访问数据的应用程序（如AI工具、IDE）。\n",
       "*   **MCP客户端：** 嵌入在LLM中，与MCP服务器保持连接，负责转发请求和返回结果。\n",
       "*   **MCP服务器：** 轻量级程序，通过标准化协议暴露特定功能（工具和数据访问），可访问本地数据源或远程服务。\n",
       "\n",
       "### 工作原理\n",
       "当LLM需要外部信息或执行特定操作时，它会通过MCP客户端调用MCP服务器上定义的“工具”。这些工具是可执行函数，能够从本地文件、数据库或远程API中检索数据或执行操作。服务器处理请求并将结果返回给LLM，LLM再结合这些结果生成响应。\n",
       "\n",
       "## MCP的优势和应用场景\n",
       "\n",
       "MCP（模型上下文协议）旨在作为AI模型的“万能转接头”，使其能够轻松获取数据或调用工具。其主要优势包括：\n",
       "\n",
       "*   **生态与效率：** 提供大量现有插件，避免重复开发，并支持开放标准，利于服务商开发API。\n",
       "*   **精细控制：** 对AI思维过程提供更精细的控制，实现更精准的操作。\n",
       "*   **易用性与安全性：** 提供开箱即用的服务器和可视化配置，无需编码，同时本地服务器进程控制数据访问，保障安全性。\n",
       "*   **扩展性：** 能够无缝接入企业知识库，如设计讨论和内部文档。\n",
       "\n",
       "MCP的应用场景广泛，包括：\n",
       "\n",
       "*   **行程规划助手：** AI助手可无缝检查日历、预订航班、发送确认邮件。\n",
       "*   **高级IDE：** 智能代码编辑器通过MCP连接文件系统、版本控制等，提供更智能的代码建议。\n",
       "*   **复杂数据分析：** AI分析平台可自动发现并与多个数据库和可视化工具交互。\n",
       "*   **AI外呼服务：** 降低语音克隆服务门槛，实现自然语言构建AI外呼。\n",
       "*   **代码开发：** 允许LLM完全访问IDE，实现从需求分析到功能上线的全流程自动化。\n",
       "\n",
       "### Sources\n",
       "1.  https://zhuanlan.zhihu.com/p/29001189476\n",
       "2.  https://www.reddit.com/r/ClaudeAI/comments/1j6zi68/sequential_thinking_mcp_vs_claude_37_extended/?tl=zh-hans\n",
       "3.  https://zhuanlan.zhihu.com/p/27327515233\n",
       "4.  https://x.com/op7418/status/1892056670160949578\n",
       "5.  https://zhuanlan.zhihu.com/p/1890326398637417398\n",
       "6.  https://x.com/aigclink/status/1909514004684587267\n",
       "7.  https://www.reddit.com/r/mcp/comments/1j6y8ct/what_are_some_of_the_best_current_use_cases_for/?tl=zh-hans\n",
       "8.  https://www.cnblogs.com/ytkah/p/18763320/mcp\n",
       "9.  https://github.com/yzfly/Awesome-MCP-ZH\n",
       "\n",
       "## MCP的未来发展\n",
       "\n",
       "模型上下文协议 (MCP) 作为AI代理技术的重要突破，正彻底改变AI与外部世界的互动方式。其未来发展趋势主要体现在以下几个方面：\n",
       "\n",
       "*   **广泛的行业应用：** 将出现针对医疗、金融、教育等特定行业的MCP服务器，实现AI与垂直领域数据的深度融合。\n",
       "*   **服务化与生态扩展：** 预计将涌现MCP即服务（MCP-as-a-Service）的商业模式，并形成多公司联盟共同推动协议的进一步发展。AIbase等平台正在构建MCP资源仓库，促进生态系统建设。\n",
       "*   **云平台深度集成：** 主要AI平台和云计算提供商（如AWS、Cloudflare）将更广泛地集成MCP，支持远程MCP服务器部署，使AI代理能安全、高效地访问和操作外部服务。\n",
       "*   **增强型AI代理能力：** MCP将促使AI从被动响应者转变为主动任务执行者，实现跨工具协作、自主任务执行和弹性模块集成，例如自动化报告、智能家居控制和个人AI秘书等。\n",
       "*   **标准化与安全性：** 作为开放标准，MCP将持续推动AI与外部资源通信的标准化，同时通过内置的访问控制和授权机制，确保数据安全和隐私。\n",
       "\n",
       "### Sources\n",
       "1. https://ikala.ai/zh-tw/blog/ikala-ai-insight/what-is-model-context-protocol-mcp/\n",
       "2. https://www.ibest.com.tw/news-detail/what-is-mcp/\n",
       "3. https://www.cloudflare.com/zh-cn/press-releases/2025/cloudflare-accelerates-ai-agent-development-remote-mcp/\n",
       "4. https://aws.amazon.com/cn/blogs/china/practical-guide-to-building-agentic-ai-applications-for-aws-china-region/\n",
       "5. https://news.qq.com/rain/a/20250523A05RLD00\n",
       "\n",
       "## 模型上下文协议（MCP）总结\n",
       "\n",
       "模型上下文协议（MCP）通过提供一个标准化的接口，极大地增强了大型语言模型（LLM）访问外部数据和工具的能力。其客户端-服务器架构、统一的通信协议以及对工具、资源和提示的支持，使得AI系统能够更智能、更高效地运行。MCP的出现不仅简化了AI应用的开发和集成，还为构建更强大、更具适应性的AI代理奠定了基础。随着MCP生态系统的不断发展，它有望在自动化、数据分析、软件开发等多个领域发挥越来越重要的作用，推动AI技术的广泛应用和创新。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(agent.get_state(thread_config).values['final_report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trace: \n",
    "\n",
    "> Note: uses 456k tokens \n",
    "\n",
    "https://smith.langchain.com/public/f1581fa5-dfc9-445c-a8f4-3518a05cd139/r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
