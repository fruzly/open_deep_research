{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Researcher\n",
    "\n",
    "This notebook demonstrates the multi-agent research approach, which uses a supervisor-researcher collaborative pattern to create comprehensive reports. The system consists of:\n",
    "\n",
    "1. A **Supervisor Agent** that plans the overall report structure and coordinates work\n",
    "2. Multiple **Research Agents** that investigate specific topics in parallel\n",
    "3. A workflow that produces a structured report with introduction, body sections, and conclusion\n",
    "\n",
    "## From repo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g:\\MyProjects\\open_deep_research\\src\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! uv pip install -U -q nest-asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the multi-agent graph\n",
    "\n",
    "Next, we'll compile the LangGraph workflow for the multi-agent research approach. This step creates the orchestration layer that manages communication between the supervisor and research agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.15\n",
      "2025-06-29 12:26:35 [info     ] 🔧 [ResearchBuilder] Starting to build multi-agent workflow\n",
      "2025-06-29 12:26:35 [info     ] 📊 [ResearchBuilder] Building research agent workflow # Start\n",
      "2025-06-29 12:26:35 [info     ] 📊 [ResearchBuilder] Building research agent workflow # Completed\n",
      "2025-06-29 12:26:35 [info     ] 👑 [SupervisorBuilder] Building supervisor workflow # Start\n",
      "2025-06-29 12:26:35 [info     ] 👑 [SupervisorBuilder] Building supervisor workflow # Completed\n",
      "2025-06-29 12:26:35 [info     ] ✅ [MultiAgentBuilder] Multi-agent workflow construction completed\n"
     ]
    }
   ],
   "source": [
    "import uuid \n",
    "import os, getpass\n",
    "import open_deep_research   \n",
    "print(open_deep_research.__version__) \n",
    "from IPython.display import Image, display, Markdown\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from open_deep_research.multi_agent import supervisor_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MemorySaver for checkpointing the agent's state\n",
    "# This enables tracking and debugging of the multi-agent interaction\n",
    "checkpointer = MemorySaver()\n",
    "agent = supervisor_builder.compile(name=\"research_team\", checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the graph structure\n",
    "# This shows how supervisor and research agents are connected in the workflow\n",
    "\n",
    "#display(Image(agent.get_graph(xray=1).draw_mermaid_png(max_retries=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 使用配置: googlesearch 模式，问题询问: True\n",
      "📋 状态传递机制已修复: 通过消息内容而非共享状态字段传递 section 信息\n",
      "🔧 工具配置修复: 启用搜索工具和Question工具，确保完整的工作流程序列\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"g:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 234, in _achat_with_retry\n",
      "    return await generation_method(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\async_client.py\", line 444, in generate_content\n",
      "    response = await rpc(\n",
      "               ^^^^^^^^^^\n",
      "  File \"g:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary_async.py\", line 231, in retry_wrapped_func\n",
      "    return await retry_target(\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary_async.py\", line 163, in retry_target\n",
      "    next_sleep = _retry_error_helper(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py\", line 214, in _retry_error_helper\n",
      "    raise final_exc from source_exc\n",
      "  File \"g:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary_async.py\", line 158, in retry_target\n",
      "    return await target()\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"g:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers_async.py\", line 88, in __await__\n",
      "    raise exceptions.from_grpc_error(rpc_error) from rpc_error\n",
      "google.api_core.exceptions.InvalidArgument: 400 * GenerateContentRequest.contents[14].parts: contents.parts must not be empty.\n",
      "* GenerateContentRequest.contents[23].parts: contents.parts must not be empty.\n",
      "\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11552\\115106755.py\", line 73, in <module>\n",
      "    response = await agent.ainvoke({\"messages\": msg}, config=thread_config)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 2788, in ainvoke\n",
      "    async for chunk in self.astream(\n",
      "  File \"g:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 2655, in astream\n",
      "    async for _ in runner.atick(\n",
      "  File \"g:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langgraph\\pregel\\runner.py\", line 294, in atick\n",
      "    await arun_with_retry(\n",
      "  File \"g:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langgraph\\pregel\\retry.py\", line 136, in arun_with_retry\n",
      "    return await task.proc.ainvoke(task.input, config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py\", line 672, in ainvoke\n",
      "    input = await asyncio.create_task(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py\", line 440, in ainvoke\n",
      "    ret = await self.afunc(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\MyProjects\\open_deep_research\\src\\open_deep_research\\multi_agent.py\", line 493, in supervisor\n",
      "    response = await llm_with_tools.ainvoke(safe_messages)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5444, in ainvoke\n",
      "    return await self.bound.ainvoke(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 394, in ainvoke\n",
      "    llm_result = await self.agenerate_prompt(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 968, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 926, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"g:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1094, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 1391, in _agenerate\n",
      "    response: GenerateContentResponse = await _achat_with_retry(\n",
      "                                        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 243, in _achat_with_retry\n",
      "    return await _achat_with_retry(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 400, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\python312\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\python312\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"g:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 237, in _achat_with_retry\n",
      "    raise ChatGoogleGenerativeAIError(\n",
      "langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Invalid argument provided to Gemini: 400 * GenerateContentRequest.contents[14].parts: contents.parts must not be empty.\n",
      "* GenerateContentRequest.contents[23].parts: contents.parts must not be empty.\n",
      "\n",
      "During task with name 'supervisor' and id '4c5c307f-cde5-cb03-10d0-08ac230ed3da'\n"
     ]
    },
    {
     "ename": "ChatGoogleGenerativeAIError",
     "evalue": "Invalid argument provided to Gemini: 400 * GenerateContentRequest.contents[14].parts: contents.parts must not be empty.\n* GenerateContentRequest.contents[23].parts: contents.parts must not be empty.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgument\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:234\u001b[39m, in \u001b[36m_achat_with_retry.<locals>._achat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m generation_method(**kwargs)\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    236\u001b[39m     \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\async_client.py:444\u001b[39m, in \u001b[36mGenerativeServiceAsyncClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    443\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m rpc(\n\u001b[32m    445\u001b[39m     request,\n\u001b[32m    446\u001b[39m     retry=retry,\n\u001b[32m    447\u001b[39m     timeout=timeout,\n\u001b[32m    448\u001b[39m     metadata=metadata,\n\u001b[32m    449\u001b[39m )\n\u001b[32m    451\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary_async.py:231\u001b[39m, in \u001b[36mAsyncRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    228\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    229\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    230\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m retry_target(\n\u001b[32m    232\u001b[39m     functools.partial(func, *args, **kwargs),\n\u001b[32m    233\u001b[39m     predicate=\u001b[38;5;28mself\u001b[39m._predicate,\n\u001b[32m    234\u001b[39m     sleep_generator=sleep_generator,\n\u001b[32m    235\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m._timeout,\n\u001b[32m    236\u001b[39m     on_error=on_error,\n\u001b[32m    237\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary_async.py:163\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary_async.py:158\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m target()\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers_async.py:88\u001b[39m, in \u001b[36m_WrappedUnaryResponseMixin.__await__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(rpc_error) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrpc_error\u001b[39;00m\n",
      "\u001b[31mInvalidArgument\u001b[39m: 400 * GenerateContentRequest.contents[14].parts: contents.parts must not be empty.\n* GenerateContentRequest.contents[23].parts: contents.parts must not be empty.\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 144\u001b[39m\n\u001b[32m    142\u001b[39m logger.error(\u001b[33m\"\u001b[39m\u001b[33m  2. 网络连接是否正常\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    143\u001b[39m logger.error(\u001b[33m\"\u001b[39m\u001b[33m  3. 模型配置是否正确\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 73\u001b[39m\n\u001b[32m     70\u001b[39m configure_logging(force_file_logging=\u001b[38;5;28;01mTrue\u001b[39;00m, log_filename=log_filename)\n\u001b[32m     71\u001b[39m logger = get_logger(\u001b[33m\"\u001b[39m\u001b[33mmulti_agent_logging\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m agent.ainvoke({\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: msg}, config=thread_config)\n\u001b[32m     74\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33m✅ 工作流执行成功！网络连接问题已修复\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     75\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mresponse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2788\u001b[39m, in \u001b[36mPregel.ainvoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2785\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]] = []\n\u001b[32m   2786\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2788\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(\n\u001b[32m   2789\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2790\u001b[39m     config,\n\u001b[32m   2791\u001b[39m     stream_mode=stream_mode,\n\u001b[32m   2792\u001b[39m     output_keys=output_keys,\n\u001b[32m   2793\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   2794\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   2795\u001b[39m     checkpoint_during=checkpoint_during,\n\u001b[32m   2796\u001b[39m     debug=debug,\n\u001b[32m   2797\u001b[39m     **kwargs,\n\u001b[32m   2798\u001b[39m ):\n\u001b[32m   2799\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2800\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2801\u001b[39m             \u001b[38;5;28misinstance\u001b[39m(chunk, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m   2802\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m (ints := chunk.get(INTERRUPT)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2803\u001b[39m         ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2655\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2653\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2654\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2655\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2656\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2657\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2658\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2659\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2660\u001b[39m ):\n\u001b[32m   2661\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2662\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[32m   2663\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langgraph\\pregel\\runner.py:294\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    292\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    295\u001b[39m         t,\n\u001b[32m    296\u001b[39m         retry_policy,\n\u001b[32m    297\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    298\u001b[39m         configurable={\n\u001b[32m    299\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    300\u001b[39m                 _acall,\n\u001b[32m    301\u001b[39m                 weakref.ref(t),\n\u001b[32m    302\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    303\u001b[39m                 retry=retry_policy,\n\u001b[32m    304\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    305\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    306\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    307\u001b[39m                 loop=loop,\n\u001b[32m    308\u001b[39m             ),\n\u001b[32m    309\u001b[39m         },\n\u001b[32m    310\u001b[39m     )\n\u001b[32m    311\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langgraph\\pregel\\retry.py:136\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policies, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    134\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    138\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py:672\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    670\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    671\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m672\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    673\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    674\u001b[39m         )\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    676\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py:440\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    438\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    442\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\src\\open_deep_research\\multi_agent.py:493\u001b[39m, in \u001b[36msupervisor\u001b[39m\u001b[34m(state, config)\u001b[39m\n\u001b[32m    490\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m🚀 [Supervisor] Invoking LLM with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(safe_messages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m messages.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    491\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m📜 [Supervisor] LLM Messages: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msafe_messages\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m llm_with_tools.ainvoke(safe_messages)\n\u001b[32m    494\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m🔧 [Supervisor] LLM response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    495\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ [Supervisor] LLM response completed with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(response.tool_calls)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mhasattr\u001b[39m(response,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtool_calls\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mand\u001b[39;00m\u001b[38;5;250m \u001b[39mresponse.tool_calls\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[32m0\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tool calls\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5444\u001b[39m, in \u001b[36mRunnableBindingBase.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5437\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5438\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m   5439\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5442\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5443\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5444\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.ainvoke(\n\u001b[32m   5445\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5446\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5447\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5448\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:394\u001b[39m, in \u001b[36mBaseChatModel.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> BaseMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m    395\u001b[39m         [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    396\u001b[39m         stop=stop,\n\u001b[32m    397\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    398\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    399\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    400\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    401\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    402\u001b[39m         **kwargs,\n\u001b[32m    403\u001b[39m     )\n\u001b[32m    404\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m, llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:968\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    959\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m    961\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    965\u001b[39m     **kwargs: Any,\n\u001b[32m    966\u001b[39m ) -> LLMResult:\n\u001b[32m    967\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m968\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m    969\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m    970\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:926\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    913\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[32m    914\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    915\u001b[39m             *[\n\u001b[32m    916\u001b[39m                 run_manager.on_llm_end(\n\u001b[32m   (...)\u001b[39m\u001b[32m    924\u001b[39m             ]\n\u001b[32m    925\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[32m0\u001b[39m]\n\u001b[32m    927\u001b[39m flattened_outputs = [\n\u001b[32m    928\u001b[39m     LLMResult(generations=[res.generations], llm_output=res.llm_output)  \u001b[38;5;66;03m# type: ignore[list-item, union-attr]\u001b[39;00m\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[32m    930\u001b[39m ]\n\u001b[32m    931\u001b[39m llm_output = \u001b[38;5;28mself\u001b[39m._combine_llm_outputs([res.llm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1094\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1092\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._agenerate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1094\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m   1095\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1096\u001b[39m     )\n\u001b[32m   1097\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1098\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:1391\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   1376\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()._agenerate(\n\u001b[32m   1377\u001b[39m         messages, stop, run_manager, **updated_kwargs\n\u001b[32m   1378\u001b[39m     )\n\u001b[32m   1380\u001b[39m request = \u001b[38;5;28mself\u001b[39m._prepare_request(\n\u001b[32m   1381\u001b[39m     messages,\n\u001b[32m   1382\u001b[39m     stop=stop,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1389\u001b[39m     tool_choice=tool_choice,\n\u001b[32m   1390\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1391\u001b[39m response: GenerateContentResponse = \u001b[38;5;28;01mawait\u001b[39;00m _achat_with_retry(\n\u001b[32m   1392\u001b[39m     request=request,\n\u001b[32m   1393\u001b[39m     **kwargs,\n\u001b[32m   1394\u001b[39m     generation_method=\u001b[38;5;28mself\u001b[39m.async_client.generate_content,\n\u001b[32m   1395\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m.default_metadata,\n\u001b[32m   1396\u001b[39m )\n\u001b[32m   1397\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:243\u001b[39m, in \u001b[36m_achat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _achat_with_retry(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189\u001b[39m, in \u001b[36mAsyncRetrying.wraps.<locals>.async_wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    188\u001b[39m async_wrapped.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m copy(fn, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    109\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     do = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter(retry_state=retry_state)\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    113\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153\u001b[39m, in \u001b[36mAsyncRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    151\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m action(retry_state)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\tenacity\\_utils.py:99\u001b[39m, in \u001b[36mwrap_to_async_func.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args: typing.Any, **kwargs: typing.Any) -> typing.Any:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\python312\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\python312\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m fn(*args, **kwargs)\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    116\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:237\u001b[39m, in \u001b[36m_achat_with_retry.<locals>._achat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m generation_method(**kwargs)\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    236\u001b[39m     \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[32m    238\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    239\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m: Invalid argument provided to Gemini: 400 * GenerateContentRequest.contents[14].parts: contents.parts must not be empty.\n* GenerateContentRequest.contents[23].parts: contents.parts must not be empty.\n",
      "During task with name 'supervisor' and id '4c5c307f-cde5-cb03-10d0-08ac230ed3da'"
     ]
    }
   ],
   "source": [
    "from open_deep_research.intelligent_research import ResearchMode\n",
    "from open_deep_research.util.logging import configure_logging, get_logger, reset_logging_config\n",
    "\n",
    "\n",
    "# 重置日志配置\n",
    "reset_logging_config()\n",
    "\n",
    "\n",
    "# Configure and run the multi-agent system\n",
    "# This sets up the model configuration and executes the research workflow\n",
    "\n",
    "# 🔧 修复后的配置 - 解决 [Errno 11001] getaddrinfo failed 错误\n",
    "# 方案1: 增强配置（推荐）- 启用搜索和问题询问功能\n",
    "config_enhanced = {\n",
    "    \"thread_id\": str(uuid.uuid4()),\n",
    "    \"search_api\": \"duckduckgo\",  # 使用DuckDuckGo搜索（无需API key）\n",
    "    \"ask_for_clarification\": True,  # 启用问题询问功能\n",
    "    \"supervisor_model\": \"google_genai:gemini-2.5-flash-lite-preview-06-17\",\n",
    "    \"researcher_model\": \"google_genai:gemini-2.5-flash-lite-preview-06-17\",\n",
    "    \"research_mode\": ResearchMode.REFLECTIVE.value, # 🧠 Enable intelligent reflection\n",
    "}\n",
    "\n",
    "# 方案2: 离线模式 - 避免所有网络请求（原配置）\n",
    "config_offline = {\n",
    "    \"thread_id\": str(uuid.uuid4()),\n",
    "    \"search_api\": \"none\",  # 禁用搜索工具，专注于基于知识的回答\n",
    "    \"supervisor_model\": \"google_genai:gemini-2.5-flash-lite-preview-06-17\",\n",
    "    \"researcher_model\": \"google_genai:gemini-2.5-flash\",\n",
    "    \"research_mode\": ResearchMode.REFLECTIVE.value, # 🧠 Enable intelligent reflection\n",
    "}\n",
    "\n",
    "# 方案3: 备选配置 - 使用DuckDuckGo搜索（无需API key）\n",
    "config_duckduckgo = {\n",
    "    \"thread_id\": str(uuid.uuid4()),\n",
    "    \"search_api\": \"duckduckgo\",  # 使用DuckDuckGo替代Tavily\n",
    "    \"supervisor_model\": \"google_genai:gemini-2.5-flash-lite-preview-06-17\",\n",
    "    \"researcher_model\": \"google_genai:gemini-2.5-flash\"\n",
    "}\n",
    "\n",
    "# 方案4: 原始配置（如果网络修复后）\n",
    "config_original = {\n",
    "    \"thread_id\": str(uuid.uuid4()),\n",
    "    \"search_api\": \"googlesearch\",\n",
    "    \"supervisor_model\": \"google_genai:gemini-2.5-flash-lite-preview-06-17\",\n",
    "    \"researcher_model\": \"google_genai:gemini-2.5-flash\",\n",
    "    \"research_mode\": ResearchMode.REFLECTIVE.value, # 🧠 Enable intelligent reflection\n",
    "    \"max_research_iterations\": 3,\n",
    "    \"ask_for_clarification\": True,\n",
    "}\n",
    "\n",
    "# 🔧 使用增强配置进行测试，解决工具缺失问题\n",
    "config = config_original\n",
    "print(f\"🔧 使用配置: {config['search_api']} 模式，问题询问: {config.get('ask_for_clarification', False)}\")\n",
    "print(\"📋 状态传递机制已修复: 通过消息内容而非共享状态字段传递 section 信息\")\n",
    "print(\"🔧 工具配置修复: 启用搜索工具和Question工具，确保完整的工作流程序列\")\n",
    "\n",
    "# Set up thread configuration with the specified parameters\n",
    "thread_config = {\"configurable\": config}  # 增加递归限制 , \"recursion_limit\": 10\n",
    "\n",
    "# Define the research topic as a user message\n",
    "# 针对离线模式优化的查询 - 使用更简单直接的查询避免递归问题\n",
    "msg = [{\"role\": \"user\", \"content\": \"请简要介绍模型上下文协议（MCP）的基本概念和主要用途。\"}]\n",
    "# msg = [{\"role\": \"user\", \"content\": \"请详细介绍Anthropic支持的MCP协议：1）MCP的架构设计和开发者指南，2）有趣的MCP服务器实现，3）与Google Agent2Agent协议的对比分析。请直接生成完整报告，无需询问后续问题。\"}]\n",
    "\n",
    "\n",
    "try:\n",
    "    from datetime import datetime\n",
    "    import uuid\n",
    "    log_filename = f\"multi_agent_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "    configure_logging(force_file_logging=True, log_filename=log_filename)\n",
    "    logger = get_logger(\"multi_agent_logging\")\n",
    "    \n",
    "    response = await agent.ainvoke({\"messages\": msg}, config=thread_config)\n",
    "    logger.info(\"✅ 工作流执行成功！网络连接问题已修复\")\n",
    "    logger.info(f\"response: {response}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    logger.error(f\"❌ 错误: {error_msg}\")\n",
    "    \n",
    "    import traceback\n",
    "    # 打印完整的traceback\n",
    "    logger.error(\"\\n🔴 完整错误追踪:\")\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # 如果是 KeyError，打印更多调试信息\n",
    "    if isinstance(e, KeyError):\n",
    "        logger.error(f\"\\n🔑 KeyError 详细信息:\")\n",
    "        logger.error(f\"缺少的键: {e.args}\")\n",
    "    \n",
    "    # 网络相关错误处理\n",
    "    if \"[Errno 11001] getaddrinfo failed\" in error_msg or \"DNS\" in error_msg:\n",
    "        logger.error(\"🔍 检测到网络/DNS错误，自动切换到离线模式...\")\n",
    "        config = config_offline\n",
    "        thread_config = {\"configurable\": config, \"recursion_limit\": 50}\n",
    "        \n",
    "        try:\n",
    "            response = await agent.ainvoke({\"messages\": msg}, config=thread_config)\n",
    "            logger.error(\"✅ 离线模式执行成功！\")\n",
    "        except Exception as offline_error:\n",
    "            logger.error(f\"❌ 离线模式也失败: {offline_error}\")\n",
    "            logger.error(\"💡 尝试使用DuckDuckGo搜索...\")\n",
    "            \n",
    "            config = config_duckduckgo\n",
    "            thread_config = {\"configurable\": config, \"recursion_limit\": 50}\n",
    "            try:\n",
    "                response = await agent.ainvoke({\"messages\": msg}, config=thread_config)\n",
    "                logger.error(\"✅ DuckDuckGo搜索模式执行成功！\")\n",
    "            except Exception as final_error:\n",
    "                logger.error(f\"❌ 所有方案都失败: {final_error}\")\n",
    "                raise final_error\n",
    "    \n",
    "    # 递归限制错误处理\n",
    "    elif \"Recursion limit\" in error_msg:\n",
    "        logger.error(\"🔍 检测到递归限制错误，尝试使用更简单的查询...\")\n",
    "        simple_msg = [{\"role\": \"user\", \"content\": \"请用几句话简单介绍MCP协议的概念。\"}]\n",
    "        \n",
    "        # 使用更高的递归限制和更简单的查询\n",
    "        config = config_offline  \n",
    "        thread_config = {\"configurable\": config, \"recursion_limit\": 100}\n",
    "        \n",
    "        try:\n",
    "            response = await agent.ainvoke({\"messages\": simple_msg}, config=thread_config)\n",
    "            logger.error(\"✅ 简化查询执行成功！\")\n",
    "        except Exception as simple_error:\n",
    "            logger.error(f\"❌ 简化查询也失败: {simple_error}\")\n",
    "            logger.error(\"💡 建议: 多智能体系统可能过于复杂，考虑使用单模型方法\")\n",
    "            raise simple_error\n",
    "    \n",
    "    # Gemini模型特定错误处理\n",
    "    elif \"parallel_tool_calls\" in error_msg:\n",
    "        logger.error(\"💡 parallel_tool_calls问题应该已修复\")\n",
    "        \n",
    "    elif \"function call turn\" in error_msg:\n",
    "        logger.error(\"💡 消息序列问题应该已修复\")\n",
    "        logger.error(\"🔍 如果仍然出现此错误，请联系开发者\")\n",
    "        \n",
    "    else:\n",
    "        logger.error(\"💡 遇到其他问题...\")\n",
    "        logger.error(\"🔄 建议检查:\")\n",
    "        logger.error(\"  1. GOOGLE_API_KEY 环境变量是否正确设置\")\n",
    "        logger.error(\"  2. 网络连接是否正常\")\n",
    "        logger.error(\"  3. 模型配置是否正确\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 发现响应数据，开始处理...\n",
      "📨 消息总数: 3\n",
      "📋 最后几条消息:\n",
      "[{'role': 'user', 'content': '请简要介绍模型上下文协议（MCP）的基本概念和主要用途。'}, {'role': 'user', 'content': '请简要介绍模型上下文协议（MCP）的基本概念和主要用途。'}, {'role': 'assistant', 'content': 'Calling tools.', 'tool_calls': [{'name': 'FinishReport', 'args': {}, 'id': '964de719-4ebf-4718-b811-e5d69bd465e9', 'type': 'tool_call'}]}]\n",
      "\n",
      "=== 结构化输出预览 ===\n",
      "消息数量: 3\n",
      "最终报告: 不存在\n",
      "完成章节: 0\n"
     ]
    }
   ],
   "source": [
    "# 🔧 修复后的响应处理代码\n",
    "import json\n",
    "from open_deep_research.message_converter import convert_langchain_messages_to_dict\n",
    "\n",
    "# 检查 response 是否存在\n",
    "if 'response' in locals() and response:\n",
    "    print(\"✅ 发现响应数据，开始处理...\")\n",
    "    messages = convert_langchain_messages_to_dict(response[\"messages\"])\n",
    "    print(f\"📨 消息总数: {len(messages)}\")\n",
    "    print(\"📋 最后几条消息:\")\n",
    "    print(messages[len(messages)-3 : len(messages)])\n",
    "\n",
    "    # 结构化输出\n",
    "    structured_response = {\n",
    "        \"messages\": messages,\n",
    "        \"final_report\": response.get(\"final_report\"),\n",
    "        \"source_str\": response.get(\"source_str\"),\n",
    "        \"completed_sections\": response.get(\"completed_sections\"),\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== 结构化输出预览 ===\")\n",
    "    print(f\"消息数量: {len(structured_response['messages'])}\")\n",
    "    print(f\"最终报告: {'存在' if structured_response['final_report'] else '不存在'}\")\n",
    "    print(f\"完成章节: {len(structured_response.get('completed_sections', []))}\")\n",
    "else:\n",
    "    print(\"⚠️ 响应数据不存在，请先成功运行上面的工作流代码\")\n",
    "    print(\"💡 当前修复状态: 并发更新问题已解决，可以重新运行工作流\")\n",
    "\n",
    "# print(\"=== 结构化输出 ===\")\n",
    "# print(json.dumps(structured_response, ensure_ascii=False, indent=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 开始生成详细报告...\n",
      "[Supervisor] 消息序列修复: 插入 user 消息以维持交替模式, 插入 user 消息以维持交替模式, 为消息 1 (assistant) 添加默认内容以避免空消息 (工具调用占位), 为消息 4 (assistant) 添加默认内容以避免空消息 (工具调用占位), 为消息 7 (assistant) 添加默认内容以避免空消息 (工具调用占位), 为消息 10 (assistant) 添加默认内容以避免空消息 (工具调用占位), 为消息 13 (assistant) 添加默认内容以避免空消息 (工具调用占位)\n",
      "❌ 错误: Invalid argument provided to Gemini: 400 * GenerateContentRequest.contents[14].parts: contents.parts must not be empty.\n",
      "\n"
     ]
    },
    {
     "ename": "ChatGoogleGenerativeAIError",
     "evalue": "Invalid argument provided to Gemini: 400 * GenerateContentRequest.contents[14].parts: contents.parts must not be empty.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgument\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:234\u001b[39m, in \u001b[36m_achat_with_retry.<locals>._achat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m generation_method(**kwargs)\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    236\u001b[39m     \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\async_client.py:444\u001b[39m, in \u001b[36mGenerativeServiceAsyncClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    443\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m rpc(\n\u001b[32m    445\u001b[39m     request,\n\u001b[32m    446\u001b[39m     retry=retry,\n\u001b[32m    447\u001b[39m     timeout=timeout,\n\u001b[32m    448\u001b[39m     metadata=metadata,\n\u001b[32m    449\u001b[39m )\n\u001b[32m    451\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary_async.py:231\u001b[39m, in \u001b[36mAsyncRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    228\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    229\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    230\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m retry_target(\n\u001b[32m    232\u001b[39m     functools.partial(func, *args, **kwargs),\n\u001b[32m    233\u001b[39m     predicate=\u001b[38;5;28mself\u001b[39m._predicate,\n\u001b[32m    234\u001b[39m     sleep_generator=sleep_generator,\n\u001b[32m    235\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m._timeout,\n\u001b[32m    236\u001b[39m     on_error=on_error,\n\u001b[32m    237\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary_async.py:163\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary_async.py:158\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m target()\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers_async.py:88\u001b[39m, in \u001b[36m_WrappedUnaryResponseMixin.__await__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(rpc_error) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrpc_error\u001b[39;00m\n",
      "\u001b[31mInvalidArgument\u001b[39m: 400 * GenerateContentRequest.contents[14].parts: contents.parts must not be empty.\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m         m.pretty_print()\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🚀 开始生成详细报告...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m agent.ainvoke({\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: msg}, config=thread_config)\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ 报告生成成功\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# 显示所有消息\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2788\u001b[39m, in \u001b[36mPregel.ainvoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2785\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]] = []\n\u001b[32m   2786\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2788\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(\n\u001b[32m   2789\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2790\u001b[39m     config,\n\u001b[32m   2791\u001b[39m     stream_mode=stream_mode,\n\u001b[32m   2792\u001b[39m     output_keys=output_keys,\n\u001b[32m   2793\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   2794\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   2795\u001b[39m     checkpoint_during=checkpoint_during,\n\u001b[32m   2796\u001b[39m     debug=debug,\n\u001b[32m   2797\u001b[39m     **kwargs,\n\u001b[32m   2798\u001b[39m ):\n\u001b[32m   2799\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2800\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2801\u001b[39m             \u001b[38;5;28misinstance\u001b[39m(chunk, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m   2802\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m (ints := chunk.get(INTERRUPT)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2803\u001b[39m         ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2655\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2653\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2654\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2655\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2656\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2657\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2658\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2659\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2660\u001b[39m ):\n\u001b[32m   2661\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2662\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[32m   2663\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langgraph\\pregel\\runner.py:294\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    292\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    295\u001b[39m         t,\n\u001b[32m    296\u001b[39m         retry_policy,\n\u001b[32m    297\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    298\u001b[39m         configurable={\n\u001b[32m    299\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    300\u001b[39m                 _acall,\n\u001b[32m    301\u001b[39m                 weakref.ref(t),\n\u001b[32m    302\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    303\u001b[39m                 retry=retry_policy,\n\u001b[32m    304\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    305\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    306\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    307\u001b[39m                 loop=loop,\n\u001b[32m    308\u001b[39m             ),\n\u001b[32m    309\u001b[39m         },\n\u001b[32m    310\u001b[39m     )\n\u001b[32m    311\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langgraph\\pregel\\retry.py:136\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policies, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    134\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    138\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py:672\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    670\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    671\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m672\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    673\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    674\u001b[39m         )\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    676\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\python312\\Lib\\asyncio\\futures.py:287\u001b[39m, in \u001b[36mFuture.__await__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    286\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncio_future_blocking = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mawait wasn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt used with future\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\python312\\Lib\\asyncio\\tasks.py:385\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    388\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\python312\\Lib\\asyncio\\futures.py:203\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\python312\\Lib\\asyncio\\tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py:440\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    438\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    442\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\src\\open_deep_research\\multi_agent.py:367\u001b[39m, in \u001b[36msupervisor\u001b[39m\u001b[34m(state, config)\u001b[39m\n\u001b[32m    357\u001b[39m llm_messages = [\n\u001b[32m    358\u001b[39m     {\n\u001b[32m    359\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    360\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: system_prompt\n\u001b[32m    361\u001b[39m     }\n\u001b[32m    362\u001b[39m ] + messages\n\u001b[32m    364\u001b[39m \u001b[38;5;66;03m# Invoke\u001b[39;00m\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    366\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m llm_with_tools.ainvoke(llm_messages)\n\u001b[32m    368\u001b[39m     ]\n\u001b[32m    369\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5444\u001b[39m, in \u001b[36mRunnableBindingBase.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5437\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5438\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m   5439\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5442\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5443\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5444\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.ainvoke(\n\u001b[32m   5445\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5446\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5447\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5448\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:394\u001b[39m, in \u001b[36mBaseChatModel.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> BaseMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m    395\u001b[39m         [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    396\u001b[39m         stop=stop,\n\u001b[32m    397\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    398\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    399\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    400\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    401\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    402\u001b[39m         **kwargs,\n\u001b[32m    403\u001b[39m     )\n\u001b[32m    404\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m, llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:968\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    959\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m    961\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    965\u001b[39m     **kwargs: Any,\n\u001b[32m    966\u001b[39m ) -> LLMResult:\n\u001b[32m    967\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m968\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m    969\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m    970\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:926\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    913\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[32m    914\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    915\u001b[39m             *[\n\u001b[32m    916\u001b[39m                 run_manager.on_llm_end(\n\u001b[32m   (...)\u001b[39m\u001b[32m    924\u001b[39m             ]\n\u001b[32m    925\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[32m0\u001b[39m]\n\u001b[32m    927\u001b[39m flattened_outputs = [\n\u001b[32m    928\u001b[39m     LLMResult(generations=[res.generations], llm_output=res.llm_output)  \u001b[38;5;66;03m# type: ignore[list-item, union-attr]\u001b[39;00m\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[32m    930\u001b[39m ]\n\u001b[32m    931\u001b[39m llm_output = \u001b[38;5;28mself\u001b[39m._combine_llm_outputs([res.llm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\python312\\Lib\\asyncio\\tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1094\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1092\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._agenerate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1094\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m   1095\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1096\u001b[39m     )\n\u001b[32m   1097\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1098\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:1391\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   1376\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()._agenerate(\n\u001b[32m   1377\u001b[39m         messages, stop, run_manager, **updated_kwargs\n\u001b[32m   1378\u001b[39m     )\n\u001b[32m   1380\u001b[39m request = \u001b[38;5;28mself\u001b[39m._prepare_request(\n\u001b[32m   1381\u001b[39m     messages,\n\u001b[32m   1382\u001b[39m     stop=stop,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1389\u001b[39m     tool_choice=tool_choice,\n\u001b[32m   1390\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1391\u001b[39m response: GenerateContentResponse = \u001b[38;5;28;01mawait\u001b[39;00m _achat_with_retry(\n\u001b[32m   1392\u001b[39m     request=request,\n\u001b[32m   1393\u001b[39m     **kwargs,\n\u001b[32m   1394\u001b[39m     generation_method=\u001b[38;5;28mself\u001b[39m.async_client.generate_content,\n\u001b[32m   1395\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m.default_metadata,\n\u001b[32m   1396\u001b[39m )\n\u001b[32m   1397\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:243\u001b[39m, in \u001b[36m_achat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _achat_with_retry(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189\u001b[39m, in \u001b[36mAsyncRetrying.wraps.<locals>.async_wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    188\u001b[39m async_wrapped.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m copy(fn, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    109\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     do = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter(retry_state=retry_state)\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    113\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153\u001b[39m, in \u001b[36mAsyncRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    151\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m action(retry_state)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\tenacity\\_utils.py:99\u001b[39m, in \u001b[36mwrap_to_async_func.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args: typing.Any, **kwargs: typing.Any) -> typing.Any:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\python312\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\python312\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m fn(*args, **kwargs)\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    116\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MyProjects\\open_deep_research\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:237\u001b[39m, in \u001b[36m_achat_with_retry.<locals>._achat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m generation_method(**kwargs)\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    236\u001b[39m     \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[32m    238\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    239\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m: Invalid argument provided to Gemini: 400 * GenerateContentRequest.contents[14].parts: contents.parts must not be empty.\n",
      "During task with name 'supervisor' and id 'e5a2fd78-495c-1f4f-744a-e0a002dfcb38'"
     ]
    }
   ],
   "source": [
    "msg = [{\"role\": \"user\", \"content\": \"请详细介绍Anthropic支持的MCP协议：1）MCP的架构设计和开发者指南，2）有趣的MCP服务器实现，3）与Google Agent2Agent协议的对比分析。请直接生成完整报告，无需询问后续问题。\"}]\n",
    "\n",
    "# 运行改进的查询\n",
    "try:\n",
    "    print(\"🚀 开始生成详细报告...\")\n",
    "    response = await agent.ainvoke({\"messages\": msg}, config=thread_config)\n",
    "    print(\"✅ 报告生成成功\")\n",
    "    \n",
    "    # 显示当前消息\n",
    "    for m in agent.get_state(thread_config).values['messages']:\n",
    "        m.pretty_print()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ 错误: {e}\")\n",
    "    if \"parallel_tool_calls\" in str(e):\n",
    "        print(\"💡 检测到模型兼容性问题，代码已自动修复\")\n",
    "        response = await agent.ainvoke({\"messages\": msg}, config=thread_config)\n",
    "        for m in agent.get_state(thread_config).values['messages']:\n",
    "            m.pretty_print()\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'final_report'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Markdown\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m Markdown(\u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfinal_report\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[31mKeyError\u001b[39m: 'final_report'"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(agent.get_state(thread_config).values['final_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# # 模型上下文协议（MCP）概述\n",
       "\n",
       "模型上下文协议（Model Context Protocol，简称 MCP）是一项开放标准，旨在革新大型语言模型（LLM）与外部数据源和工具的交互方式。它通过提供一个标准化的接口，解决了AI模型因数据孤岛和缺乏外部交互能力而面临的局限性。MCP 被形象地比喻为“AI 应用的 USB-C 端口”，它使得AI应用能够像“开卷考试”一样，实时获取和利用外部世界的最新信息和功能，从而极大地增强了AI代理的能力，使其能够执行更复杂、更具实际意义的任务。本文将深入探讨MCP的基本概念、主要用途、工作原理，以及它在构建强大的AI Agent和实现与外部系统集成方面所扮演的关键角色。\n",
       "\n",
       "## 模型上下文协议（MCP）的基本概念\n",
       "\n",
       "模型上下文协议（Model Context Protocol，简称 MCP）是由 Anthropic 推动的一项开放标准，旨在标准化大型语言模型（LLM）应用与外部数据源和工具的连接方式。它被形象地比喻为“AI 应用的 USB-C 端口”。\n",
       "\n",
       "MCP 的核心概念包括：\n",
       "*   **标准化集成**：提供统一接口，使 LLM 能够无缝访问实时信息和外部工具，避免复杂的定制化集成。\n",
       "*   **上下文提供**：允许应用程序以结构化的方式向 LLM 提供外部数据作为上下文，显著提升模型的性能和相关性。\n",
       "*   **增强 AI 能力**：使 AI 代理能够读取指定外部数据，并调用工具执行任务，从而超越简单的对话功能，成为更强大的 AI Agent。\n",
       "\n",
       "### Sources\n",
       "1.  https://www.anthropic.com/news/model-context-protocol\n",
       "2.  https://modelcontextprotocol.io/introduction\n",
       "3.  https://www.ibest.com.tw/news-detail/what-is-mcp/\n",
       "4.  https://zhuanlan.zhihu.com/p/29001189476\n",
       "\n",
       "## MCP的主要用途\n",
       "\n",
       "模型上下文协议（MCP）的主要用途在于解决AI模型因数据孤岛限制而无法充分发挥潜力的问题。它定义了应用程序与AI模型之间交换上下文信息的标准化方式，使AI开发者和系统集成商能够更便捷地将大型语言模型（LLM）与外部数据源和工具集成。\n",
       "\n",
       "MCP使得LLM能够获得“动手”能力，在各种环境中执行操作或调用数据。其常见应用场景包括：\n",
       "\n",
       "*   **数据集成与访问**：连接AI助手到各种数据系统，实现通用数据访问，避免为每个数据集编写自定义代码。\n",
       "*   **工具调用**：使LLM能够调用外部工具，例如获取实时市场数据、进行浏览器自动化操作（如使用Playwright）。\n",
       "*   **增强AI应用**：支持多轮对话系统、代码生成工具和企业级数据分析等复杂AI应用，通过提供外部上下文信息减少模型幻觉。\n",
       "\n",
       "### Sources\n",
       "1.  https://zhuanlan.zhihu.com/p/2732715233\n",
       "2.  https://www.threads.com/@mr.__.l/post/DHIHf5PBtyb/%E4%B8%BB%E8%A6%81%E7%94%A8%E9%80%94%E8%88%87%E6%87%89%E7%94%A8%E5%A0%B4%E6%99%AFmcp-%E7%9A%84%E7%94%A8%E9%80%94%E5%9C%A8%E6%96%BC%E7%82%BA-ai-%E9%96%8B%E7%99%BC%E8%80%85%E8%88%87%E7%B3%BB%E7%B5%B1%E6%95%B4%E5%90%88%E6%8F%90%E4%BE%9B%E4%BE%BF%E5%88%A9%E8%97%89%E7%94%B1-mcp%E9%96%8B%E7%99%BC%E8%80%85%E5%8F%AF%E4%BB%A5%E8%AE%93%E4%BB%BB%E6%84%8F%E7%AC%A6%E5%90%88%E8%A6%8F%E7%AF%84%E7%9A%84-llm-%E7%8D%B2%E5%BE%97%E5%8B%95%E6%89%8B%E8%83%BD%E5%8A%9B%E5%9C%A8%E5%90%84%E7%A8%AE%E7%92%B0%E5%A2%83%E4%B8%AD%E5%9F%B7%E8%A1%8C%E6%93%8D%E4%BD%9C%E6%88%96%E8%AA%BF%E7%94%A8%E8%B3%87%E6%96%99\n",
       "3.  https://www.anthropic.com/news/model-context-protocol\n",
       "4.  https://github.com/microsoft/playwright-mcp\n",
       "\n",
       "## MCP的工作原理\n",
       "\n",
       "微通道板（MCP）是一种用于探测单个粒子和光子的电子倍增器。其工作原理基于二次电子发射效应，实现信号的显著放大。\n",
       "\n",
       "**工作流程：**\n",
       "*   当一个入射粒子或光子进入MCP的微通道并撞击通道壁时，会激发并释放出初级电子。\n",
       "*   在微通道两端施加的电场作用下，这些电子被加速并沿着通道前进。由于通道壁通常涂有高电阻材料，电子在飞行过程中会不断撞击壁面。\n",
       "*   每次撞击都会产生更多的二次电子，形成一个级联效应（电子雪崩）。\n",
       "*   最终，从通道输出端会产生一个比原始输入信号强得多的电子脉冲，从而实现信号的放大。\n",
       "\n",
       "MCPs因其高增益、快速响应和高空间分辨率等特点，广泛应用于夜视设备、质谱仪和科学研究等领域。\n",
       "\n",
       "### Sources\n",
       "1. https://en.wikipedia.org/wiki/Microchannel_plate_detector\n",
       "2. https://szphoton.com/blogs/articles/how-does-an-mcp-work\n",
       "3. https://www0.mi.infn.it/~sleoni/TEACHING/Nuc-Phys-Det/PDF/papers/MCP.pdf\n",
       "\n",
       "## MCP在AI Agent中的作用\n",
       "\n",
       "模型上下文协议（MCP）是一个开放标准，旨在标准化AI助手或代理与数据所在系统之间的连接方式。它充当AI模型和工具的通用适配器，简化了AI代理与外部世界的集成。\n",
       "\n",
       "MCP在AI Agent中扮演着关键角色：\n",
       "*   **实现多步任务**：MCP使AI代理能够执行复杂的多步任务，例如数据检索、文档摘要或信息保存。\n",
       "*   **提供结构化连接**：它为AI代理提供了安全、结构化的“管道”，使其能够与外部系统和数据源进行交互。\n",
       "*   **简化Agent构建**：通过MCP服务器暴露的能力，MCP简化了AI代理的构建过程，避免了为每个AI模型或工具编写自定义代码。\n",
       "*   **增强上下文利用**：MCP支持AI代理利用更长的上下文窗口，从而能够处理更多信息并提供更连贯的响应。\n",
       "*   **促进生态系统集成**：它允许AI代理连接到庞大且多样化的数据源和功能，从而利用不断发展的MCP兼容工具生态系统。\n",
       "\n",
       "### Sources\n",
       "1.  https://www.anthropic.com/news/model-context-protocol\n",
       "2.  https://medium.com/@elisowski/mcp-explained-the-new-standard-connecting-ai-to-everything-79c5a1c98288\n",
       "3.  https://cloud.google.com/products/agent-builder?hl=zh-CN\n",
       "4.  https://huggingface.co/blog/Kseniase/mcp\n",
       "5.  https://aws.amazon.com/cn/blogs/china/fast-fashion-e-commerce-agent-design-ideas-and-application-practice-part-two/\n",
       "\n",
       "## MCP与外部数据源和工具的集成方式\n",
       "\n",
       "模型上下文协议（MCP）是一个开放标准，旨在实现大型语言模型（LLM）应用程序与外部数据源和工具的无缝集成。它通过提供一个统一的接口，标准化了AI模型获取外部信息和执行操作的方式，被形象地比喻为AI应用的“USB-C端口”。\n",
       "\n",
       "MCP集成的主要方式和优势包括：\n",
       "\n",
       "*   **标准化交互**：MCP解决了传统集成的碎片化问题，允许LLM直接连接到各种数据源和业务工具，无需为每个数据集编写自定义代码。\n",
       "*   **实时数据与上下文**：AI应用通过MCP服务器从外部知识库或数据源（如通过向量检索、关键词匹配）搜索并获取实时信息和业务数据，将其作为上下文提供给LLM，从而生成更相关、有用的响应。\n",
       "*   **自定义集成**：开发者可以创建自定义集成，将LLM直接连接到对工作流程至关重要的工具和数据源，使LLM能够在现有软件中运行并从中获取洞察。\n",
       "*   **应用场景**：MCP支持AI代理频繁与外部工具和数据源交互，例如GitHub Copilot利用MCP服务器从代码仓库和外部资源中获取数据。\n",
       "\n",
       "### Sources\n",
       "1.  https://support.anthropic.com/zh-TW/articles/10949351-%E5%9C%A8%E6%A1%8C%E9%9D%A2%E7%89%88-claude-%E4%B8%8A%E9%96%8B%E5%A7%8B%E4%BD%BF%E7%94%A8%E6%A8%A1%E5%9E%8B%E4%B8%8A%E4%B8%8B%E6%96%87%E5%8D%8F%E8%AD%B0-mcp\n",
       "2.  https://modelcontextprotocol.io/introduction\n",
       "3.  https://www.anthropic.com/news/model-context-protocol\n",
       "4.  https://zhuanlan.zhihu.com/p/30387285411\n",
       "5.  https://cloud.google.com/vertex-ai\n",
       "6.  https://github.com/features/copilot\n",
       "\n",
       "## ## 结论\n",
       "\n",
       "模型上下文协议（MCP）作为连接AI模型与外部世界的重要桥梁，通过标准化数据和工具的交互方式，极大地拓展了AI的能力边界。它使得AI代理能够超越训练数据的局限，实时获取信息、执行操作，从而在各种应用场景中展现出更强的智能和实用性。从赋能AI Agent到促进与企业数据和系统的深度集成，MCP正逐步成为构建下一代智能应用的关键技术。随着MCP生态的不断发展和完善，我们可以期待AI在未来能够承担更多复杂任务，并与人类社会产生更深层次的互动与协作。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(agent.get_state(thread_config).values['final_report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trace: \n",
    "\n",
    "> Note: uses 456k tokens \n",
    "\n",
    "https://smith.langchain.com/public/f1581fa5-dfc9-445c-a8f4-3518a05cd139/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试修复后的工作流\n",
    "print(\"🔧 测试修复后的Gemini API工作流\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 简单测试消息\n",
    "test_query = \"请简要介绍模型上下文协议（MCP）的基本概念和主要用途。\"\n",
    "\n",
    "try:\n",
    "    # 重新创建agent，确保使用最新的修复\n",
    "    checkpointer = MemorySaver()\n",
    "    agent = supervisor_builder.compile(name=\"test_agent\", checkpointer=checkpointer)\n",
    "    \n",
    "    # 配置\n",
    "    config = {\n",
    "        \"thread_id\": str(uuid.uuid4()),\n",
    "        \"search_api\": \"google\",\n",
    "        \"supervisor_model\": \"google_genai:gemini-2.5-flash\",\n",
    "        \"researcher_model\": \"google_genai:gemini-2.5-flash\",\n",
    "        \"max_research_iterations\": 2\n",
    "    }\n",
    "    \n",
    "    thread_config = {\"configurable\": config, \"recursion_limit\": 20}\n",
    "    \n",
    "    # 测试消息\n",
    "    msg = [{\"role\": \"user\", \"content\": test_query}]\n",
    "    \n",
    "    print(f\"📝 测试查询: {test_query}\")\n",
    "    print(\"🚀 开始执行工作流...\")\n",
    "    \n",
    "    # 执行工作流\n",
    "    response = await agent.ainvoke({\"messages\": msg}, config=thread_config)\n",
    "    \n",
    "    print(\"✅ 工作流执行成功！\")\n",
    "    print(f\"📊 响应类型: {type(response)}\")\n",
    "    \n",
    "    # 检查最终状态\n",
    "    final_state = agent.get_state(thread_config)\n",
    "    if hasattr(final_state, 'values') and 'messages' in final_state.values:\n",
    "        messages = final_state.values['messages']\n",
    "        print(f\"📨 最终消息数量: {len(messages)}\")\n",
    "        \n",
    "        # 检查是否有最终报告\n",
    "        for i, msg in enumerate(messages[-3:]):  # 检查最后几条消息\n",
    "            if hasattr(msg, 'content'):\n",
    "                content = msg.content\n",
    "            elif isinstance(msg, dict):\n",
    "                content = msg.get('content', '')\n",
    "            else:\n",
    "                content = str(msg)\n",
    "            \n",
    "            if content and len(content) > 100:  # 可能是报告\n",
    "                print(f\"📄 发现报告内容 (消息 {len(messages)-3+i}):\")\n",
    "                print(content[:200] + \"...\" if len(content) > 200 else content)\n",
    "                break\n",
    "    \n",
    "    print(\"🎉 修复成功！没有遇到空消息错误。\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 错误: {e}\")\n",
    "    \n",
    "    # 检查是否是空消息错误\n",
    "    if \"contents.parts must not be empty\" in str(e):\n",
    "        print(\"🔍 仍然存在空消息错误，需要进一步调试\")\n",
    "    else:\n",
    "        print(\"🔍 这是其他类型的错误\")\n",
    "    \n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
